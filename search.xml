<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CentOS 7用DevStack安装OpenStack</title>
    <url>/posts/776b29f3.html</url>
    <content><![CDATA[<h2 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h2><h3 id="OpenStack源码"><a href="#OpenStack源码" class="headerlink" title="OpenStack源码"></a>OpenStack源码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;openstack</span><br></pre></td></tr></table></figure>

<h3 id="DevStack源码"><a href="#DevStack源码" class="headerlink" title="DevStack源码"></a>DevStack源码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;git.openstack.org&#x2F;cgit&#x2F;openstack-dev&#x2F;devstack</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="设置aliyun的base源"><a href="#设置aliyun的base源" class="headerlink" title="设置aliyun的base源"></a>设置aliyun的base源</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">[root@compute ~]# yum makecache</span><br></pre></td></tr></table></figure>

<h3 id="DevStack和OpenStack源码可以替换为TryStack镜像"><a href="#DevStack和OpenStack源码可以替换为TryStack镜像" class="headerlink" title="DevStack和OpenStack源码可以替换为TryStack镜像"></a>DevStack和OpenStack源码可以替换为TryStack镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># use TryStack git mirror</span><br><span class="line">GIT_BASE&#x3D;http:&#x2F;&#x2F;git.trystack.cn</span><br><span class="line">NOVNC_REPO&#x3D;http:&#x2F;&#x2F;git.trystack.cn&#x2F;kanaka&#x2F;noVNC.git</span><br><span class="line">SPICE_REPO&#x3D;http:&#x2F;&#x2F;git.trystack.cn&#x2F;git&#x2F;spice&#x2F;spice-html5.git</span><br></pre></td></tr></table></figure>

<h3 id="pip源地址可以换为国内的豆瓣源"><a href="#pip源地址可以换为国内的豆瓣源" class="headerlink" title="pip源地址可以换为国内的豆瓣源"></a>pip源地址可以换为国内的豆瓣源</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# mkdir &#x2F;root&#x2F;.pip</span><br><span class="line">[root@compute ~]# cat &#x2F;root&#x2F;.pip&#x2F;pip.conf</span><br><span class="line">[global]</span><br><span class="line">index-url &#x3D; http:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;</span><br><span class="line">trusted-host &#x3D; pypi.douban.com</span><br></pre></td></tr></table></figure>

<h2 id="安装阶段"><a href="#安装阶段" class="headerlink" title="安装阶段"></a>安装阶段</h2><h3 id="下载DevStack"><a href="#下载DevStack" class="headerlink" title="下载DevStack"></a>下载DevStack</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone http:&#x2F;&#x2F;git.trystack.cn&#x2F;openstack-dev&#x2F;devstack.git -b stable&#x2F;ocata</span><br></pre></td></tr></table></figure>

<h3 id="创建stack用户"><a href="#创建stack用户" class="headerlink" title="创建stack用户"></a>创建stack用户</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@controller home]# mkdir -p &#x2F;home&#x2F;stack&#x2F;logs</span><br><span class="line">[root@controller home]# cd devstack&#x2F;tools&#x2F;</span><br><span class="line">[root@controller home]# sudo .&#x2F;create-stack-user.sh</span><br><span class="line">[root@controller home]# sudo passwd stack</span><br><span class="line">[root@controller home]# sudo chown –R stack:stack &#x2F;home&#x2F;devstack</span><br><span class="line">[root@controller home]# sudo chown –R stack:stack &#x2F;home&#x2F;stack</span><br></pre></td></tr></table></figure>

<h3 id="授权stack用户"><a href="#授权stack用户" class="headerlink" title="授权stack用户"></a>授权stack用户</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# vim &#x2F;etc&#x2F;sudoers</span><br><span class="line">第98行，添加1行</span><br><span class="line">stack ALL&#x3D;(ALL:ALL) ALL</span><br></pre></td></tr></table></figure>

<h3 id="创建local-conf文件"><a href="#创建local-conf文件" class="headerlink" title="创建local.conf文件"></a>创建local.conf文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat &#x2F;home&#x2F;devstack&#x2F;local.conf </span><br><span class="line">[[local|localrc]]</span><br><span class="line"></span><br><span class="line"># use TryStack git mirror</span><br><span class="line">GIT_BASE&#x3D;http:&#x2F;&#x2F;git.trystack.cn</span><br><span class="line">NOVNC_REPO&#x3D;http:&#x2F;&#x2F;git.trystack.cn&#x2F;kanaka&#x2F;noVNC.git</span><br><span class="line">SPICE_REPO&#x3D;http:&#x2F;&#x2F;git.trystack.cn&#x2F;git&#x2F;spice&#x2F;spice-html5.git</span><br><span class="line"></span><br><span class="line">#OFFLINE&#x3D;True</span><br><span class="line">RECLONE&#x3D;True</span><br><span class="line"></span><br><span class="line"># Define images to be automatically downloaded during the DevStack built process.</span><br><span class="line">DOWNLOAD_DEFAULT_IMAGES&#x3D;False</span><br><span class="line">IMAGE_URLS&#x3D;&quot;http:&#x2F;&#x2F;images.trystack.cn&#x2F;cirros&#x2F;cirros-0.3.4-x86_64-disk.img&quot;</span><br><span class="line"></span><br><span class="line">HOST_IP&#x3D;192.168.1.225</span><br><span class="line"></span><br><span class="line"># Credentials</span><br><span class="line">DATABASE_PASSWORD&#x3D;pass</span><br><span class="line">ADMIN_PASSWORD&#x3D;pass</span><br><span class="line">SERVICE_PASSWORD&#x3D;pass</span><br><span class="line">SERVICE_TOKEN&#x3D;pass</span><br><span class="line">RABBIT_PASSWORD&#x3D;pass</span><br><span class="line"></span><br><span class="line">HORIZON_BRANCH&#x3D;stable&#x2F;ocata</span><br><span class="line">KEYSTONE_BRANCH&#x3D;stable&#x2F;ocata</span><br><span class="line">NOVA_BRANCH&#x3D;stable&#x2F;ocata</span><br><span class="line">NEUTRON_BRANCH&#x3D;stable&#x2F;ocata</span><br><span class="line">GLANCE_BRANCH&#x3D;stable&#x2F;ocata</span><br><span class="line">CINDER_BRANCH&#x3D;stable&#x2F;ocata</span><br><span class="line"></span><br><span class="line">#keystone</span><br><span class="line">KEYSTONE_TOKEN_FORMAT&#x3D;UUID</span><br><span class="line"></span><br><span class="line">##Heat</span><br><span class="line">HEAT_BRANCH&#x3D;stable&#x2F;ocata</span><br><span class="line">enable_service h-eng h-api h-api-cfn h-api-cw</span><br><span class="line"></span><br><span class="line">## Swift</span><br><span class="line">SWIFT_BRANCH&#x3D;stable&#x2F;ocata</span><br><span class="line">ENABLED_SERVICES+&#x3D;,s-proxy,s-object,s-container,s-account</span><br><span class="line">SWIFT_REPLICAS&#x3D;1</span><br><span class="line">SWIFT_HASH&#x3D;011688b44136573e209e</span><br><span class="line"></span><br><span class="line"># Enabling Neutron (network) Service</span><br><span class="line">disable_service n-net</span><br><span class="line">enable_service q-svc</span><br><span class="line">enable_service q-agt</span><br><span class="line">enable_service q-dhcp</span><br><span class="line">enable_service q-l3</span><br><span class="line">enable_service q-meta</span><br><span class="line">enable_service q-metering</span><br><span class="line">enable_service neutron</span><br><span class="line"></span><br><span class="line">## Neutron options</span><br><span class="line">Q_USE_SECGROUP&#x3D;True</span><br><span class="line">FLOATING_RANGE&#x3D;&quot;192.168.1.0&#x2F;24&quot;</span><br><span class="line">FIXED_RANGE&#x3D;&quot;10.0.0.0&#x2F;24&quot;</span><br><span class="line">NETWORK_GATEWAY&#x3D;&quot;10.0.0.2&quot;</span><br><span class="line">Q_FLOATING_ALLOCATION_POOL&#x3D;start&#x3D;192.168.1.150,end&#x3D;192.168.1.180</span><br><span class="line">PUBLIC_NETWORK_GATEWAY&#x3D;&quot;192.168.1.200&quot;</span><br><span class="line">Q_L3_ENABLED&#x3D;True</span><br><span class="line">PUBLIC_INTERFACE&#x3D;eth0</span><br><span class="line">Q_USE_PROVIDERNET_FOR_PUBLIC&#x3D;True</span><br><span class="line">OVS_PHYSICAL_BRIDGE&#x3D;br-ex</span><br><span class="line">PUBLIC_BRIDGE&#x3D;br-ex</span><br><span class="line">OVS_BRIDGE_MAPPINGS&#x3D;public:br-ex</span><br><span class="line"></span><br><span class="line"># #VLAN configuration.</span><br><span class="line">Q_PLUGIN&#x3D;ml2</span><br><span class="line">ENABLE_TENANT_VLANS&#x3D;True</span><br><span class="line"></span><br><span class="line"># Logging</span><br><span class="line">LOGFILE&#x3D;&#x2F;home&#x2F;stack&#x2F;logs&#x2F;stack.sh.log</span><br><span class="line">VERBOSE&#x3D;True</span><br><span class="line">LOG_COLOR&#x3D;True</span><br><span class="line">SCREEN_LOGDIR&#x3D;&#x2F;home&#x2F;stack&#x2F;logs</span><br></pre></td></tr></table></figure>

<h3 id="以stack用户身份运行脚本安装"><a href="#以stack用户身份运行脚本安装" class="headerlink" title="以stack用户身份运行脚本安装"></a>以stack用户身份运行脚本安装</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# su stack</span><br><span class="line">[root@controller ~]# cd &#x2F;home&#x2F;devstack&#x2F;</span><br><span class="line">[root@controller devstack]# .&#x2F;stack.sh</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>CentOS7通过yum安装最新版本MySQL</title>
    <url>/posts/undefined.html</url>
    <content><![CDATA[<p>CentOS 7之后的版本yum的默认源中使用MariaDB替代原先MySQL，因此安装方式较为以往有一些改变：</p>
<p>卸载原来的MariaDB</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum remove -y mariadb-config-3:10.1.17-1.el7.x86_64</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>下载mysql的源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;dev.mysql.com&#x2F;get&#x2F;mysql57-community-release-el7-7.noarch.rpm</span><br></pre></td></tr></table></figure>

<p>安装yum库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum localinstall -y mysql57-community-release-el7-7.noarch.rpm</span><br></pre></td></tr></table></figure>

<p>安装MySQL</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y mysql-community-server</span><br></pre></td></tr></table></figure>

<p>启动MySQL服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start mysqld.service</span><br></pre></td></tr></table></figure>

<p>MySQL5.7加强了root用户的安全性，因此在第一次安装后会初始化一个随机密码，以下为查看初始随机密码的方式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep &#39;temporary password&#39; &#x2F;var&#x2F;log&#x2F;mysqld.log</span><br></pre></td></tr></table></figure>

<p>使用初始随机密码登录后MySQL会强制要求修改密码，否则无法正常使用，（密码必须包含小写、大写字母及特殊字符，当然也有其他方法不受此限制，再次不多做描述），修改方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET PASSWORD &#x3D; PASSWORD(&#39;your new password&#39;);</span><br><span class="line">ALTER USER &#39;root&#39;@&#39;localhost&#39; PASSWORD EXPIRE NEVER;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>
<p>然后退出后即可用新密码登录。</p>
<p>远程连接授权：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;your password&#39; WITH GRANT OPTION;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS 7 安装NFS</title>
    <url>/posts/3b526fbd.html</url>
    <content><![CDATA[<p>1、安装NFS</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install nfs-utils rpcbind -y</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>2、启动服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start rpcbind.service</span><br><span class="line">systemctl enable rpcbind.service</span><br><span class="line">systemctl start nfs.service</span><br><span class="line">systemctl enable nfs.service</span><br></pre></td></tr></table></figure>

<p>3、配置nfs</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@server01 ~]# cat &#x2F;etc&#x2F;exports</span><br><span class="line"></span><br><span class="line">&#x2F;data&#x2F;mnt&#x2F;mov&#x2F; 10.104.0.0&#x2F;16(rw,no_root_squash,no_all_squash,sync,anonuid&#x3D;501,anongid&#x3D;501)</span><br><span class="line">&#x2F;data&#x2F;application&#x2F;file-police-tempFilePath&#x2F; 10.104.0.0&#x2F;16(rw,no_root_squash,no_all_squash,sync,anonuid&#x3D;501,anongid&#x3D;501)</span><br></pre></td></tr></table></figure>

<p>4、加载nfs配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">exportfs -rv</span><br></pre></td></tr></table></figure>

<p>5、测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nfs服务器挂载情况</span><br><span class="line">showmount -e</span><br><span class="line">挂载测试</span><br><span class="line">mount -t nfs 10.104.38.28:&#x2F;data&#x2F;mnt&#x2F;mov &#x2F;data&#x2F;mnt&#x2F;mov</span><br><span class="line">mount -t nfs 10.104.38.28:&#x2F;data&#x2F;application&#x2F;file-police-tempFilePath &#x2F;data&#x2F;application&#x2F;file-police-tempFilePath</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>nfs</category>
      </categories>
      <tags>
        <tag>nfs</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS7 LDAP统一认证部署</title>
    <url>/posts/c197f166.html</url>
    <content><![CDATA[<h2 id="LDAP介绍"><a href="#LDAP介绍" class="headerlink" title="LDAP介绍"></a>LDAP介绍</h2><p>LDAP是轻量目录访问协议，英文全称是Lightweight Directory Access Protocol，简称为LDAP。它是基于X.500标准的，但是简单多了并且可以根据需要定制。与X.500不同，LDAP支持TCP/IP。LDAP的核心规范在RFC中都有定义，所有与LDAP相关的RFC都可以在LDAPman RFC网页中找到。</p>
<a id="more"></a>
<h2 id="使用目的"><a href="#使用目的" class="headerlink" title="使用目的"></a>使用目的</h2><p>使用LDAP对运维相关用户名密码做统一管理。可以实现一个帐号登录多个不同系统。</p>
<h2 id="LDAP部署"><a href="#LDAP部署" class="headerlink" title="LDAP部署"></a>LDAP部署</h2><h3 id="LDAP-Server端安装"><a href="#LDAP-Server端安装" class="headerlink" title="LDAP Server端安装"></a>LDAP Server端安装</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# yum install -y openldap openldap-clients openldap-servers migrationtools</span><br></pre></td></tr></table></figure>
<h3 id="LDAP-配置"><a href="#LDAP-配置" class="headerlink" title="LDAP 配置"></a>LDAP 配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#配置 OpenLDAP Server</span><br><span class="line">[root@object1 ~]# vim &#x2F;etc&#x2F;openldap&#x2F;slapd.d&#x2F;cn\&#x3D;config&#x2F;olcDatabase\&#x3D;\&#123;2\&#125;hdb.ldif </span><br><span class="line">dn: olcDatabase&#x3D;&#123;2&#125;hdb</span><br><span class="line"></span><br><span class="line">修改两行</span><br><span class="line">olcSuffix: dc&#x3D;hyman,dc&#x3D;com</span><br><span class="line">olcRootDN: cn&#x3D;Manager,dc&#x3D;hyman,dc&#x3D;com</span><br><span class="line">新增一行</span><br><span class="line">olcRootPW: 123456</span><br><span class="line"></span><br><span class="line">#配置 Monitoring Database</span><br><span class="line">[root@object1 ~]# vim &#x2F;etc&#x2F;openldap&#x2F;slapd.d&#x2F;cn\&#x3D;config&#x2F;olcDatabase\&#x3D;\&#123;1\&#125;monitor.ldif</span><br><span class="line">dn: olcDatabase&#x3D;&#123;1&#125;monitor</span><br><span class="line"></span><br><span class="line">修改一行</span><br><span class="line">olcAccess: &#123;0&#125;to * by dn.base&#x3D;&quot;gidNumber&#x3D;0+uidNumber&#x3D;0,cn&#x3D;peercred,cn&#x3D;extern</span><br><span class="line"> al,cn&#x3D;auth&quot; read by dn.base&#x3D;&quot;cn&#x3D;Manager,dc&#x3D;hyman,dc&#x3D;com&quot; read by * none</span><br><span class="line"></span><br><span class="line">#初始化 LDAP database</span><br><span class="line">[root@object1 ~]# cp &#x2F;usr&#x2F;share&#x2F;openldap-servers&#x2F;DB_CONFIG.example &#x2F;var&#x2F;lib&#x2F;ldap&#x2F;DB_CONFIG</span><br><span class="line">[root@object1 ~]# chown -R ldap.ldap &#x2F;var&#x2F;lib&#x2F;ldap&#x2F;</span><br><span class="line"></span><br><span class="line">#测试 configuration</span><br><span class="line">[root@object1 ~]# slaptest -u</span><br><span class="line"></span><br><span class="line">#启动服务并开机自启</span><br><span class="line">[root@object1 ~]# systemctl start slapd</span><br><span class="line">[root@object1 ~]# systemctl enable slapd</span><br><span class="line"></span><br><span class="line">#查看状态</span><br><span class="line">[root@object1 ~]# netstat -lt | grep ldap</span><br><span class="line"></span><br><span class="line">#将所有的配置LDAP server, 添加到LDAP schemas中</span><br><span class="line">[root@object1 ~]# cd &#x2F;etc&#x2F;openldap&#x2F;schema&#x2F;</span><br><span class="line"> ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f cosine.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f nis.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f collective.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f corba.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f core.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f duaconf.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f dyngroup.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f inetorgperson.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f java.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f ppolicy.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f pmi.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f openldap.ldif</span><br><span class="line">[root@object1 schema]# ldapadd -Y EXTERNAL -H ldapi:&#x2F;&#x2F;&#x2F; -D &quot;cn&#x3D;config&quot; -f misc.ldif</span><br><span class="line"></span><br><span class="line">#使用Migration Tools 创建 LDAP DIT</span><br><span class="line">[root@object1 schema]# cd &#x2F;usr&#x2F;share&#x2F;migrationtools&#x2F;</span><br><span class="line">[root@object1 migrationtools]# vim migrate_common.ph</span><br><span class="line">修改61行</span><br><span class="line">$NAMINGCONTEXT&#123;&#39;group&#39;&#125;             &#x3D; &quot;ou&#x3D;Groups&quot;;</span><br><span class="line">修改71行</span><br><span class="line">$DEFAULT_MAIL_DOMAIN &#x3D; &quot;hyman.com&quot;;</span><br><span class="line">修改74行</span><br><span class="line">$DEFAULT_BASE &#x3D; &quot;dc&#x3D;hyman,dc&#x3D;com&quot;;</span><br><span class="line">修改90行</span><br><span class="line">$EXTENDED_SCHEMA &#x3D; 1;</span><br><span class="line"></span><br><span class="line">#创建 base.ldif</span><br><span class="line">[root@object1 migrationtools]# .&#x2F;migrate_base.pl &gt; &#x2F;root&#x2F;base.ldif</span><br><span class="line"></span><br><span class="line">#导入LDAP database</span><br><span class="line">[root@object1 migrationtools]# ldapadd -x -W -D &quot;cn&#x3D;Manager,dc&#x3D;hyman,dc&#x3D;com&quot; -f &#x2F;root&#x2F;base.ldif</span><br><span class="line"></span><br><span class="line">#创建用户和用户组，并将其从本地数据库迁移到LDAP数据库</span><br><span class="line">[root@object1 migrationtools]# mkdir &#x2F;home&#x2F;guests</span><br><span class="line">[root@object1 migrationtools]# useradd -d &#x2F;home&#x2F;guests&#x2F;ldapuser1 ldapuser1</span><br><span class="line">[root@object1 migrationtools]# useradd -d &#x2F;home&#x2F;guests&#x2F;ldapuser2 ldapuser2</span><br><span class="line">[root@object1 migrationtools]# echo &quot;password&quot; | passwd --stdin ldapuser1</span><br><span class="line">[root@object1 migrationtools]# echo &quot;password&quot; | passwd --stdin ldapuser2</span><br><span class="line"></span><br><span class="line">#过滤掉这些用户和组和密码从&#x2F;etc&#x2F;shadow到不同的文件</span><br><span class="line">[root@object1 ~]# getent passwd | tail -n 5 &gt; &#x2F;root&#x2F;users</span><br><span class="line">[root@object1 ~]# getent shadow | tail -n 5 &gt; &#x2F;root&#x2F;shadow</span><br><span class="line">[root@object1 ~]# getent group | tail -n 5 &gt; &#x2F;root&#x2F;groups</span><br><span class="line"></span><br><span class="line">#创建这些用户使用migrationtools</span><br><span class="line">[root@object1 ~]# cd &#x2F;usr&#x2F;share&#x2F;migrationtools&#x2F;</span><br><span class="line">[root@object1 migrationtools]# vim migrate_passwd.pl </span><br><span class="line">修改188行</span><br><span class="line">把 &#x2F;etc&#x2F;shadow 替换为 &#x2F;root&#x2F;shadow</span><br><span class="line"></span><br><span class="line">[root@object1 migrationtools]# .&#x2F;migrate_passwd.pl &#x2F;root&#x2F;users &gt; users.ldif</span><br><span class="line">[root@object1 migrationtools]# .&#x2F;migrate_passwd.pl &#x2F;root&#x2F;groups &gt; groups.ldif</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#上传这些用户和组LDAP数据库</span><br><span class="line">[root@object1 ~]# ldapadd -x -W -D &quot;cn&#x3D;Manager,dc&#x3D;hyman,dc&#x3D;com&quot; -f users.ldif</span><br><span class="line">[root@object1 ~]# ldapadd -x -W -D &quot;cn&#x3D;Manager,dc&#x3D;hyman,dc&#x3D;com&quot; -f groups.ldif</span><br><span class="line"></span><br><span class="line">#所有记录搜索LDAP DIT</span><br><span class="line">[root@object1 ~]# ldapsearch -x -b &quot;dc&#x3D;hyman,dc&#x3D;com&quot; -H ldap:&#x2F;&#x2F;127.0.0.1</span><br></pre></td></tr></table></figure>

<h2 id="LDAP-客户端验证"><a href="#LDAP-客户端验证" class="headerlink" title="LDAP 客户端验证"></a>LDAP 客户端验证</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@block1 ~]# yum install -y nss-pam*</span><br><span class="line">[root@block1 ~]# authconfig-tui </span><br><span class="line">选择第二个Use LDAP</span><br><span class="line">[root@block1 ~]# su ldaduser1</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>LDAP</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS 7 安装KVM虚拟机</title>
    <url>/posts/c1929220.html</url>
    <content><![CDATA[<p>kvm相关安装包及其作用<br>qemu-kvm    主要的KVM程序包<br>python-virtinst    创建虚拟机所需要的命令行工具和程序库<br>virt-manager    GUI虚拟机管理工具<br>virt-top    虚拟机统计命令<br>virt-viewer    GUI连接程序，连接到已配置好的虚拟机<br>libvirt    C语言工具包，提供libvirt服务<br>libvirt-client    为虚拟客户机提供的C语言工具包<br>virt-install    基于libvirt服务的虚拟机创建命令<br>bridge-utils    创建和管理桥接设备的工具</p>
<a id="more"></a>
<h2 id="验证CPU是否支持KVM；如果结果中有vmx（Intel）或svm-AMD-字样，就说明CPU的支持的。"><a href="#验证CPU是否支持KVM；如果结果中有vmx（Intel）或svm-AMD-字样，就说明CPU的支持的。" class="headerlink" title="验证CPU是否支持KVM；如果结果中有vmx（Intel）或svm(AMD)字样，就说明CPU的支持的。"></a>验证CPU是否支持KVM；如果结果中有vmx（Intel）或svm(AMD)字样，就说明CPU的支持的。</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# egrep &#39;(vmx|svm)&#39; &#x2F;proc&#x2F;cpuinfo</span><br></pre></td></tr></table></figure>

<h2 id="安装KVM及其依赖项"><a href="#安装KVM及其依赖项" class="headerlink" title="安装KVM及其依赖项"></a>安装KVM及其依赖项</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# yum install qemu-kvm virt-install bridge-utils libvirt virt-install virt-manager -y</span><br></pre></td></tr></table></figure>

<h2 id="验证安装结果"><a href="#验证安装结果" class="headerlink" title="验证安装结果"></a>验证安装结果</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# lsmod | grep kvm</span><br><span class="line">kvm_intel             162153  0 </span><br><span class="line">kvm                   525409  1 kvm_intel</span><br></pre></td></tr></table></figure>

<h2 id="开启kvm服务，并且设置其开机自动启动"><a href="#开启kvm服务，并且设置其开机自动启动" class="headerlink" title="开启kvm服务，并且设置其开机自动启动"></a>开启kvm服务，并且设置其开机自动启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# systemctl start libvirtd</span><br><span class="line">[root@object1 ~]# systemctl enable libvirtd</span><br></pre></td></tr></table></figure>

<h2 id="配置网桥模式"><a href="#配置网桥模式" class="headerlink" title="配置网桥模式"></a>配置网桥模式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;创建 ifcfg-br0 文件</span><br><span class="line">[root@object1 ~]# cat &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-br0 </span><br><span class="line">BOOTPROTO&#x3D;&quot;static&quot;</span><br><span class="line">DEVICE&#x3D;&quot;br0&quot;</span><br><span class="line">NAME&#x3D;&quot;br0&quot;</span><br><span class="line">TYPE&#x3D;&quot;Bridge&quot;</span><br><span class="line">ONBOOT&#x3D;&quot;yes&quot;</span><br><span class="line">IPADDR&#x3D;&quot;192.168.1.226&quot;</span><br><span class="line">NETMASK&#x3D;&quot;255.255.255.0&quot;</span><br><span class="line">GATEWAY&#x3D;&quot;192.168.1.200&quot;</span><br><span class="line">DNS1&#x3D;&quot;114.114.114.114&quot;</span><br><span class="line">DNS2&#x3D;&quot;8.8.8.8&quot;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;修改ifcfg-eno16777736 文件</span><br><span class="line">[root@object1 ~]# cat &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eno16777736 </span><br><span class="line">#TYPE&#x3D;&quot;Ethernet&quot;</span><br><span class="line">#BOOTPROTO&#x3D;&quot;static&quot;</span><br><span class="line">#DEFROUTE&#x3D;&quot;yes&quot;</span><br><span class="line">#PEERDNS&#x3D;&quot;yes&quot;</span><br><span class="line">#PEERROUTES&#x3D;&quot;yes&quot;</span><br><span class="line">#IPV4_FAILURE_FATAL&#x3D;&quot;no&quot;</span><br><span class="line">#IPV6INIT&#x3D;&quot;yes&quot;</span><br><span class="line">#IPV6_AUTOCONF&#x3D;&quot;yes&quot;</span><br><span class="line">#IPV6_DEFROUTE&#x3D;&quot;yes&quot;</span><br><span class="line">#IPV6_PEERDNS&#x3D;&quot;yes&quot;</span><br><span class="line">#IPV6_PEERROUTES&#x3D;&quot;yes&quot;</span><br><span class="line">#IPV6_FAILURE_FATAL&#x3D;&quot;no&quot;</span><br><span class="line">#NAME&#x3D;&quot;eno16777736&quot;</span><br><span class="line">#UUID&#x3D;&quot;f3f3b4f8-02ad-44f4-83f7-3639f4df3bff&quot;</span><br><span class="line">#DEVICE&#x3D;&quot;eno16777736&quot;</span><br><span class="line">#ONBOOT&#x3D;&quot;yes&quot;</span><br><span class="line">#IPADDR&#x3D;192.168.1.226</span><br><span class="line">#NETMASK&#x3D;255.255.255.0</span><br><span class="line">#GATEWAY&#x3D;192.168.1.200</span><br><span class="line">#PEERDNS&#x3D;&quot;yes&quot;</span><br><span class="line">#DNS1&#x3D;8.8.8.8</span><br><span class="line">NAME&#x3D;&quot;eno16777736&quot;</span><br><span class="line">UUID&#x3D;&quot;f3f3b4f8-02ad-44f4-83f7-3639f4df3bff&quot;</span><br><span class="line">DEVICE&#x3D;&quot;eno16777736&quot;</span><br><span class="line">BRIDGE&#x3D;&quot;br0&quot;</span><br><span class="line">ONBOOT&#x3D;&quot;yes&quot;</span><br></pre></td></tr></table></figure>

<h2 id="重启网络服务"><a href="#重启网络服务" class="headerlink" title="重启网络服务"></a>重启网络服务</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# systemctl restart network</span><br><span class="line">[root@object1 ~]# brctl show</span><br><span class="line">bridge name	bridge id		STP enabled	interfaces</span><br><span class="line">br0		8000.000c29e16c76	no		eno16777736</span><br><span class="line">virbr0		8000.000000000000	yes</span><br></pre></td></tr></table></figure>

<h2 id="安装虚拟机"><a href="#安装虚拟机" class="headerlink" title="安装虚拟机"></a>安装虚拟机</h2><p>//下载cirros linux，下载地址：<a href="http://download.cirros-cloud.net/">http://download.cirros-cloud.net/</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# virt-install -n test001 -r 2048 --disk &#x2F;home&#x2F;test.img,format&#x3D;qcow2,size&#x3D;1 --network bridge&#x3D;br0 --os-type&#x3D;linux --os-variant&#x3D;rhel7.2 --cdrom &#x2F;root&#x2F;cirros-0.3.5-x86_64-disk.img --vnc --vncport&#x3D;5900 --vnclisten&#x3D;0.0.0.0</span><br></pre></td></tr></table></figure>

<h2 id="使用VNC-Viewer连接该虚拟机"><a href="#使用VNC-Viewer连接该虚拟机" class="headerlink" title="使用VNC Viewer连接该虚拟机"></a>使用VNC Viewer连接该虚拟机</h2><p>官网下载：<a href="https://www.realvnc.com/download/vnc/">https://www.realvnc.com/download/vnc/</a></p>
<p>//通过图形界面操作</p>
<h2 id="安装X-X-Window-System"><a href="#安装X-X-Window-System" class="headerlink" title="安装X(X Window System)"></a>安装X(X Window System)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# yum groupinstall &quot;X Window System&quot; -y</span><br></pre></td></tr></table></figure>

<h2 id="安装GNOME-GNOME-Desktop"><a href="#安装GNOME-GNOME-Desktop" class="headerlink" title="安装GNOME(GNOME Desktop)"></a>安装GNOME(GNOME Desktop)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# yum groupinstall &quot;GNOME Desktop&quot; -y</span><br></pre></td></tr></table></figure>

<h2 id="使用virt-manager管理kvm"><a href="#使用virt-manager管理kvm" class="headerlink" title="使用virt-manager管理kvm"></a>使用virt-manager管理kvm</h2><p>//本地需要安装xmanager和xshell工具 ，并使用xshell建立连接时勾选x11转移。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# virt-manager</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>CentOS 7.x 卸载 iptables后，导致网络无法连接</title>
    <url>/posts/67cbc615.html</url>
    <content><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>生产执行yum remove iptables时，以为对系统没有影响，卸载后重启系统无法远程连接，查看原因是相关依赖包一并被删除，未仔细查看卸载信息</p>
<a id="more"></a>

<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>VNC方式登陆服务器，先手动配置服务器网络：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ifconfig eth0 10.81.54.16 netmask 255.255.252.0</span><br><span class="line">route add -net 0.0.0.0&#x2F;0 gw 47.97.51.247</span><br><span class="line">route add -net 10.0.0.0&#x2F;0 gw 10.81.55.247</span><br><span class="line">route add -net 100.64.0.0&#x2F;10 gw 10.81.55.247</span><br></pre></td></tr></table></figure>

<p>服务器网络配置好之后，使用 yum install 方式将被卸载的安装包再安装回来，当然这时如果不需要 iptables 服务可以不用再安装iptables</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y initscripts dhclient</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS 7通过yum安装ansible</title>
    <url>/posts/32a16d14.html</url>
    <content><![CDATA[<h1 id="一、ansible介绍"><a href="#一、ansible介绍" class="headerlink" title="一、ansible介绍"></a>一、ansible介绍</h1><h2 id="1、ansible-简介"><a href="#1、ansible-简介" class="headerlink" title="1、ansible 简介"></a>1、ansible 简介</h2><p>Ansible官方的 title 是“Ansible is Simple IT Automation”——简单的自动化IT工具。Ansible是一款为类Unix系统开发的自由开源的配置和自动化工具。它用Python写成，类似于Chef和Puppet，但是有一个不同的优点是我们不需要在节点中安装任何客户端。它使用SSH来和节点进行通信。</p>
<a id="more"></a>
<h2 id="2、ansible-特点"><a href="#2、ansible-特点" class="headerlink" title="2、ansible 特点"></a>2、ansible 特点</h2><p>（1） No agents：不需要在被管控主机上安装任意客户端；<br>（2） No server：无服务器端，使用时直接运行命令即可；<br>（3） Modules in any languages：基于模块工作，可使用任意语言开发模块<br>（4） YAML，not code：使用yaml语言定制剧本playbook；<br>（5） SSH by default：基于SSH工作；<br>（6） Strong multi-tier solution：可实现多级指挥；</p>
<h1 id="二、Ansible安装使用"><a href="#二、Ansible安装使用" class="headerlink" title="二、Ansible安装使用"></a>二、Ansible安装使用</h1><h2 id="1、-设置EPEL仓库"><a href="#1、-设置EPEL仓库" class="headerlink" title="1、 设置EPEL仓库"></a>1、 设置EPEL仓库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# rpm -iUvh http:&#x2F;&#x2F;dl.fedoraproject.org&#x2F;pub&#x2F;epel&#x2F;7&#x2F;x86_64&#x2F;e&#x2F;epel-release-7-9.noarch.rpm</span><br></pre></td></tr></table></figure>

<h2 id="2、使用yum安装Ansible"><a href="#2、使用yum安装Ansible" class="headerlink" title="2、使用yum安装Ansible"></a>2、使用yum安装Ansible</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# yum install -y ansible</span><br></pre></td></tr></table></figure>

<h2 id="3、设置用于节点鉴权的SSH密钥"><a href="#3、设置用于节点鉴权的SSH密钥" class="headerlink" title="3、设置用于节点鉴权的SSH密钥"></a>3、设置用于节点鉴权的SSH密钥</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#在Ansible服务端生成密钥</span><br><span class="line">[root@object1 ~]# ssh-keygen </span><br><span class="line">#使用ssh-copy-id命令来复制Ansible公钥到节点中</span><br><span class="line">[root@object1 ~]# ssh-copy-id -i root@192.168.1.215</span><br></pre></td></tr></table></figure>

<h2 id="4、为Ansible定义节点的清单"><a href="#4、为Ansible定义节点的清单" class="headerlink" title="4、为Ansible定义节点的清单"></a>4、为Ansible定义节点的清单</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# cat &#x2F;etc&#x2F;ansible&#x2F;hosts</span><br><span class="line">[test]</span><br><span class="line">192.168.1.226</span><br><span class="line">192.168.1.215</span><br></pre></td></tr></table></figure>

<h2 id="5、尝试在Ansible服务端运行命令"><a href="#5、尝试在Ansible服务端运行命令" class="headerlink" title="5、尝试在Ansible服务端运行命令"></a>5、尝试在Ansible服务端运行命令</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#使用ping检查ansible节点的连通性</span><br><span class="line">[root@object1 ~]# ansible -m ping &#39;test&#39;</span><br><span class="line">192.168.1.226 | UNREACHABLE! &#x3D;&gt; &#123;</span><br><span class="line">    &quot;changed&quot;: false, </span><br><span class="line">    &quot;msg&quot;: &quot;Failed to connect to the host via ssh: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).\r\n&quot;, </span><br><span class="line">    &quot;unreachable&quot;: true</span><br><span class="line">&#125;</span><br><span class="line">192.168.1.215 | SUCCESS &#x3D;&gt; &#123;</span><br><span class="line">    &quot;changed&quot;: false, </span><br><span class="line">    &quot;ping&quot;: &quot;pong&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#检查Ansible节点的运行时间（uptime）</span><br><span class="line">[root@object1 ~]# ansible -m command -a &quot;uptime&quot; &quot;test&quot;</span><br><span class="line">192.168.1.226 | UNREACHABLE! &#x3D;&gt; &#123;</span><br><span class="line">    &quot;changed&quot;: false, </span><br><span class="line">    &quot;msg&quot;: &quot;Failed to connect to the host via ssh: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).\r\n&quot;, </span><br><span class="line">    &quot;unreachable&quot;: true</span><br><span class="line">&#125;</span><br><span class="line">192.168.1.215 | SUCCESS | rc&#x3D;0 &gt;&gt;</span><br><span class="line"> 07:11:16 up 42 days, 13:43,  1 user,  load average: 0.00, 0.00, 0.00</span><br><span class="line"></span><br><span class="line">#检查节点的内核版本</span><br><span class="line">[root@object1 ~]# ansible -m command -a &quot;uname -r&quot; &quot;test&quot;</span><br><span class="line">192.168.1.226 | UNREACHABLE! &#x3D;&gt; &#123;</span><br><span class="line">    &quot;changed&quot;: false, </span><br><span class="line">    &quot;msg&quot;: &quot;Failed to connect to the host via ssh: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).\r\n&quot;, </span><br><span class="line">    &quot;unreachable&quot;: true</span><br><span class="line">&#125;</span><br><span class="line">192.168.1.215 | SUCCESS | rc&#x3D;0 &gt;&gt;</span><br><span class="line">2.6.32-573.3.1.el6.x86_64</span><br><span class="line"></span><br><span class="line">#给节点增加用户</span><br><span class="line">[root@object1 ~]# ansible -m command -a &quot;useradd test&quot; &quot;test&quot;</span><br><span class="line">192.168.1.226 | SUCCESS | rc&#x3D;0 &gt;&gt;</span><br><span class="line">192.168.1.215 | SUCCESS | rc&#x3D;0 &gt;&gt;</span><br></pre></td></tr></table></figure>

<h2 id="6、模块的使用"><a href="#6、模块的使用" class="headerlink" title="6、模块的使用"></a>6、模块的使用</h2><p>查看各模块的使用方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible-doc [options] [modules]  ：Show Ansible module documentation </span><br><span class="line">-l 列出所有的ansible模块 </span><br><span class="line">-s 列出该模块的相关指令</span><br><span class="line">可以直接使用 ansible-doc 模块名 来查看模块的使用，如</span><br><span class="line"># ansible-doc htpasswd</span><br></pre></td></tr></table></figure>

<h1 id="三、playbook的使用"><a href="#三、playbook的使用" class="headerlink" title="三、playbook的使用"></a>三、playbook的使用</h1><p>YAML Ain’t Markup Language，即YAML不是XML。不过，在开发的这种语言时，YAML的意思其实是：”Yet Another Markup Language”（仍是一种标记语言）。</p>
<p>YAML的语法和其他高阶语言类似，并且可以简单表达清单、散列表、标量等数据结构。其结构（Structure）通过空格来展示，序列（Sequence）里的项用”-“来代表，Map里的键值对用”:”分隔。</p>
<h2 id="1、playbook使用"><a href="#1、playbook使用" class="headerlink" title="1、playbook使用"></a>1、playbook使用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible-playbook test.yaml</span><br></pre></td></tr></table></figure>

<p>下面就是一个只包含了一个play的playbook，在写playbook的时候，一定要记住在 hosts，yum（模块儿名）等后带空格，否则会报错。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#这个是你选择的主机</span><br><span class="line">- hosts: webservers</span><br><span class="line">#这个是变量</span><br><span class="line">  vars:</span><br><span class="line">    http_port: 80</span><br><span class="line">    max_clients: 200</span><br><span class="line">#远端的执行权限</span><br><span class="line">  remote_user: root  tasks:</span><br><span class="line">#利用yum模块来操作</span><br><span class="line">  - name: ensure apache is at the latest version</span><br><span class="line">    yum: pkg&#x3D;httpd state&#x3D;latest</span><br><span class="line">  - name: write the apache config file    </span><br><span class="line">    template: src&#x3D;&#x2F;srv&#x2F;httpd.j2 dest&#x3D;&#x2F;etc&#x2F;httpd.conf</span><br><span class="line"> #触发重启服务器</span><br><span class="line">    notify:</span><br><span class="line">    - restart apache</span><br><span class="line">  - name: ensure apache is running    </span><br><span class="line">    service: name&#x3D;httpd state&#x3D;started</span><br><span class="line"> #这里的restart apache 和上面的触发是配对的。这就是handlers的作用。相当于tag</span><br><span class="line">  handlers:</span><br><span class="line">    - name: restart apache</span><br><span class="line">      service: name&#x3D;httpd state&#x3D;restarted</span><br></pre></td></tr></table></figure>

<h2 id="2、playbook案例"><a href="#2、playbook案例" class="headerlink" title="2、playbook案例"></a>2、playbook案例</h2><p>corosync.yaml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- hosts: hanodes      #指定要执行任务的主机，可由冒号分隔主机组</span><br><span class="line">  remote_user: root   #指定远程主机上执行任务的用户</span><br><span class="line">  vars:  #定义如下2个变量</span><br><span class="line">    crmsh: crmsh-1.2.6.4.el6.x86_64.rpm  </span><br><span class="line">    pssh: pssh-2.3.1-2.el6.x86_64.rpm</span><br><span class="line">  tasks:    #指定需执行的任务列表，每个task都有其name和使用的模块及参数</span><br><span class="line">    - name: test connection      </span><br><span class="line">      ping:        #ping模块无需执行参数</span><br><span class="line">      remote_user: jason  #在task中指定远程主机上执行任务的用户</span><br><span class="line">      sudo: yes   #使用sudo在远程主机上执行任务</span><br><span class="line">    - name: corosync installing      </span><br><span class="line">      yum: name&#x3D;corosync state&#x3D;present</span><br><span class="line">    - name: pacemaker installing          #定义一个软件安装任务</span><br><span class="line">      yum: name&#x3D;pacemaker state&#x3D;present   #使用yum安装，并配置需安装的软件名（name），及状态（state）</span><br><span class="line">    - name: crmsh rpm packages      </span><br><span class="line">      copy: src&#x3D;&#x2F;ansible&#x2F;corosync&#x2F;packages&#x2F;&#123;&#123; crmsh &#125;&#125; dest&#x3D;&#x2F;tmp&#x2F;&#123;&#123; crmsh &#125;&#125;</span><br><span class="line">    - name: pssh rpm packages      </span><br><span class="line">      copy: src&#x3D;&#x2F;ansible&#x2F;corosync&#x2F;packages&#x2F;&#123;&#123; pssh &#125;&#125; dest&#x3D;&#x2F;tmp&#x2F;&#123;&#123; pssh &#125;&#125;</span><br><span class="line">    - name: crmsh installing      </span><br><span class="line">      command: yum -y reinstall &#x2F;tmp&#x2F;&#123;&#123; crmsh &#125;&#125; &#x2F;tmp&#x2F;&#123;&#123; pssh &#125;&#125;</span><br><span class="line">    - name: authkey configure file      </span><br><span class="line">      copy: src&#x3D;&#x2F;ansible&#x2F;corosync&#x2F;conf&#x2F;authkey dest&#x3D;&#x2F;etc&#x2F;corosync&#x2F;authkey</span><br><span class="line">    - name: authkey mode 400   #定义一个文件权限设置任务</span><br><span class="line">      file: path&#x3D;&#x2F;etc&#x2F;corosync&#x2F;authkey mode&#x3D;400</span><br><span class="line">      notify:   #定义一个通知，当此任务执行时，可以激发响应的handler</span><br><span class="line">        - restart corosync</span><br><span class="line">    - name: corosync.conf configure file      </span><br><span class="line">      copy: src&#x3D;&#x2F;ansible&#x2F;corosync&#x2F;conf&#x2F;corosync.conf dest&#x3D;&#x2F;etc&#x2F;corosync&#x2F;corosync.conf      </span><br><span class="line">      tags:</span><br><span class="line">        - conf      </span><br><span class="line">      notify:</span><br><span class="line">        - restart corosync</span><br><span class="line">    - name: ensure the corosync service startup on boot      </span><br><span class="line">      service: name&#x3D;corosync state&#x3D;started enabled&#x3D;yes</span><br><span class="line">  handlers:   #定义当关注的资源发生变化时，需采取的操作</span><br><span class="line">    - name: restart corosync  #定义一个服务重启任务</span><br><span class="line">      service: name&#x3D;corosync state&#x3D;restarted</span><br></pre></td></tr></table></figure>

<p>heartbeat.yaml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- hosts: hbhosts</span><br><span class="line">    remote_user: root    </span><br><span class="line">    tasks:</span><br><span class="line">        - name: ensure heartbeat latest version     </span><br><span class="line">         yum: name&#x3D;heartbeat state&#x3D;present</span><br><span class="line">        - name: authkeys configure file          </span><br><span class="line">         copy: src&#x3D;&#x2F;root&#x2F;hb_conf&#x2F;authkeys dest&#x3D;&#x2F;etc&#x2F;ha.d&#x2F;authkeys</span><br><span class="line">        - name: authkeys mode 600</span><br><span class="line">          file: path&#x3D;&#x2F;etc&#x2F;ha.d&#x2F;authkeys mode&#x3D;600</span><br><span class="line">          notify:</span><br><span class="line">            - restart heartbeat</span><br><span class="line">        - name: ha.cf configure file          </span><br><span class="line">         copy: src&#x3D;&#x2F;root&#x2F;hb_conf&#x2F;ha.cf dest&#x3D;&#x2F;etc&#x2F;ha.d&#x2F;ha.cf          </span><br><span class="line">         notify:</span><br><span class="line">            - restart heartbeat    </span><br><span class="line">         handlers:</span><br><span class="line">        - name: restart heartbeat        </span><br><span class="line">            service: name&#x3D;heartbeat state&#x3D;restarted</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title>Haproxy配置支持https协议转发</title>
    <url>/posts/2c64f378.html</url>
    <content><![CDATA[<h2 id="haproxy版本"><a href="#haproxy版本" class="headerlink" title="haproxy版本"></a>haproxy版本</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# haproxy -v</span><br><span class="line">HA-Proxy version 1.5.4 2014&#x2F;09&#x2F;02</span><br><span class="line">Copyright 2000-2014 Willy Tarreau &lt;w@1wt.eu&gt;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="生成pem文件"><a href="#生成pem文件" class="headerlink" title="生成pem文件"></a>生成pem文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##申请通过的域名证书，下载后有两个文件1_52yifu.wang_bundle.crt和2_52yifu.wang.key</span><br><span class="line">##两个文件合成一个pem文件即可</span><br><span class="line"></span><br><span class="line">cat 1_52yifu.wang_bundle.crt 52yifu.pem | tree 52yifu.pem</span><br></pre></td></tr></table></figure>

<h2 id="http跳转https"><a href="#http跳转https" class="headerlink" title="http跳转https"></a>http跳转https</h2><p>把所有请求<a href="http://www.52yifu.wang的地址全部跳转为https://www.52yifu.com这个地址。">http://www.52yifu.wang的地址全部跳转为https://www.52yifu.com这个地址。</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">defaults</span><br><span class="line">    mode                    http</span><br><span class="line">    log                     global</span><br><span class="line">    option                  dontlognull</span><br><span class="line">    option http-server-close</span><br><span class="line">    option                  redispatch</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout http-request    10s</span><br><span class="line">    timeout queue           1m</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line">    timeout http-keep-alive 10s</span><br><span class="line">    timeout check           10s</span><br><span class="line">    maxconn                 3000</span><br><span class="line">	tune.ssl.default-dh-param 2048</span><br><span class="line"></span><br><span class="line">frontend app</span><br><span class="line">    bind *:80</span><br><span class="line">	acl is_http hdr_beg(host) 52yifu.wang</span><br><span class="line">	redirect scheme https if !&#123; ssl_fc &#125;</span><br><span class="line">	bind *:443 ssl crt &#x2F;etc&#x2F;haproxy&#x2F;ilanni.com.pem</span><br><span class="line">#	acl cloud   url_sub -i &#x2F;cloud</span><br><span class="line">	</span><br><span class="line">	use_backend app	      if cloud</span><br><span class="line">	use_backend nginx	  if is_http</span><br><span class="line"></span><br><span class="line">	default_backend       app</span><br><span class="line"></span><br><span class="line">backend nginx</span><br><span class="line">	balance source</span><br><span class="line">	server web1 127.0.0.1:86 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3</span><br><span class="line"></span><br><span class="line">backend app</span><br><span class="line">	balance source</span><br><span class="line">	server web1 127.0.0.1:8080 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3</span><br></pre></td></tr></table></figure>

<h2 id="http与https并存配置"><a href="#http与https并存配置" class="headerlink" title="http与https并存配置"></a>http与https并存配置</h2><p>服务器同时开放<a href="http://52yifu.wang和https://52yifu.wang的访问形式。">http://52yifu.wang和https://52yifu.wang的访问形式。</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">defaults</span><br><span class="line">    mode                    http</span><br><span class="line">    log                     global</span><br><span class="line">    option                  dontlognull</span><br><span class="line">    option http-server-close</span><br><span class="line">    option                  redispatch</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout http-request    10s</span><br><span class="line">    timeout queue           1m</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line">    timeout http-keep-alive 10s</span><br><span class="line">    timeout check           10s</span><br><span class="line">    maxconn                 3000</span><br><span class="line">	tune.ssl.default-dh-param 2048</span><br><span class="line"></span><br><span class="line">frontend app</span><br><span class="line">    bind *:80</span><br><span class="line">	acl is_http hdr_beg(host) 52yifu.wang</span><br><span class="line">	redirect scheme https if !&#123; ssl_fc &#125;</span><br><span class="line">	bind *:443 ssl crt &#x2F;etc&#x2F;haproxy&#x2F;ilanni.com.pem</span><br><span class="line">	</span><br><span class="line">	use_backend nginx	  if is_http</span><br><span class="line">	default_backend       tomcat</span><br><span class="line"></span><br><span class="line">backend nginx</span><br><span class="line">	balance source</span><br><span class="line">	server web1 127.0.0.1:86 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3</span><br><span class="line"></span><br><span class="line">backend tomcat</span><br><span class="line">	balance source</span><br><span class="line">	server web1 127.0.0.1:8080 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3</span><br><span class="line"></span><br><span class="line">frontend app443</span><br><span class="line">	bind *:443 ssl crt &#x2F;etc&#x2F;haproxy&#x2F;52yifu.pem</span><br><span class="line">	acl is_443 hdr_beg(host) 52yifu.wang</span><br><span class="line"></span><br><span class="line">	use_backend nginx443	  if is_443</span><br><span class="line">	default_backend           tomcat443</span><br><span class="line"></span><br><span class="line">backend nginx443</span><br><span class="line">	balance source</span><br><span class="line">	server web1 127.0.0.1:86 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3</span><br><span class="line"></span><br><span class="line">backend tomcat443</span><br><span class="line">	balance source</span><br><span class="line">	server web1 127.0.0.1:8080 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3</span><br></pre></td></tr></table></figure>

<h2 id="同台服务器不同域名之间的https与http配置"><a href="#同台服务器不同域名之间的https与http配置" class="headerlink" title="同台服务器不同域名之间的https与http配置"></a>同台服务器不同域名之间的https与http配置</h2><p>同一台服务器对52yifu.wang域名访问的全部跳转为<a href="https://52yifu.wan，而对52yifu.com访问走http协议，也就是跳转到http://52yifu.com这个地址。">https://52yifu.wan，而对52yifu.com访问走http协议，也就是跳转到http://52yifu.com这个地址。</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">defaults</span><br><span class="line">    mode                    http</span><br><span class="line">    log                     global</span><br><span class="line">    option                  dontlognull</span><br><span class="line">    option http-server-close</span><br><span class="line">    option                  redispatch</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout http-request    10s</span><br><span class="line">    timeout queue           1m</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line">    timeout http-keep-alive 10s</span><br><span class="line">    timeout check           10s</span><br><span class="line">    maxconn                 3000</span><br><span class="line">	tune.ssl.default-dh-param 2048</span><br><span class="line"></span><br><span class="line">frontend weblb</span><br><span class="line">	bind *:80</span><br><span class="line">	acl is_com hdr_beg(host) 52yifu.com</span><br><span class="line">	acl is_wang hdr_beg(host) 52yifu.wang</span><br><span class="line">	redirect prefix https:&#x2F;&#x2F;52yifu.wang if is_wang</span><br><span class="line"></span><br><span class="line">	use_backend haproxyserver if is_com</span><br><span class="line"></span><br><span class="line">backend haproxyserver</span><br><span class="line">	balance source</span><br><span class="line">	server web1 127.0.0.1:9090 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3</span><br><span class="line"></span><br><span class="line">frontend weblb443</span><br><span class="line">	bind *:443 ssl crt &#x2F;etc&#x2F;haproxy&#x2F;52yifu.pem</span><br><span class="line">	acl is_443 hdr_beg(host) 52yifu.wang</span><br><span class="line"></span><br><span class="line">	use_backend httpserver443 if is_443</span><br><span class="line"></span><br><span class="line">backend httpserver443</span><br><span class="line">	balance source</span><br><span class="line">	server web1 127.0.0.1:7070 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3</span><br></pre></td></tr></table></figure>

<h2 id="同台服务器多域名均使用https配置"><a href="#同台服务器多域名均使用https配置" class="headerlink" title="同台服务器多域名均使用https配置"></a>同台服务器多域名均使用https配置</h2><p>同一台服务器对52yifu.wang和52yifu.com访问走https是协议。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">defaults</span><br><span class="line">    mode                    http</span><br><span class="line">    log                     global</span><br><span class="line">    option                  dontlognull</span><br><span class="line">    option http-server-close</span><br><span class="line">    option                  redispatch</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout http-request    10s</span><br><span class="line">    timeout queue           1m</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line">    timeout http-keep-alive 10s</span><br><span class="line">    timeout check           10s</span><br><span class="line">    maxconn                 3000</span><br><span class="line">	tune.ssl.default-dh-param 2048</span><br><span class="line"></span><br><span class="line">frontend web80</span><br><span class="line">	bind *:80</span><br><span class="line">	acl is_http hdr_beg(host) 52yifu.wang</span><br><span class="line">	redirect scheme https if !&#123; ssl_fc &#125;</span><br><span class="line"></span><br><span class="line">	bind *:443 ssl crt &#x2F;etc&#x2F;haproxy&#x2F;52yfiu.pem</span><br><span class="line">	acl is_haproxy hdr_beg(host) 52yifu.com</span><br><span class="line">	redirect scheme https if !&#123; ssl_fc &#125;</span><br><span class="line"></span><br><span class="line">	bind *:443 ssl crt &#x2F;etc&#x2F;haproxy&#x2F;52yifu.pem</span><br><span class="line">	use_backend httpserver if is_http</span><br><span class="line">	use_backend haproxyserver if is_haproxy</span><br><span class="line"></span><br><span class="line">backend httpserver</span><br><span class="line">	balance source</span><br><span class="line">	server web1 127.0.0.1:6060 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3</span><br><span class="line"></span><br><span class="line">backend haproxyserver</span><br><span class="line">	balance source</span><br><span class="line">	server web1 127.0.0.1:9090 maxconn 1024 weight 3 check inter 2000 rise 2 fall 3</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>haproxy</category>
      </categories>
      <tags>
        <tag>harpoxy</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos7基于ansible批量部署SSH免密钥</title>
    <url>/posts/a3830fab.html</url>
    <content><![CDATA[<h1 id="应用场景："><a href="#应用场景：" class="headerlink" title="应用场景："></a>应用场景：</h1><p>作为运维，经常会遇到批量管理Linux服务器，为了免去输入远程服务器的账号密码苦恼，可使用SSH的免秘钥登录</p>
<a id="more"></a>
<h1 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h1><h2 id="生成密钥对"><a href="#生成密钥对" class="headerlink" title="生成密钥对"></a>生成密钥对</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -f ~&#x2F;.ssh&#x2F;id_rsa -P &quot;&quot;</span><br></pre></td></tr></table></figure>

<h2 id="添加-etc-ansible-hosts主机"><a href="#添加-etc-ansible-hosts主机" class="headerlink" title="添加/etc/ansible/hosts主机"></a>添加/etc/ansible/hosts主机</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[server]</span><br><span class="line">172.16.2.31</span><br><span class="line">172.16.2.32</span><br></pre></td></tr></table></figure>
<h2 id="批量分发秘钥"><a href="#批量分发秘钥" class="headerlink" title="批量分发秘钥"></a>批量分发秘钥</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible server -m authorized_key -a  &quot;user&#x3D;root key&#x3D;&#39;&#123;&#123; lookup(&#39;file&#39;,&#39;&#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub&#39;) &#125;&#125;&#39;&quot; -k</span><br></pre></td></tr></table></figure>

<img src="/images/20180614100857.png" width="100%" height="100%">

]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>ansible</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+github+next搭建的blog</title>
    <url>/posts/2dc404d6.html</url>
    <content><![CDATA[<hr>
<ol>
<li>搭建环境：Windows 10</li>
<li>软件工具：git、node.js、hexo、Markdownpad2</li>
</ol>
<a id="more"></a>
<hr>
<h1 id="一、环境搭建"><a href="#一、环境搭建" class="headerlink" title="一、环境搭建"></a><strong>一、环境搭建</strong></h1><ul>
<li>安装git</li>
</ul>
<blockquote>
<p>git官网(<a href="http://git-scm.com">http://git-scm.com</a>)</p>
</blockquote>
<ul>
<li>安装Node.js</li>
</ul>
<blockquote>
<p>node.js官网(<a href="https://nodejs.org/en/">https://nodejs.org/en/</a>)</p>
</blockquote>
<h1 id="二、安装和配置Hexo"><a href="#二、安装和配置Hexo" class="headerlink" title="二、安装和配置Hexo"></a><strong>二、安装和配置Hexo</strong></h1><ul>
<li><p>执行cmd命令</p>
<blockquote>
<p>npm install -g hexo-cli</p>
</blockquote>
</li>
<li><p>本地创建Hexo文件夹，本目录下执行cmd命令</p>
<blockquote>
<p>hexo init</p>
<p>npm install</p>
</blockquote>
</li>
<li><p>启动Hexo</p>
<blockquote>
<p>hexo server</p>
</blockquote>
</li>
<li><p>更改hexo主题，在Hexo目录下载next主题</p>
<blockquote>
<p>git clone <a href="https://github.com/iissnan/hexo-theme-next">https://github.com/iissnan/hexo-theme-next</a> themes/next</p>
</blockquote>
</li>
<li><p>修改Hexo配置文件_config.yml</p>
<blockquote>
<p>theme: next</p>
</blockquote>
</li>
<li><p>重启Hexo，基本更改过来了，其他修改具体查看<a href="http://theme-next.iissnan.com/getting-started.html">GITBUB</a></p>
</li>
<li><p>创建文章，执行cmd命令</p>
</li>
</ul>
<blockquote>
<p>hexo new 文章主题</p>
</blockquote>
<ul>
<li><p>执行命令后，在文件下的source_posts，自动生成以后缀为md的文件，修改md内容</p>
</li>
<li><p>生成html文件，执行cmd命令</p>
</li>
</ul>
<blockquote>
<p>hexo d -g</p>
</blockquote>
<ul>
<li>自动会生存静态文件在public文件夹下，把里面的文件全部上传至自己的github下即可</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Linux几个常用命令使用介绍</title>
    <url>/posts/2fa28e16.html</url>
    <content><![CDATA[<h3 id="grep命令"><a href="#grep命令" class="headerlink" title="grep命令"></a>grep命令</h3><p>文本查找命令, 能够使用正则表达式的方式搜索文本，其搜索对象可以是单个或则多个文件</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 grep [option] [regex] [path]</span><br><span class="line"></span><br><span class="line">-o 只按行显示匹配的字符</span><br><span class="line">-c 只输出匹配行的数目</span><br><span class="line">-n 显示匹配行的行号</span><br><span class="line">-v 显示不包含匹配文本的行</span><br><span class="line">-i 不区分大小写 (grep是大小写敏感的)</span><br><span class="line">-R 文件夹下递归搜索</span><br><span class="line">-l 只显示匹配的文件名 </span><br><span class="line">-H 显示文件名</span><br><span class="line">-A NUM(after)显示匹配的后几行</span><br><span class="line">-B NUM(before)显示匹配的前几行</span><br><span class="line">-C NUM显示匹配的前后几行 </span><br><span class="line">–color 标出颜色</span><br></pre></td></tr></table></figure>

<h3 id="ls命令"><a href="#ls命令" class="headerlink" title="ls命令"></a>ls命令</h3><p>ls是命令行中用的最多的命令之一了，用于显示目录下的文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 ls [option]</span><br><span class="line"></span><br><span class="line">-a 列出所有文件，包括’.’开头的隐藏文件</span><br><span class="line">-h 使打印结果易于使用者查看(human readable)</span><br><span class="line">-l 列出文件的详细信息：创建者，创建时间，读写权限等</span><br><span class="line">-s 显示文件大小</span><br><span class="line">-t 按时间进行文件的排序</span><br><span class="line">-S 以大小进行排序</span><br><span class="line">-r 当前条件逆序</span><br><span class="line">-L 显示文件链接名</span><br><span class="line">-R 将目录中所有文件都递归显示出来</span><br></pre></td></tr></table></figure>

<h3 id="find命令"><a href="#find命令" class="headerlink" title="find命令"></a>find命令</h3><p>文件查找命令,find命令将递归的搜索目录下符合要求的所有文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 find [path] [option] [expression]</span><br><span class="line"></span><br><span class="line">-name 查找名为filename的文件</span><br><span class="line">-perm 查找符合执行权限 -user 按照文件的所属主查找</span><br><span class="line">-mtime -n +n 按照文件的更改时间查找文件，n代表天数</span><br><span class="line">-ctime -n +n 按照创建时间查找</span><br><span class="line">-newer f1 !f2 查更改时间在f1和f2之间的文件 </span><br><span class="line">-size n 查找长度为n块的文件，一块为512 bytes</span><br><span class="line">-depth 使得查找在进入子目录前先行查找完本目录</span><br><span class="line">-prune 查找时忽略某个目录 -type 按文件类型查找，b为块设备，d为目录，f为普通文档</span><br></pre></td></tr></table></figure>
<h3 id="wc命令"><a href="#wc命令" class="headerlink" title="wc命令"></a>wc命令</h3><p>用于统计输入中的字节数，字数，行数并输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 wc [option] [filename]</span><br><span class="line"></span><br><span class="line">-c 统计字节数</span><br><span class="line">-l 统计行数</span><br><span class="line">-m 统计字符数</span><br><span class="line">-w 统计字数，一个字为由空白，跳格或换行字符分隔的字符串</span><br></pre></td></tr></table></figure>

<h3 id="cat命令"><a href="#cat命令" class="headerlink" title="cat命令"></a>cat命令</h3><p>连结命令(Concatenation)，连结多个文本，或者以标准输出形式打印文件的内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 cat [option] [filename]</span><br><span class="line"></span><br><span class="line">-n 队输出的所有行编号</span><br><span class="line">-b 与-n类似，但空行不编号</span><br></pre></td></tr></table></figure>

<h3 id="tail命令"><a href="#tail命令" class="headerlink" title="tail命令"></a>tail命令</h3><p>文本查看命令，可以看文本的最后几行。tail命令的优点在于其内容能够与输入同步更新，非常适用于查看实时日志。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 tail [option] [filename]</span><br><span class="line"></span><br><span class="line">-n number 定位参数，+5表示从第五行开始显示，10或-10表示显示最后10行</span><br><span class="line"></span><br><span class="line">-f 监控文本变化，更新内容</span><br><span class="line"></span><br><span class="line">-k number 从number所指的KB处开始读取</span><br></pre></td></tr></table></figure>

<h3 id="head命令"><a href="#head命令" class="headerlink" title="head命令"></a>head命令</h3><p>该命令与tail命令类似，默认显示文件前两行的内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 head [option] [filename]</span><br><span class="line"></span><br><span class="line">-n number 显示前几行,-5表示文件中除了最后5行之外的所有内容</span><br><span class="line"></span><br><span class="line">-c number 显示前几个字节</span><br></pre></td></tr></table></figure>

<h3 id="du命令"><a href="#du命令" class="headerlink" title="du命令"></a>du命令</h3><p>该命令用于查看系统中文件和目录所占用的空间</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 du [option] [name]</span><br><span class="line"></span><br><span class="line">-h 用human readable的方式显示</span><br><span class="line"></span><br><span class="line">--max-depth&#x3D;number 最大的查询层次</span><br><span class="line"></span><br><span class="line">-a 显示所有文件的大小，默认只显示目录的大小</span><br></pre></td></tr></table></figure>

<h3 id="which和whereis"><a href="#which和whereis" class="headerlink" title="which和whereis"></a>which和whereis</h3><p>which命令的作用是在PATH变量制定的路径中，查找系统命令的位置。<br>whereis命令用于程序名的搜索，且只能搜索｛二进制文件，man说明文件，源代码文件｝。whereis的查询时通过查询系统的数据库文件记录，所以速度比find更快，但由于数据库的更新频率较为缓慢，其结果与实际状况并不一定一致。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-m 只查找说明文件</span><br><span class="line"></span><br><span class="line">-b 只查找二进制文件</span><br></pre></td></tr></table></figure>

<h3 id="sort命令"><a href="#sort命令" class="headerlink" title="sort命令"></a>sort命令</h3><p>sort命令用于对文本进行排序，并将结果输出。其以文本的每一行为单位，从首字符向后，依次按照ascii码值进行比较，最后升序排列。（默认是忽略每行前面空格的）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 sort [option] [filename]</span><br><span class="line"></span><br><span class="line">-u 忽略重复行</span><br><span class="line"></span><br><span class="line">-n 按照数字大小排序</span><br><span class="line"></span><br><span class="line">-r 逆序</span><br><span class="line"></span><br><span class="line">-k start,endstart为比较的起始位置，end为结束位置</span><br></pre></td></tr></table></figure>

<h3 id="netstat命令"><a href="#netstat命令" class="headerlink" title="netstat命令"></a>netstat命令</h3><p>netstat用于输出linux系统的网络情况信息，以前面试的时候还被问过：“如何查看占用某个端口的程序的pid?”，这个问题实际用netstat -anp输出，然后再grep一下即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 netstat [option]</span><br><span class="line"></span><br><span class="line">-a 显示所有socket连接</span><br><span class="line"></span><br><span class="line">-l 显示监控中(listening)的socket连接</span><br><span class="line"></span><br><span class="line">-n 直接使用ip地址，而不使用域名服务器</span><br><span class="line"></span><br><span class="line">-p 显示正在使用socket的程序的pid和名称</span><br><span class="line"></span><br><span class="line">-r 打印路由表</span><br><span class="line"></span><br><span class="line">-t 显示TCP传输协议的连线状况</span><br><span class="line"></span><br><span class="line">-u 显示UDP传输协议的连线状况</span><br><span class="line"></span><br><span class="line">-s 显示网络工作信息统计表</span><br></pre></td></tr></table></figure>

<h3 id="more命令"><a href="#more命令" class="headerlink" title="more命令"></a>more命令</h3><p>more命令用于显示文件的内容，与cat和tail等命令不同的是，more命令是按页显示文件内容，同时具有搜寻字符串的功能。（由于more具有向前翻页功能，因此该命令会加载整个文件）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 more [option] [filename]</span><br><span class="line"></span><br><span class="line">+n 从第n行开始显示</span><br><span class="line"></span><br><span class="line">-n 定义屏幕大小为n行</span><br><span class="line"></span><br><span class="line">+&#x2F;pattern 再显示前按pattern匹配子串并显示</span><br><span class="line"></span><br><span class="line">-s 把连续的多个空行显示为一行</span><br><span class="line"></span><br><span class="line">常用操作命令：</span><br><span class="line"></span><br><span class="line">Enter 向下n行，默认为1行</span><br><span class="line"></span><br><span class="line">Ctrl+F 跳过一屏</span><br><span class="line"></span><br><span class="line">Ctrl+B 返回上一屏</span><br><span class="line"></span><br><span class="line">空格键 向下滚动一屏</span><br><span class="line"></span><br><span class="line">&#x3D; 输出当前行的行号</span><br><span class="line"></span><br><span class="line">在more模式中回车，输入&#x2F;pattern可以持续向下搜索</span><br></pre></td></tr></table></figure>

<h3 id="less命令"><a href="#less命令" class="headerlink" title="less命令"></a>less命令</h3><p>less命令与more命令对应，既可以前后翻看文件，同时还有前后搜索功能，除此之外，less在查看前不会加载整个文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 less [option] [filename]</span><br><span class="line"></span><br><span class="line">－N 显示每行的行号</span><br><span class="line"></span><br><span class="line">-i 忽略搜索时的大小写</span><br><span class="line"></span><br><span class="line">-s 将连续空行显示为一行</span><br><span class="line"></span><br><span class="line">-m 显示百分比</span><br><span class="line"></span><br><span class="line">常用操作命令：</span><br><span class="line"></span><br><span class="line">&#x2F;字符串 向下搜索“字符串”功能</span><br><span class="line"></span><br><span class="line">?字符串 向上搜索“字符串”功能</span><br><span class="line"></span><br><span class="line">n 重复前一个搜索</span><br><span class="line"></span><br><span class="line">空格键 滚动一页</span><br><span class="line"></span><br><span class="line">d 滚动半页</span><br><span class="line"></span><br><span class="line">b 回溯一页</span><br><span class="line"></span><br><span class="line">y 回溯一行</span><br><span class="line"></span><br><span class="line">q 退出less命令</span><br></pre></td></tr></table></figure>

<h3 id="ps命令"><a href="#ps命令" class="headerlink" title="ps命令"></a>ps命令</h3><p>ps命令用来在Linux系统中显示进程的状态快照，其参数选项可谓非常之多。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 ps [option]</span><br><span class="line"></span><br><span class="line">-a 显示所有用户的进程</span><br><span class="line"></span><br><span class="line">-x 显示没有控制终端的进程</span><br><span class="line"></span><br><span class="line">-u 按照用户名称查询进程</span><br><span class="line"></span><br><span class="line">-f 列出全部信息，常和其它选项联用</span><br><span class="line"></span><br><span class="line">-j 用任务格式来显示进程</span><br><span class="line"></span><br><span class="line">-e 显示所有进程</span><br></pre></td></tr></table></figure>

<h3 id="kill命令"><a href="#kill命令" class="headerlink" title="kill命令"></a>kill命令</h3><p>kill命令用于终止指定的进程，其工作原理是通过向进程发送指定的信号。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本格式 kill [params] [pid]</span><br><span class="line"></span><br><span class="line">常用的是：</span><br><span class="line"></span><br><span class="line">kill -9 pid &#x2F;&#x2F;强制终止</span><br><span class="line"></span><br><span class="line">-1 Hup 终端断线</span><br><span class="line"></span><br><span class="line">-2 INT 中断（同Ctrl+c）</span><br><span class="line"></span><br><span class="line">-3 QUIT 退出(同Ctrl+\)</span><br><span class="line"></span><br><span class="line">-15 TERM 终止，是默认的信号，如果应用本身会捕获该信号，则不能终止</span><br><span class="line"></span><br><span class="line">-9 KILL 强制终止</span><br><span class="line"></span><br><span class="line">-18 CONT 继续</span><br><span class="line"></span><br><span class="line">-19 STOP 暂停(同Ctrl+z)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>grep</tag>
        <tag>sed</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes应用程序部署工作流程</title>
    <url>/posts/484d1c8d.html</url>
    <content><![CDATA[<ol>
<li>用户通过“kubectl”部署一个新的应用程序。Kubectl将请求发送到API服务器。</li>
<li>API服务器接收请求并将其存储在数据存储（etcd）中。将请求写入数据存储后，API服务器将完成请求。<a id="more"></a></li>
<li>监视器检测资源变化并向控制器发送通知以对其进行操作。</li>
<li>Controller会检测新应用并创建新的pod以匹配所需的实例数量。将拾取对存储模型的任何更改以创建或删除Pod。</li>
<li>调度程序根据条件为节点分配新的pod。Scheduler决定在集群中的特定节点上运行Pod。调度程序使用节点信息修改模型。</li>
<li>节点上的Kubelet检测到具有自身分配的pod，并通过容器运行时（例如Docker）部署所请求的容器。每个节点都会监视存储，以查看它分配给哪些pod运行。它对分配给它的资源采取必要的操作，如创建/删除Pod。</li>
<li>Kubeproxy管理容器的网络流量 - 包括服务发现和负载平衡。Kubeproxy负责想要进行交互的Pod之间的通信。<!--more-->
<img src="/images/app_deploy_workflow.png" width="100%" height="100%"></li>
</ol>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL备份说明</title>
    <url>/posts/3af5d712.html</url>
    <content><![CDATA[<h2 id="使用规范"><a href="#使用规范" class="headerlink" title="使用规范"></a>使用规范</h2><h3 id="实例级备份恢复"><a href="#实例级备份恢复" class="headerlink" title="实例级备份恢复"></a>实例级备份恢复</h3><p>使用innobackupex，在业务空闲期执行，考虑到IO影响及 FLUSH TABLE WITH READ LOCAK 拷贝非INNODB文件的锁表时间。</p>
<p>常规备份中，使用innobackupex在从库备份执行，在无从库的情况下，允许在业务低峰期对整个实例拷贝。    </p>
<a id="more"></a>
<h3 id="库、表级别备份恢复"><a href="#库、表级别备份恢复" class="headerlink" title="库、表级别备份恢复"></a>库、表级别备份恢复</h3><p>考虑 数据量、磁盘IO情况、恢复难度问题。</p>
<p>mysqldump锁表时间长，备份时间长，但是导入方便，适合数据量小但是表格多 的库/表级别备份。</p>
<p>innobackupex锁表时间短，备份时间短，但是恢复较复杂，需要discord tablespace及 import TABLESPACE，除非允许备份文件成立单个实例，适合表数据量大但表格数量少的库/表级别备份。</p>
<h3 id="SQL结果备份及恢复"><a href="#SQL结果备份及恢复" class="headerlink" title="SQL结果备份及恢复"></a>SQL结果备份及恢复</h3><p>如果是单表简单查询，使用mysqldump，添加where条件，例如：mysqldump -S /tmp/mysql3330.sock -uroot -p –databases db1 –tables tb1 tb2 tb3 -d &gt;/data/backup/3330/mysqldump_20161229.sql 。</p>
<p>如果是复杂SQL查询结果，使用 INTO OUTFILE，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#FIELDS TERMINATED BY &#39;,&#39; 字段间分割符</span><br><span class="line">#OPTIONALLY ENCLOSED BY &#39;&quot;&#39; 将字段包围 对数值型无效</span><br><span class="line">#LINES TERMINATED BY &#39;\n&#39; 换行符</span><br><span class="line"> </span><br><span class="line">#查询导出</span><br><span class="line">select * into outfile &#39;&#x2F;tmp&#x2F;pt.txt&#39; FIELDS TERMINATED BY &#39;,&#39; OPTIONALLY ENCLOSED BY &#39;&quot;&#39; LINES TERMINATED BY &#39;\n&#39; from pt where id &gt;3;</span><br><span class="line"> </span><br><span class="line">#加载数据</span><br><span class="line">load data infile &#39;&#x2F;tmp&#x2F;pt1.txt&#39;  into table pt FIELDS TERMINATED BY &#39;,&#39; OPTIONALLY ENCLOSED BY &#39;&quot;&#39; LINES TERMINATED BY &#39;\n&#39;</span><br></pre></td></tr></table></figure>

<h3 id="表结构备份"><a href="#表结构备份" class="headerlink" title="表结构备份"></a>表结构备份</h3><p>使用mysqldump，添加-d参数。</p>
<h2 id="mysqldump"><a href="#mysqldump" class="headerlink" title="mysqldump"></a>mysqldump</h2><p>支持功能多且全面，但是锁表时间是个风险点，使用时注意，同时，若是5.6版本之前的，要充分考虑buffer pool的使用情况。</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>通过general log查看mysqldump运行原理，详细流程见代码块 mysqldump。</p>
<p>mysqldump运行中，第一步，会检查数据库的配置情况，例如是否设置GTID模式及参数配置；第二步，锁所有表格，只允许读操作；第三步，逐个拷贝表格，生成创建表格上SQL（字符集为binary），再SELECT * FROM 表格 生成数据脚步（字符集为UTF8）；第4步，解锁。</p>
<p>当导出全实例或者大数据库时，这里有2个需要注意到问题：</p>
<ul>
<li>锁表的时间<br>基本可以算是从开始到结束都是锁表期间，不能对数据库进行写操作，只能读<br>线上主库无法支持这么长时间的锁表操作<br>线上从库，应考虑对复制到影响</li>
<li>buffer pool的影响<br>由于是采用SELECT * 生成SQL语句，大量读操作，会把缓存里的数据清理出来，导致热点数据移出，对线上DML操作带来严重影响</li>
</ul>
<p>5.6后版本,新增了young buffer pool，一秒内以这个数据被再次访问，则会进入到buffer pool 的warm区。youny区占buffer pool的3/8，剩下的5/8为warm区，可以有效保证热点数据不被清出。</p>
<h3 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h3><p>以下参数在使用过程中，需要留意，根据实际情况添加：</p>
<ul>
<li><p>–master-data=1 /2<br>生产change master to语句，这里注意，lock table 的时间，会提前到最开始的时候，不过相差的时间段非常小。<br>1 则是生产 change master to语句 不加注释符号，直接执行；<br>2 生成change master to语句，加注释符号</p>
</li>
<li><p>–singe-transaction<br>确保事物一致性，建议在GTID模式添加</p>
</li>
<li><p>–set-gtid-purged=ON / OFF<br>在GTID模式下的dump语句，会自动在备份文件之前生成<br>如果打算把该脚本放在非GTID模式的数据库执行，建议添加 –set-gtid-purged=OFF ，关闭生成purge 或者是去文件中注释掉该语句</p>
</li>
<li><p>-d<br>只导出表结构</p>
</li>
<li><p>–databases<br>不更随–tables的时候，可以指定多个db，如果指定了–tables，则默认第一个是database，其他的是table<br>也就是只允许导多个DB的数据文件，或者导同个DB的多个table文件；不允许到不同DB的某些table文件</p>
</li>
</ul>
<h3 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h3><p>语法主要有以下三类：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Usage: mysqldump [OPTIONS] database [tables]</span><br><span class="line">OR     mysqldump [OPTIONS] --databases [OPTIONS] DB1 [DB2 DB3...]</span><br><span class="line">OR     mysqldump [OPTIONS] --all-databases [OPTIONS]</span><br></pre></td></tr></table></figure>

<h4 id="实例备份恢复"><a href="#实例备份恢复" class="headerlink" title="实例备份恢复"></a>实例备份恢复</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#实例备份</span><br><span class="line">mysqldump -S &#x2F;tmp&#x2F;mysql3330.sock -uroot -p --all-datqabases &gt;&#x2F;data&#x2F;backup&#x2F;3330&#x2F;mysqldump_20161229.sql</span><br><span class="line"> </span><br><span class="line">#实例恢复</span><br><span class="line">#新建实例后，导入脚本</span><br><span class="line">mysql --socket&#x3D;&#x2F;tmp&#x2F;mysql3306.sock -uroot -p &lt; &#x2F;data&#x2F;backup&#x2F;3330&#x2F;mysqldump_20161229.sql</span><br></pre></td></tr></table></figure>

<h4 id="部分备份恢复"><a href="#部分备份恢复" class="headerlink" title="部分备份恢复"></a>部分备份恢复</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定单个或者多个DB备份</span><br><span class="line">mysqldump -S &#x2F;tmp&#x2F;mysql3330.sock -uroot -p db1 db2 db3 &gt;&#x2F;data&#x2F;backup&#x2F;3330&#x2F;mysqldump_20161229.sql</span><br><span class="line">mysqldump -S &#x2F;tmp&#x2F;mysql3330.sock -uroot -p --databases db1 db2 db3 &gt;&#x2F;data&#x2F;backup&#x2F;3330&#x2F;mysqldump_20161229.sql</span><br><span class="line"> </span><br><span class="line">#指定单个或者多个表格备份</span><br><span class="line">mysqldump -S &#x2F;tmp&#x2F;mysql3330.sock -uroot -p --databases db1 --tables tb1 tb2 tb3 &gt;&#x2F;data&#x2F;backup&#x2F;3330&#x2F;mysqldump_20161229.sql</span><br><span class="line">mysqldump -S &#x2F;tmp&#x2F;mysql3330.sock -uroot -p db1 tb1 tb2 tb3 &gt;&#x2F;data&#x2F;backup&#x2F;3330&#x2F;mysqldump_20161229.sql</span><br><span class="line"> </span><br><span class="line">#只导出单个表格的某些行数据</span><br><span class="line">mysqldump -S &#x2F;tmp&#x2F;mysql3330.sock -uroot -pycf.com zero pt --where&#x3D;&#39;1&#x3D;1 limit 2&#39; &gt;&#x2F;data&#x2F;backup&#x2F;3330&#x2F;mysqldump_20161229.sql</span><br><span class="line"> </span><br><span class="line">#只备份表结构，不要表数据</span><br><span class="line">mysqldump -S &#x2F;tmp&#x2F;mysql3330.sock -uroot -p --databases db1 --tables tb1 tb2 tb3 -d &gt;&#x2F;data&#x2F;backup&#x2F;3330&#x2F;mysqldump_20161229.sql</span><br><span class="line"> </span><br><span class="line">#只备份表数据，不要表结构</span><br><span class="line">mysqldump -S &#x2F;tmp&#x2F;mysql3330.sock -uroot -pycf.com zero pt --where&#x3D;&#39;id&gt;3&#39; --no-create-info  &gt;&#x2F;data&#x2F;backup&#x2F;3330&#x2F;mysqldump_20161229.sql</span><br><span class="line"> </span><br><span class="line">#恢复数据</span><br><span class="line">source &#x2F;data&#x2F;backup&#x2F;3330&#x2F;mysqldump_20161229.sql</span><br></pre></td></tr></table></figure>

<h2 id="PerconaXtraBackup"><a href="#PerconaXtraBackup" class="headerlink" title="PerconaXtraBackup"></a>PerconaXtraBackup</h2><p>PerconaXtraBackup软件中，含有xtrabackup跟innobackupex，xtrabackup中不备份表结构，innobackupex调用xtrabackup子线程后再备份表结构，故常用innobackupex，xtraback不做日常使用。目前支持 Myisam,innodb，可以备份 .frm, .MRG, .MYD, .MYI, .MAD, .MAI, .TRG, .TRN, .ARM, .ARZ, .CSM, CSV, .opt, .par, innoDB data 及innobdb log 文件。</p>
<h3 id="innobackupex原理（全量说明）"><a href="#innobackupex原理（全量说明）" class="headerlink" title="innobackupex原理（全量说明）"></a>innobackupex原理（全量说明）</h3><p>对数据库文件进行copy操作，同时建立多一个xtrabackup log 同步mysql的redo线程，copy数据文件结束时，flush table with read lock，拷贝非innodb数据文件的文件，拷贝结束后解锁。原理图见下图（图片来自知数堂）。通过general log查看mysqldump运行原理，详细流程见代码块 innobackupex。</p>
<p><img src="http://images2015.cnblogs.com/blog/608061/201612/608061-20161228162608179-421054534.png" alt=""></p>
<p>这里需要注意2个点：</p>
<ul>
<li><p>锁表时间<br>innobackupex锁表时间是 data文件及log文件copy结束时，才锁表，锁表时长为拷贝non-InnoDB tables and files的时长，相对时间较短，对业务影响小。</p>
</li>
<li><p>大事务<br>copy数据文件的过程中，由于是不锁表，允许数据进行DML操作，这里需要注意，如果这个时候，拷贝的过程中有大事务一直没有提交，界面显示log scanned up，持续copy binlog追上数据库的binlog文件，并且该时间点刚好所有事务已提交（这里测试的时候，如果是单条 insert ，delete，update的大事务，则是要等待单条完成才提交，但是如果是begin事务里边的，不用等待是否commit or rollback，begin里边的单条事务执行结束，则就开始提交，恢复的时候，当作是undo 事务，不会提交该事物，回滚该事务）。大事务容易导致备份时长加长，IO占用。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2016-12-26T15:18:39.627366Z     1659 Connect    root@localhost on  using Socket</span><br><span class="line">2016-12-26T15:18:39.627789Z     1659 Query    SET SESSION wait_timeout&#x3D;2147483</span><br><span class="line">2016-12-26T15:18:39.628193Z     1659 Query    SHOW VARIABLES </span><br><span class="line">#记录LSN号码，开始copy ibd文件</span><br><span class="line">2016-12-26T15:18:55.673740Z     1659 Query    SET SESSION lock_wait_timeout&#x3D;31536000</span><br><span class="line">2016-12-26T15:18:55.674281Z     1659 Query    FLUSH NO_WRITE_TO_BINLOG TABLES</span><br><span class="line">#强制把没有 还没写入binlog 磁盘文件的缓存 强制刷新到磁盘</span><br><span class="line">#开始拷贝数据库文件，这里需要注意，如果这个时候，拷贝的过程中有大事务一直没有提交，则会一直拷贝其产生的 ，界面显示log scanned up，直到copy binlog追上数据库的binlog文件，并且该时间点刚好所有事务已提交（这里测试的时候，如果是单条 insert ，delete，update的大事务，则是要等待单条完成才提交，但是如果是begin事务里边的，不用等待是否commit or rollback，begin里边的单条事务执行结束，则就开始提交，恢复的时候，当作是undo 事务，不会提交该事物，回滚该事务。 ）</span><br><span class="line">2016-12-26T15:18:55.676345Z     1659 Query    FLUSH TABLES WITH READ LOCK</span><br><span class="line">#锁表，只允许读，不允许写及其他架构修改操作</span><br><span class="line">#拷贝除innodb 数据文件外的其他所有文件，包括表结构等，Starting to backup non-InnoDB tables and files</span><br><span class="line">2016-12-26T15:18:59.691409Z     1659 Query    SHOW MASTER STATUS</span><br><span class="line">#记录 备份到的 binlog文件及position位置，这个记录在 xtrabackup_binlog_info 文件，可提供复制使用</span><br><span class="line">2016-12-26T15:18:59.734418Z     1659 Query    SHOW VARIABLES</span><br><span class="line">2016-12-26T15:18:59.754530Z     1659 Query    FLUSH NO_WRITE_TO_BINLOG ENGINE LOGS</span><br><span class="line">2016-12-26T15:18:59.968452Z     1659 Query    UNLOCK TABLES</span><br><span class="line">#解锁，表格恢复可写，架构可修改</span><br><span class="line">2016-12-26T15:18:59.991046Z     1659 Query    SELECT UUID()</span><br><span class="line">2016-12-26T15:19:00.005980Z     1659 Query    SELECT VERSION()</span><br></pre></td></tr></table></figure>

<h3 id="重要参数-1"><a href="#重要参数-1" class="headerlink" title="重要参数"></a>重要参数</h3><h4 id="备份参数"><a href="#备份参数" class="headerlink" title="备份参数"></a>备份参数</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex [--compress] [--compress-threads&#x3D;NUMBER-OF-THREADS] [--compress-chunk-size&#x3D;CHUNK-SIZE]</span><br><span class="line">             [--encrypt&#x3D;ENCRYPTION-ALGORITHM] [--encrypt-threads&#x3D;NUMBER-OF-THREADS] [--encrypt-chunk-size&#x3D;CHUNK-SIZE]</span><br><span class="line">             [--encrypt-key&#x3D;LITERAL-ENCRYPTION-KEY] | [--encryption-key-file&#x3D;MY.KEY]</span><br><span class="line">             [--include&#x3D;REGEXP]</span><br><span class="line">             [--user&#x3D;NAME]</span><br><span class="line">             [--password&#x3D;WORD] [--port&#x3D;PORT] [--socket&#x3D;SOCKET]</span><br><span class="line">             [--no-timestamp] [--ibbackup&#x3D;IBBACKUP-BINARY]</span><br><span class="line">             [--slave-info] [--galera-info] [--stream&#x3D;tar|xbstream]</span><br><span class="line">             [--defaults-file&#x3D;MY.CNF] [--defaults-group&#x3D;GROUP-NAME]</span><br><span class="line">             [--databases&#x3D;LIST]</span><br><span class="line">             [--no-lock] #不执行FLUSH TABLES WITH READ LOCK，建议不使用，不会拷贝undo及redo文件</span><br><span class="line">             [--no-timestamp]</span><br><span class="line">             [--kill-long-queries-timeout&#x3D;#] </span><br><span class="line">             [--tmpdir&#x3D;DIRECTORY] [--tables-file&#x3D;FILE]</span><br><span class="line">             [--history&#x3D;NAME]</span><br><span class="line">             [--incremental] [--incremental-basedir]</span><br><span class="line">             [--incremental-dir] [--incremental-force-scan] [--incremental-lsn]</span><br><span class="line">             [--incremental-history-name&#x3D;NAME] [--incremental-history-uuid&#x3D;UUID]</span><br><span class="line">             [--close-files] [--compact]  BACKUP-ROOT-DIR</span><br></pre></td></tr></table></figure>

<h4 id="准备还原参数"><a href="#准备还原参数" class="headerlink" title="准备还原参数"></a>准备还原参数</h4><p>根据 BACKUP-DIR/xtrabackup_logfile创建新的logfile，xtrabackup为子进程，不连接数据库服务.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex --apply-log [--use-memory&#x3D;B]</span><br><span class="line">             [--defaults-file&#x3D;MY.CNF]</span><br><span class="line">             [--export] [--redo-only] [--ibbackup&#x3D;IBBACKUP-BINARY]</span><br><span class="line">             BACKUP-DIR</span><br></pre></td></tr></table></figure>

<h4 id="备份目录拷贝参数"><a href="#备份目录拷贝参数" class="headerlink" title="备份目录拷贝参数"></a>备份目录拷贝参数</h4><p>拷贝备份目录到指定目录，备份目录及拷贝目录文件均存在<br>innobackupex –copy-back [–defaults-file=MY.CNF] [–defaults-group=GROUP-NAME] BACKUP-DIR</p>
<p>移动备份目录到指定目录，备份目录为空<br>innobackupex –move-back [–defaults-file=MY.CNF] [–defaults-group=GROUP-NAME] BACKUP-DIR</p>
<h3 id="使用说明-1"><a href="#使用说明-1" class="headerlink" title="使用说明"></a>使用说明</h3><h4 id="实例备份及恢复"><a href="#实例备份及恢复" class="headerlink" title="实例备份及恢复"></a>实例备份及恢复</h4><h5 id="全量备份"><a href="#全量备份" class="headerlink" title="全量备份"></a>全量备份</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#全量备份 实例备份及恢复</span><br><span class="line">#备份</span><br><span class="line">innobackupex --defaults-file&#x3D;&#x2F;data&#x2F;mysql&#x2F;mysql3330.cnf --user&#x3D;root --password&#x3D;ycf.com --no-timestamp  &#x2F;data&#x2F;backup&#x2F;3330&#x2F;20161229</span><br><span class="line">innobackupex --apply-log  &#x2F;data&#x2F;backup&#x2F;3330&#x2F;20161229</span><br><span class="line"> </span><br><span class="line">#恢复</span><br><span class="line">innobackupex --copy-back --datadir&#x3D;&#x2F;data&#x2F;mysql&#x2F;mysql3350&#x2F;data &#x2F;data&#x2F;backup&#x2F;3330&#x2F;20161229</span><br></pre></td></tr></table></figure>

<h5 id="增量备份恢复"><a href="#增量备份恢复" class="headerlink" title="增量备份恢复"></a>增量备份恢复</h5><p>#增量备份</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex --defaults-file&#x3D;&#x2F;data&#x2F;mysql&#x2F;mysql3376.cnf --user&#x3D;root --password&#x3D;ycf.com --no-timestamp --incremental-basedir&#x3D;&#x2F;data&#x2F;backup&#x2F;3330&#x2F;20161229 --incremental &#x2F;data&#x2F;backup&#x2F;mysql3376&#x2F;20161230diff</span><br><span class="line"> </span><br><span class="line">innobackupex --defaults-file&#x3D;&#x2F;data&#x2F;mysql&#x2F;mysql3376.cnf --user&#x3D;root --password&#x3D;ycf.com --no-timestamp --incremental-basedir&#x3D;&#x2F;data&#x2F;backup&#x2F;3330&#x2F;20161230diff --incremental &#x2F;data&#x2F;backup&#x2F;mysql3376&#x2F;20161231diff</span><br><span class="line"> </span><br><span class="line">#增量恢复</span><br><span class="line">#现在完整备份文件中中应用redo日志，记得是redo-only， redo-only， redo-only， redo-only， 不是readonly，打死记得，不要乱来！！！！！！</span><br><span class="line">innobackupex --apply-log --redo-only &#x2F;data&#x2F;backup&#x2F;3330&#x2F;20161229</span><br><span class="line"> </span><br><span class="line">#应用第一个增量备份文件的redo日志到完整备份文件夹中</span><br><span class="line">innobackupex --apply-log --redo-only &#x2F;data&#x2F;backup&#x2F;3330&#x2F;20161229 --incremental-dir&#x3D;&#x2F;data&#x2F;backup&#x2F;mysql3376&#x2F;20161230diff</span><br><span class="line"> </span><br><span class="line">#应用最后一个增量备份文件的redo日志到完整备份文件夹中，可以直接apply-log</span><br><span class="line">innobackupex --apply-log &#x2F;data&#x2F;backup&#x2F;3330&#x2F;20161229 --incremental-dir&#x3D;&#x2F;data&#x2F;backup&#x2F;mysql3376&#x2F;20161231diff</span><br></pre></td></tr></table></figure>

<h5 id="部分备份"><a href="#部分备份" class="headerlink" title="部分备份"></a>部分备份</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#部分备份</span><br><span class="line">#指定数据库备份</span><br></pre></td></tr></table></figure>
<p>innobackupex –defaults-file=/data/mysql/mysql3330.cnf –databases=’zero mysql’ –user=root –password=ycf.com –no-timestamp /data/backup/3330/20161202</p>
<p>#指定表格备份<br>#3.1 –include 使用正则表达式</p>
<p>#3.2 –table-file 备份的完整表名写在file文件中<br>vim /tmp/backupfile #每行写一个库名，或者一个表的全名（database.table），写完库名或者表名后，千万不要有空格或者其他空白符号，会导致识别不了该表格或者库名，从而导致跳过<br>innobackupex –defaults-file=/data/mysql/mysql3330.cnf –tables-file=/tmp/backupfile –user=root –password=ycf.com –no-timestamp  /data/backup/3330/20161204</p>
<p>#3.3 –databases 完整库名和表名写在一起，用空格隔开<br>innobackupex –defaults-file=/data/mysql/mysql3330.cnf –user=root –password=ycf.com –no-timestamp –databases=zero.s1 /data/backup/3330/20161229</p>
<p>#指定表格恢复(开启独立表空间)<br>#首先要自己现在需要恢复的数据库上，创建该表格，然后discard tablespace,拷贝ibd文件过来，chown 文件所有者及用户组为mysql，再 import tablespace。<br>#如果有大量表格，用这个操作就比较麻烦，需要一个个来创建，包括指定数据库，也是这样处理，整个数据库先创建之后，在一个个表格discard，再import。<br>ALTER TABLE S1 DISCARD TABLESPACE;<br>ALTER TABLE S1 import TABLESPACE;</p>
<pre><code></code></pre>]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>备份</tag>
      </tags>
  </entry>
  <entry>
    <title>Python基本算法</title>
    <url>/posts/eaad5d7d.html</url>
    <content><![CDATA[<h2 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h2><p>用来评估算法运行效率的一个东西</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def func():</span><br><span class="line">    print(&quot;hello world&quot;)</span><br><span class="line"> </span><br><span class="line">def func1(n):</span><br><span class="line">    for i in range(n):</span><br><span class="line">        print(&quot;hello world&quot;)</span><br><span class="line"> </span><br><span class="line">def func2(n):</span><br><span class="line">    for i in range(n):</span><br><span class="line">        for j in range(n):</span><br><span class="line">            print(&quot;hello world&quot;)</span><br><span class="line"> </span><br><span class="line">def func3(n):</span><br><span class="line">    for i in range(n):</span><br><span class="line">        for j in range(n):</span><br><span class="line">            for k in range(n):</span><br><span class="line">                print(&quot;hello world&quot;)</span><br><span class="line"></span><br><span class="line">时间复杂度分别为：O(1),O(n),O(n^2),O(n^3)</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<p>特殊的时间复杂度</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def func4(n):</span><br><span class="line">    while n &gt;1:</span><br><span class="line">        print(n)</span><br><span class="line">        n &#x3D; n&#x2F;&#x2F;2</span><br><span class="line"></span><br><span class="line">时间复杂度叫O(logn)</span><br></pre></td></tr></table></figure>
<p>时间复杂度是用来估计算法运行时间的一个式子（单位）</p>
<p>一般来说，时间复杂度高的算法比复杂度低的算法慢</p>
<p>常见的时间复杂度（按效率排序）</p>
<p>O(1)&lt;O(logn)&lt;O(n)&lt;O(nlogn)&lt;O(n^2)&lt;O(n*nlogn)&lt;O(n^3)</p>
<p>通常简单的判断时间复杂度的方法：</p>
<p>循环减半的过程——O(logn)</p>
<p>基层循环就是n的几次方的复杂度</p>
<h2 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h2><p>用来评估算法内存占用大小的一个式子</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h3><p>二分查找又叫折半查找，二分查找应该属于减治技术的成功应用。所谓减治法，就是将原问题分解成若干个子问题后，利用了规模为n的原问题的解与较小规模（通常是n/2）的子问题的解之间的关系。<br>二分查找利用了记录按关键码有序的特点，其基本思想为：在有序表中，取中间记录作为比较对象，若给定值与中间记录的关键码相等，则查找成功；若给定值小于中间记录的关键码，则在中间记录的左半边继续查找；若给定值大于中间记录的关键码，则在中间记录右半边区继续查找。不断重复上述过程，直到查找成功，或所查找的区域无记录，查找失败。<br>二分查找的时间复杂度是O(log(n))，最坏情况下的时间复杂度是O(n)。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在一个排序数组中找一个数，返回该数出现的任意位置，如果不存在，返回-1</span><br><span class="line">样例</span><br><span class="line">给出数组 [1, 2, 2, 4, 5, 5].</span><br><span class="line"></span><br><span class="line">对于 target &#x3D; 2, 返回 1 或者 2.</span><br><span class="line">对于 target &#x3D; 5, 返回 4 或者 5.</span><br><span class="line">对于 target &#x3D; 6, 返回 -1.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line">#coding&#x3D;utf-8</span><br><span class="line"></span><br><span class="line">#自定义函数，实现二分查找，并返回查找结果</span><br><span class="line">def binary_search(find, list1) :</span><br><span class="line">  low &#x3D; 0</span><br><span class="line">  high &#x3D; len(list1)</span><br><span class="line">  while low &lt;&#x3D; high :</span><br><span class="line">    mid &#x3D; (low + high) &#x2F; 2</span><br><span class="line">    if list1[mid] &#x3D;&#x3D; find :</span><br><span class="line">      return mid</span><br><span class="line">    #左半边</span><br><span class="line">    elif list1[mid] &gt; find :</span><br><span class="line">      high &#x3D; mid -1</span><br><span class="line">    #右半边</span><br><span class="line">    else :</span><br><span class="line">      low &#x3D; mid + 1</span><br><span class="line">  #未找到返回-1</span><br><span class="line">  return -1</span><br><span class="line"></span><br><span class="line">list1 &#x3D; [1,2,3,7,8,9,10,5]</span><br><span class="line">#进行二分查找算法前必须保证要查找的序列时有序的，这里假设是升序列表</span><br><span class="line">list1.sort()</span><br><span class="line"></span><br><span class="line">print &quot;原有序列表为:&quot;,list1</span><br><span class="line">try :</span><br><span class="line">  find &#x3D; int(raw_input(&quot;请输入要查找的数：&quot;))</span><br><span class="line">except :</span><br><span class="line">  print &quot;请输入正整数！&quot;</span><br><span class="line">  exit()</span><br><span class="line"></span><br><span class="line">result &#x3D; binary_search(find, list1)</span><br><span class="line">if result !&#x3D; -1 : </span><br><span class="line">  print &quot;要找的元素%d的序号为：%d&quot; %(find,result)</span><br><span class="line">else :</span><br><span class="line">  print &quot;未找到！&quot;</span><br></pre></td></tr></table></figure>

<h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><h4 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h4><p>插入排序的基本操作就是将一个数据插入到已经排好序的有序数据中，从而得到一个新的、个数加一的有序数据，算法适用于少量数据的排序，时间复杂度为O(n^2)。是稳定的排序方法。插入算法把要排序的数组分成两部分：第一部分包含了这个数组的所有元素，但将最后一个元素除外（让数组多一个空间才有插入的位置），而第二部分就只包含这一个元素（即待插入元素）。在第一部分排序完成后，再将这个最后元素插入到已排好序的第一部分中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">list &#x3D; [55,45,345,6,7,2,88,53,12,889,21]</span><br><span class="line">print list</span><br><span class="line"></span><br><span class="line">def insert_sort(list):</span><br><span class="line">        count &#x3D; len(list)</span><br><span class="line">        for i in range(1,count):</span><br><span class="line">                k &#x3D; list[i]</span><br><span class="line">                #print i</span><br><span class="line">                #print k</span><br><span class="line">                j &#x3D; i -1</span><br><span class="line">                #print list[j]</span><br><span class="line">                while j &gt;&#x3D; 0:</span><br><span class="line">                        if list[j] &gt;k:</span><br><span class="line">                                list[j+1]&#x3D;list[j]</span><br><span class="line">                                list[j]&#x3D;k</span><br><span class="line">                        j -&#x3D;1</span><br><span class="line">                        #print list</span><br><span class="line">        return list</span><br><span class="line">print insert_sort(list)</span><br></pre></td></tr></table></figure>

<h4 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h4><p>它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。<br>每一趟只能将一个数归位, 如果有n个数进行排序,只需将n-1个数归位, 也就是说要进行n-1趟操作(已经归位的数不用再比较)<br>缺点: 冒泡排序解决了桶排序浪费空间的问题, 但是冒泡排序的效率特别低</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line"># coding:utf-8</span><br><span class="line"></span><br><span class="line">def bubbleSort(nums):</span><br><span class="line">    for i in range(len(nums)-1):    # 这个循环负责设置冒泡排序进行的次数</span><br><span class="line">        for j in range(len(nums)-i-1):  # ｊ为列表下标</span><br><span class="line">            if nums[j] &gt; nums[j+1]:</span><br><span class="line">                nums[j], nums[j+1] &#x3D; nums[j+1], nums[j]</span><br><span class="line">    return nums</span><br><span class="line"></span><br><span class="line">nums &#x3D; [5,2,45,6,8,2,1]</span><br><span class="line"></span><br><span class="line">print bubbleSort(nums)</span><br></pre></td></tr></table></figure>

<h4 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h4><p>基本思想：第1趟，在待排序记录r1 ~ r[n]中选出最小的记录，将它与r1交换；第2趟，在待排序记录r2 ~ r[n]中选出最小的记录，将它与r2交换；以此类推，第i趟在待排序记录r[i] ~ r[n]中选出最小的记录，将它与r[i]交换，使有序序列不断增长直到全部排序完毕。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line"># coding:utf-8</span><br><span class="line"></span><br><span class="line">def selectSort(nums):</span><br><span class="line">    for i in range(len(nums)):</span><br><span class="line">        max_index &#x3D; 0</span><br><span class="line">        for j in range(len(nums)-i):</span><br><span class="line">            if nums[max_index] &lt; nums[j]:</span><br><span class="line">                max_index &#x3D; j</span><br><span class="line">        nums[max_index], nums[len(nums)-i-1] &#x3D; nums[len(nums)-i-1], nums[max_index]</span><br><span class="line">    return nums</span><br><span class="line"></span><br><span class="line">nums &#x3D; [6,2,54435,3141]</span><br><span class="line">print selectSort(nums)</span><br></pre></td></tr></table></figure>

<h4 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h4><p>通排序非常浪费空间, 比如需要排序的范围在0~2000之间, 需要排序的数是[3,9,4,2000], 同样需要2001个空间</p>
<p>注意: 通排序不能排序小数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line"># coding:utf-8</span><br><span class="line"></span><br><span class="line">def bucketSort(nums):</span><br><span class="line">    # 选择一个最大的数</span><br><span class="line">    max_num &#x3D; max(nums)</span><br><span class="line">    # 创建一个元素全是0的列表, 当做桶</span><br><span class="line">    bucket &#x3D; [0]*(max_num+1)</span><br><span class="line">    # 把所有元素放入桶中, 即把对应元素个数加一</span><br><span class="line">    for i in nums:</span><br><span class="line">        bucket[i] +&#x3D; 1</span><br><span class="line"></span><br><span class="line">    # 存储排序好的元素</span><br><span class="line">    sort_nums &#x3D; []</span><br><span class="line">    # 取出桶中的元素</span><br><span class="line">    for j in range(len(bucket)):</span><br><span class="line">        if bucket[j] !&#x3D; 0:</span><br><span class="line">            for y in range(bucket[j]):</span><br><span class="line">                sort_nums.append(j)</span><br><span class="line"></span><br><span class="line">    return sort_nums</span><br><span class="line"></span><br><span class="line">nums &#x3D; [5,6,3,2,1,65,2,0,8,0]</span><br><span class="line">print bucketSort(nums)</span><br></pre></td></tr></table></figure>

<h4 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h4><p>希尔排序的实质就是分组插入排序，该方法又称缩小增量排序，因DL．Shell于1959年提出而得名。<br>希尔排序，也称递减增量排序算法，是插入排序的一种更高效的改进版本。希尔排序是非稳定排序算法。<br>希尔排序是基于插入排序的以下两点性质而提出改进方法的：<br>插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率<br>但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line"># coding:utf-8</span><br><span class="line"></span><br><span class="line">def shellSort(nums):</span><br><span class="line">    # 设定步长</span><br><span class="line">    step &#x3D; len(nums)&#x2F;2</span><br><span class="line">    while step &gt; 0:</span><br><span class="line">        for i in range(step, len(nums)):</span><br><span class="line">            # 类似插入排序, 当前值与指定步长之前的值比较, 符合条件则交换位置</span><br><span class="line">            while i &gt;&#x3D; step and nums[i-step] &gt; nums[i]:</span><br><span class="line">                nums[i], nums[i-step] &#x3D; nums[i-step], nums[i]</span><br><span class="line">                i -&#x3D; step</span><br><span class="line">        step &#x3D; step&#x2F;2</span><br><span class="line">    return nums</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    nums &#x3D; [9,3,5,8,2,7,1]</span><br><span class="line">    print shellSort(nums)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL自带压力测试工具mysqlslap的使用方法</title>
    <url>/posts/11144fe1.html</url>
    <content><![CDATA[<p>mysqlslap是从5.1.4版开始的一个MySQL官方提供的压力测试工具。通过模拟多个并发客户端访问MySQL来执行压力测试，并输出计时信息。并且能很好的对比多个存储引擎在相同环境下的并发压力性能差别。可以指定SQL语句。如果没有指定SQL语句，mysqlslap会自动生成查询schema的SELECT语句。</p>
<a id="more"></a>
<p>1、查看帮助信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 local]# mysqlslap --help</span><br><span class="line">mysqlslap  Ver 1.0 Distrib 5.7.18, for Linux (x86_64)</span><br><span class="line">Copyright (c) 2005, 2017, Oracle and&#x2F;or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and&#x2F;or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Run a query multiple times against the server.</span><br><span class="line"></span><br><span class="line">Usage: mysqlslap [OPTIONS]</span><br><span class="line"></span><br><span class="line">Default options are read from the following files in the given order:</span><br><span class="line">&#x2F;etc&#x2F;my.cnf &#x2F;etc&#x2F;mysql&#x2F;my.cnf &#x2F;usr&#x2F;etc&#x2F;my.cnf ~&#x2F;.my.cnf </span><br><span class="line">The following groups are read: mysqlslap client</span><br><span class="line">The following options may be given as the first argument:</span><br><span class="line">--print-defaults        Print the program argument list and exit.</span><br><span class="line">--no-defaults           Don&#39;t read default options from any option file,</span><br><span class="line">                        except for login file.</span><br><span class="line">--defaults-file&#x3D;#       Only read default options from the given file #.</span><br><span class="line">--defaults-extra-file&#x3D;# Read this file after the global files are read.</span><br><span class="line">--defaults-group-suffix&#x3D;#</span><br><span class="line">                        Also read groups with concat(group, suffix)</span><br><span class="line">--login-path&#x3D;#          Read this path from the login file.</span><br><span class="line">  -?, --help          Display this help and exit.</span><br><span class="line">  -a, --auto-generate-sql 自动生成测试表和数据</span><br><span class="line">                      Generate SQL where not supplied by file or command line.</span><br><span class="line">  --auto-generate-sql-add-autoincrement 增加auto_increment一列</span><br><span class="line">                      Add an AUTO_INCREMENT column to auto-generated tables.</span><br><span class="line">  --auto-generate-sql-execute-number&#x3D;# 自动生成的查询的个数</span><br><span class="line">                      Set this number to generate a set number of queries to</span><br><span class="line">                      run.</span><br><span class="line">  --auto-generate-sql-guid-primary 增加基于GUID的主键</span><br><span class="line">                      Add GUID based primary keys to auto-generated tables.</span><br><span class="line">  --auto-generate-sql-load-type&#x3D;name 测试语句的类型。取值包括：read，key，write，update和mixed(默认)</span><br><span class="line">                      Specify test load type: mixed, update, write, key, or</span><br><span class="line">                      read; default is mixed.</span><br><span class="line">  --auto-generate-sql-secondary-indexes&#x3D;# 增加二级索引的个数，默认是0</span><br><span class="line">                      Number of secondary indexes to add to auto-generated</span><br><span class="line">                      tables.</span><br><span class="line">  --auto-generate-sql-unique-query-number&#x3D;# 不同查询的数量，默认值是10</span><br><span class="line">                      Number of unique queries to generate for automatic tests.</span><br><span class="line">  --auto-generate-sql-unique-write-number&#x3D;# 不同插入的数量，默认是100</span><br><span class="line">                      Number of unique queries to generate for</span><br><span class="line">                      auto-generate-sql-write-number.</span><br><span class="line">  --auto-generate-sql-write-number&#x3D;# </span><br><span class="line">                      Number of row inserts to perform for each thread (default</span><br><span class="line">                      is 100).</span><br><span class="line">  --commit&#x3D;#          多少条DML后提交一次</span><br><span class="line">					  Commit records every X number of statements.</span><br><span class="line">  -C, --compress      如果服务器和客户端支持都压缩，则压缩信息传递</span><br><span class="line">					  Use compression in server&#x2F;client protocol.</span><br><span class="line">  -c, --concurrency&#x3D;name 模拟N个客户端并发执行select。可指定多个值，以逗号或者 --delimiter 参数指定的值做为分隔符</span><br><span class="line">                      Number of clients to simulate for query to run.</span><br><span class="line">  --create&#x3D;name       指定用于创建表的.sql文件或者字串</span><br><span class="line">					  File or string to use create tables.</span><br><span class="line">  --create-schema&#x3D;name 指定待测试的数据库名，MySQL中schema也就是database，默认是mysqlslap</span><br><span class="line">                      Schema to run tests in.</span><br><span class="line">  --csv[&#x3D;name]        Generate CSV output to named file or to stdout if no file</span><br><span class="line">                      is named.</span><br><span class="line">  -#, --debug[&#x3D;#]     This is a non-debug version. Catch this and exit.</span><br><span class="line">  --debug-check       This is a non-debug version. Catch this and exit.</span><br><span class="line">  -T, --debug-info    打印内存和CPU的信息</span><br><span class="line">					  This is a non-debug version. Catch this and exit.</span><br><span class="line">  --default-auth&#x3D;name Default authentication client-side plugin to use.</span><br><span class="line">  -F, --delimiter&#x3D;name 文件中的SQL语句使用分割符号</span><br><span class="line">                      Delimiter to use in SQL statements supplied in file or</span><br><span class="line">                      command line.</span><br><span class="line">  --detach&#x3D;#           每执行完N个语句，先断开再重新打开连接</span><br><span class="line">					  Detach (close and reopen) connections after X number of</span><br><span class="line">                      requests.</span><br><span class="line">  --enable-cleartext-plugin </span><br><span class="line">                      Enable&#x2F;disable the clear text authentication plugin.</span><br><span class="line">  -e, --engine&#x3D;name   创建测试表所使用的存储引擎，可指定多个</span><br><span class="line">			          Storage engine to use for creating the table.</span><br><span class="line">  -h, --host&#x3D;name     Connect to host.</span><br><span class="line">  -i, --iterations&#x3D;#  迭代执行的次数</span><br><span class="line">					  Number of times to run the tests.</span><br><span class="line">  --no-drop           Do not drop the schema after the test.</span><br><span class="line">  -x, --number-char-cols&#x3D;name 自动生成的测试表中包含多少个字符类型的列，默认1</span><br><span class="line">                      Number of VARCHAR columns to create in table if</span><br><span class="line">                      specifying --auto-generate-sql.</span><br><span class="line">  -y, --number-int-cols&#x3D;name 自动生成的测试表中包含多少个数字类型的列，默认1</span><br><span class="line">                      Number of INT columns to create in table if specifying</span><br><span class="line">                      --auto-generate-sql.</span><br><span class="line">  --number-of-queries&#x3D;# 总的测试查询次数(并发客户数×每客户查询次数)</span><br><span class="line">                      Limit each client to this number of queries (this is not</span><br><span class="line">                      exact).</span><br><span class="line">  --only-print        只输出模拟执行的结果，不实际执行Do not connect to the databases, but instead print out</span><br><span class="line">                      what would have been done.</span><br><span class="line">  -p, --password[&#x3D;name] </span><br><span class="line">                      Password to use when connecting to server. If password is</span><br><span class="line">                      not given it&#39;s asked from the tty.</span><br><span class="line">  --plugin-dir&#x3D;name   Directory for client-side plugins.</span><br><span class="line">  -P, --port&#x3D;#        Port number to use for connection.</span><br><span class="line">  --post-query&#x3D;name   测试完成以后执行的SQL语句的文件或者字符串 这个过程不影响时间计算Query to run or file containing query to execute after</span><br><span class="line">                      tests have completed.</span><br><span class="line">  --post-system&#x3D;name  测试完成以后执行的系统语句 这个过程不影响时间计算system() string to execute after tests have completed.</span><br><span class="line">  --pre-query&#x3D;name    测试执行之前执行的SQL语句的文件或者字符串 这个过程不影响时间计算Query to run or file containing query to execute before</span><br><span class="line">                      running tests.</span><br><span class="line">  --pre-system&#x3D;name   测试执行之前执行的系统语句 这个过程不影响时间计算system() string to execute before running tests.</span><br><span class="line">  --protocol&#x3D;name     The protocol to use for connection (tcp, socket, pipe,</span><br><span class="line">                      memory).</span><br><span class="line">  -q, --query&#x3D;name    指定自定义.sql脚本执行测试。例如可以调用自定义的一个存储过程或者sql语句来执行测试Query to run or file containing query to run.</span><br><span class="line">  --secure-auth       Refuse client connecting to server if it uses old</span><br><span class="line">                      (pre-4.1.1) protocol. Deprecated. Always TRUE</span><br><span class="line">  -s, --silent        不输出Run program in silent mode - no output.</span><br><span class="line">  -S, --socket&#x3D;name   The socket file to use for connection.</span><br><span class="line">  --sql-mode&#x3D;name     Specify sql-mode to run mysqlslap tool.</span><br><span class="line">  --ssl-mode&#x3D;name     SSL connection mode.</span><br><span class="line">  --ssl               Deprecated. Use --ssl-mode instead.</span><br><span class="line">                      (Defaults to on; use --skip-ssl to disable.)</span><br><span class="line">  --ssl-verify-server-cert </span><br><span class="line">                      Deprecated. Use --ssl-mode&#x3D;VERIFY_IDENTITY instead.</span><br><span class="line">  --ssl-ca&#x3D;name       CA file in PEM format.</span><br><span class="line">  --ssl-capath&#x3D;name   CA directory.</span><br><span class="line">  --ssl-cert&#x3D;name     X509 cert in PEM format.</span><br><span class="line">  --ssl-cipher&#x3D;name   SSL cipher to use.</span><br><span class="line">  --ssl-key&#x3D;name      X509 key in PEM format.</span><br><span class="line">  --ssl-crl&#x3D;name      Certificate revocation list.</span><br><span class="line">  --ssl-crlpath&#x3D;name  Certificate revocation list path.</span><br><span class="line">  --tls-version&#x3D;name  TLS version to use, permitted values are: TLSv1, TLSv1.1</span><br><span class="line">  -u, --user&#x3D;name     User for login if not current user.</span><br><span class="line">  -v, --verbose       输出更多的信息More verbose output; you can use this multiple times to</span><br><span class="line">                      get even more verbose output.</span><br><span class="line">  -V, --version       Output version information and exit.</span><br></pre></td></tr></table></figure>

<p>2、以自动生成测试表和数据的形式，分别模拟 50 和 100 个客户端并发连接处理 1000 个 query 的情况。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 local]# mysqlslap -uroot -p&#39;CAOcao123~!@&#39; -a --concurrency&#x3D;50,100 --number-of-queries&#x3D;1000 </span><br><span class="line">mysqlslap: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Benchmark</span><br><span class="line">	Average number of seconds to run all queries: 0.605 seconds</span><br><span class="line">	Minimum number of seconds to run all queries: 0.605 seconds</span><br><span class="line">	Maximum number of seconds to run all queries: 0.605 seconds</span><br><span class="line">	Number of clients running queries: 50</span><br><span class="line">	Average number of queries per client: 20</span><br><span class="line"></span><br><span class="line">Benchmark</span><br><span class="line">	Average number of seconds to run all queries: 0.534 seconds</span><br><span class="line">	Minimum number of seconds to run all queries: 0.534 seconds</span><br><span class="line">	Maximum number of seconds to run all queries: 0.534 seconds</span><br><span class="line">	Number of clients running queries: 100</span><br><span class="line">	Average number of queries per client: 10</span><br></pre></td></tr></table></figure>

<p>3、增加 –debug-info 选项，可以输出内存和CPU信息。 </p>
]]></content>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>SaltStack学习记录</title>
    <url>/posts/8764777f.html</url>
    <content><![CDATA[<h3 id="SaltStack-介绍"><a href="#SaltStack-介绍" class="headerlink" title="SaltStack 介绍"></a>SaltStack 介绍</h3><p>salt是一个新的基础平台管理工具, 只需要花费数分钟即可运行起来, 扩展性足以支撑管理上万台服务器, 数秒钟即可完成数据传递。 经常被描述为Func加强版+Puppet精简版。 SaltStack采用C/S架构。简单的说: salt是一种全新的基础设施管理方式, 部署轻松, 在几分钟内可运行起来, 扩展性好。很容易管理上万台服务器, 速度快, 服务器之间秒级通信。 salt底层采用动态的连接总线, 使其可以用于编配, 远程执行, 配置管理等等。最为重要的一点, salt是开源的。 而且是python实现的自动化运维工具, 这意味着我们可以对其进行一些改动, 在其基础之上加上我们想要的功能, 对其进行二次开发。</p>
<a id="more"></a>
<h3 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h3><p>系统Centos7.2 x64两台, 一台为master,另一台为minion。</p>
<h3 id="SaltStack安装"><a href="#SaltStack安装" class="headerlink" title="SaltStack安装"></a>SaltStack安装</h3><h4 id="两台安装epel"><a href="#两台安装epel" class="headerlink" title="两台安装epel"></a>两台安装epel</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# rpm -Uvh http:&#x2F;&#x2F;ftp.linux.ncsu.edu&#x2F;pub&#x2F;epel&#x2F;7&#x2F;x86_64&#x2F;e&#x2F;epel-release-7-9.noarch.rpm</span><br></pre></td></tr></table></figure>

<h4 id="在master安装"><a href="#在master安装" class="headerlink" title="在master安装"></a>在master安装</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# yum -y install salt-master</span><br></pre></td></tr></table></figure>

<h4 id="在minion上运行"><a href="#在minion上运行" class="headerlink" title="在minion上运行"></a>在minion上运行</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# yum -y install salt-minion</span><br></pre></td></tr></table></figure>

<h3 id="SAltStack配置"><a href="#SAltStack配置" class="headerlink" title="SAltStack配置"></a>SAltStack配置</h3><h4 id="修改master配置文件，并启动服务"><a href="#修改master配置文件，并启动服务" class="headerlink" title="修改master配置文件，并启动服务"></a>修改master配置文件，并启动服务</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# vim &#x2F;etc&#x2F;salt&#x2F;master</span><br><span class="line">修改interface: 192.168.1.226 #master ip地址</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# systemctl start salt-master</span><br></pre></td></tr></table></figure>

<h4 id="修改被管理端-minion-，并启动服务"><a href="#修改被管理端-minion-，并启动服务" class="headerlink" title="修改被管理端(minion)，并启动服务"></a>修改被管理端(minion)，并启动服务</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# vim &#x2F;etc&#x2F;salt&#x2F;minion</span><br><span class="line">修改master： 192.168.1.226</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# systemctl start salt-minion</span><br></pre></td></tr></table></figure>

<h4 id="master接受minion的托管请求，在master上操作"><a href="#master接受minion的托管请求，在master上操作" class="headerlink" title="master接受minion的托管请求，在master上操作"></a>master接受minion的托管请求，在master上操作</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# salt-key -L</span><br><span class="line">Accepted Keys:</span><br><span class="line">Denied Keys:</span><br><span class="line">Unaccepted Keys:</span><br><span class="line">192.168.1.136</span><br><span class="line">Rejected Keys:</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# salt-key -a 192.168.1.136</span><br><span class="line">The following keys are going to be accepted:</span><br><span class="line">Unaccepted Keys:</span><br><span class="line">192.168.1.136</span><br><span class="line">Proceed? [n&#x2F;Y] y</span><br><span class="line">Key for minion 192.168.1.136 accepted.</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# salt-key -L</span><br><span class="line">Accepted Keys:</span><br><span class="line">192.168.1.136</span><br><span class="line">Denied Keys:</span><br><span class="line">Unaccepted Keys:</span><br><span class="line">Rejected Keys:</span><br></pre></td></tr></table></figure>

<h3 id="SAltStack操作"><a href="#SAltStack操作" class="headerlink" title="SAltStack操作"></a>SAltStack操作</h3><h4 id="基本操作命令通用格式"><a href="#基本操作命令通用格式" class="headerlink" title="基本操作命令通用格式"></a>基本操作命令通用格式</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">格式: 命令 对象 执行模块 参数salt ‘*’ cmd.run “ping -c 4 www.baidu.com&quot;</span><br></pre></td></tr></table></figure>

<h3 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# salt &#39;*&#39; cmd.run &#39;uptime&#39;</span><br><span class="line">192.168.1.136:</span><br><span class="line">     21:14:44 up 1 day,  2:29,  3 users,  load average: 0.10, 0.18, 0.13</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# salt &#39;*&#39; cmd.run &#39;date&#39;</span><br><span class="line">192.168.1.136:</span><br><span class="line">    Fri May  5 21:14:51 CST 2017</span><br><span class="line"></span><br><span class="line">[root@object1 ~]# salt &#39;*&#39; disk.usage</span><br></pre></td></tr></table></figure>

<p><strong>默认情况下master和minion之间使用以下端口进行通信:<br>4505(publish_port)：salt的消息发布系统<br>4506(ret_port)：salt客户端与服务端通信的端口<br>cmd.run 为模块,又称之为超级命令. 可以执行Linux中的任何命令</strong></p>
<h3 id="Salt-States"><a href="#Salt-States" class="headerlink" title="Salt States"></a>Salt States</h3><p>SLS(代表Salt State文件)是Salt State系统的核心。SLS描述了系统的目标状态, 由格式简单的数据构成。 这经常被称作配置管理。</p>
<h4 id="默认的数据-YAML"><a href="#默认的数据-YAML" class="headerlink" title="默认的数据 - YAML"></a>默认的数据 - YAML</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper、Dubbo-Admin管理平台的搭建</title>
    <url>/posts/577b95cf.html</url>
    <content><![CDATA[<p> ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。</p>
<p>ZooKeeper官网为：<a href="http://zookeeper.apache.org/">http://zookeeper.apache.org/</a></p>
<a id="more"></a>
<p>Dubbo是Alibaba开源的分布式服务框架，它最大的特点是按照分层的方式来架构，使用这种方式可以使各个层之间解耦合（或者最大限度地松耦合）。从服务模型的角度来看，Dubbo采用的是一种非常简单的模型，要么是提供方提供服务，要么是消费方消费服务，所以基于这一点可以抽象出服务提供方（Provider）和服务消费方（Consumer）两个角色。关于注册中心、协议支持、服务监控等内容。</p>
<p>Dubbo官网为：<a href="http://dubbo.io/">http://dubbo.io/</a></p>
<h2 id="一、zookeeper安装与启动"><a href="#一、zookeeper安装与启动" class="headerlink" title="一、zookeeper安装与启动"></a>一、zookeeper安装与启动</h2><p>首先需要安装JdK，从Oracle的Java网站下载，安装很简单，就不再详述。<br>zookeeper的下载地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;www.apache.org&#x2F;dyn&#x2F;closer.cgi&#x2F;zookeeper&#x2F;</span><br></pre></td></tr></table></figure>

<p>下载后直接解压，不用安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 home]# tar zxvf zookeeper-3.4.10.tar.gz</span><br></pre></td></tr></table></figure>

<p>修改默认配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 conf]# cp zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>

<p>参数说明:</p>
<p>tickTime：zookeeper中使用的基本时间单位, 毫秒值这个时间是作为Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。</p>
<p>dataDir：数据目录. 可以是任意目录，默认情况下，Zookeeper 将写数据</p>
<p>的日志文件也保存在这个目录里。</p>
<p>clientPort：这个端口就是客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。</p>
<p>至此, zookeeper的单机模式已经配置好了，启动ZooKeeper</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 zookeeper-3.4.10]# sh bin&#x2F;zkServer.sh start</span><br></pre></td></tr></table></figure>

<h2 id="二、Dubbo-admin管理平台的安装"><a href="#二、Dubbo-admin管理平台的安装" class="headerlink" title="二、Dubbo-admin管理平台的安装"></a>二、Dubbo-admin管理平台的安装</h2><p>因为zookeeper只是一个黑框，我们无法看到是否存在了什么提供者或消费者，这时就要借助Dubbo-Admin管理平台来实时的查看，也可以通过这个平台来管理提者和消费者。制作了基于jdk1.8打包的dubbo-admin.war</p>
<p>下载地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;download.csdn.net&#x2F;detail&#x2F;qq_30567735&#x2F;9826361</span><br></pre></td></tr></table></figure>

<p>dubbo源码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;alibaba&#x2F;dubbo</span><br></pre></td></tr></table></figure>

<p>下载好dubbo-admin.war后，我们就可以按常用的web部署方式进行部署即可，把war包放到tomcat的webapps目录下，启动tomcat，后再部署下相应的参数。</p>
<p>启动tomcat</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 apache-tomcat-7.0.62]# sh bin&#x2F;startup.sh</span><br></pre></td></tr></table></figure>

<p>访问项目地址即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http: &#x2F;&#x2F;ip地址:端口号&#x2F;dubbo-admin-2.5.4&#x2F;</span><br></pre></td></tr></table></figure>

<p><img src="http://imglf2.nosdn.127.net/img/R3hDdlA4YitONFp2Wm5UbFJ2anArNHgxdEhiT0drYW5QU3FYM1R2cTNhbi9jaWFCcnYvbmtnPT0.jpg?imageView&thumbnail=500x0&quality=96&stripmeta=0&type=jpg" alt=""></p>
]]></content>
  </entry>
  <entry>
    <title>CentOS 7 安装字体</title>
    <url>/posts/16b1f71f.html</url>
    <content><![CDATA[<p>字体默认存放路径：/usr/share/fonts</p>
<a id="more"></a>
<h1 id="在-usr-share-fonts目录下建立一个子目录："><a href="#在-usr-share-fonts目录下建立一个子目录：" class="headerlink" title="在/usr/share/fonts目录下建立一个子目录：　　"></a>在/usr/share/fonts目录下建立一个子目录：　　</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost fonts]# mkdir lolaage</span><br></pre></td></tr></table></figure>

<h1 id="将需要安装的字体文件上传到步骤1中建立的目录中："><a href="#将需要安装的字体文件上传到步骤1中建立的目录中：" class="headerlink" title="将需要安装的字体文件上传到步骤1中建立的目录中："></a>将需要安装的字体文件上传到步骤1中建立的目录中：</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost lolaage]# rz</span><br></pre></td></tr></table></figure>

<h1 id="建立字体索引信息，并更新字体缓存："><a href="#建立字体索引信息，并更新字体缓存：" class="headerlink" title="建立字体索引信息，并更新字体缓存："></a>建立字体索引信息，并更新字体缓存：</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install mkfontscale fontconfig</span><br><span class="line"></span><br><span class="line">[root@localhost lolaage]# mkfontscale</span><br><span class="line">[root@localhost lolaage]# mkfontdir</span><br><span class="line">[root@localhost lolaage]# fc-cache</span><br></pre></td></tr></table></figure>

<h1 id="查看系统中安装的中文字体："><a href="#查看系统中安装的中文字体：" class="headerlink" title="查看系统中安装的中文字体："></a>查看系统中安装的中文字体：</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost lolaage]# fc-list</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>fonts</tag>
      </tags>
  </entry>
  <entry>
    <title>新增阿里云ECS服务器作为TCP负载均衡访问失败</title>
    <url>/posts/4f7efc26.html</url>
    <content><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>由于需要扩大业务访问量，新增ECS服务器，作为TCP负载均衡，添加到阿里云负载均衡下，添加成功后，业务总是会出现发送不了消息，登陆不成功。</p>
<a id="more"></a>如图所示
<img src="/images/729984945541266907.jpg" width="100%" height="100%">

<h1 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h1><p>初步找遍了防火墙、业务配置问题都没发现问题，单独调用一台服务器作为开发调试，也是正常，在负载均衡器里面，只添加一台新增服务器也是正常的，添加两台后就立马不行。最后无奈只能提交给工单，给阿里云客服处理，结果反馈是需要在ECS服务器上需要添加内核参数。是因为rp_filter特性和负载均衡底层LVS的策略路由产生冲突，导致访问出现异常。改成0,是因为从mac层面接受数据包 拆包到ip层 对应lo后 会以其他网卡发送数据包 ，由于rp_filter参数为1  所以 不允许从非此网卡接受的数据从该网卡发出 导致了 大量的syn超时。</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>具体参照阿里云TCP负载均衡不能访问解决方案：<a href="https://help.aliyun.com/knowledge_detail/55206.html">https://help.aliyun.com/knowledge_detail/55206.html</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#编辑&#x2F;etc&#x2F;sysctl.conf文件，将系统配置文件中的以下三个参数值设置为0。</span><br><span class="line"></span><br><span class="line">net.ipv4.conf.default.rp_filter &#x3D; 0</span><br><span class="line">net.ipv4.conf.all.rp_filter &#x3D; 0</span><br><span class="line">net.ipv4.conf.eth0.rp_filter &#x3D; 0</span><br><span class="line">net.ipv4.conf.eth1.rp_filter &#x3D; 0 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#执行sysctl -p命令，使配置生效。</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>tcp</tag>
        <tag>lb</tag>
      </tags>
  </entry>
  <entry>
    <title>服务器执行df -h卡住问题解决</title>
    <url>/posts/4b9519cf.html</url>
    <content><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>生产执行df -h时，卡住不动，无法返回结果</p>
<a id="more"></a>

<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>使用strace去追踪到底在哪里卡住了，执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">strace df -h</span><br></pre></td></tr></table></figure>

<p>显示出卡住的地方</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">stat(&quot;&#x2F;data&#x2F;nfs&quot;,</span><br></pre></td></tr></table></figure>

<p>看目录应该是mount出现问题，执行mount，查看挂载情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aly-99 bin]# mount</span><br><span class="line">sysfs on &#x2F;sys type sysfs (rw,nosuid,nodev,noexec,relatime)</span><br><span class="line">proc on &#x2F;proc type proc (rw,nosuid,nodev,noexec,relatime)</span><br><span class="line">devtmpfs on &#x2F;dev type devtmpfs (rw,nosuid,size&#x3D;32893592k,nr_inodes&#x3D;8223398,mode&#x3D;755)</span><br><span class="line">securityfs on &#x2F;sys&#x2F;kernel&#x2F;security type securityfs (rw,nosuid,nodev,noexec,relatime)</span><br><span class="line">tmpfs on &#x2F;dev&#x2F;shm type tmpfs (rw,nosuid,nodev)</span><br><span class="line">devpts on &#x2F;dev&#x2F;pts type devpts (rw,nosuid,noexec,relatime,gid&#x3D;5,mode&#x3D;620,ptmxmode&#x3D;000)</span><br><span class="line">tmpfs on &#x2F;run type tmpfs (rw,nosuid,nodev,mode&#x3D;755)</span><br><span class="line">tmpfs on &#x2F;sys&#x2F;fs&#x2F;cgroup type tmpfs (ro,nosuid,nodev,noexec,mode&#x3D;755)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent&#x3D;&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;systemd-cgroups-agent,name&#x3D;systemd)</span><br><span class="line">pstore on &#x2F;sys&#x2F;fs&#x2F;pstore type pstore (rw,nosuid,nodev,noexec,relatime)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)</span><br><span class="line">cgroup on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)</span><br><span class="line">configfs on &#x2F;sys&#x2F;kernel&#x2F;config type configfs (rw,relatime)</span><br><span class="line">&#x2F;dev&#x2F;vda1 on &#x2F; type ext4 (rw,relatime,data&#x3D;ordered)</span><br><span class="line">debugfs on &#x2F;sys&#x2F;kernel&#x2F;debug type debugfs (rw,relatime)</span><br><span class="line">mqueue on &#x2F;dev&#x2F;mqueue type mqueue (rw,relatime)</span><br><span class="line">hugetlbfs on &#x2F;dev&#x2F;hugepages type hugetlbfs (rw,relatime)</span><br><span class="line">nfsd on &#x2F;proc&#x2F;fs&#x2F;nfsd type nfsd (rw,relatime)</span><br><span class="line">&#x2F;dev&#x2F;vdb on &#x2F;data type ext4 (rw,relatime,data&#x3D;ordered)</span><br><span class="line">sunrpc on &#x2F;var&#x2F;lib&#x2F;nfs&#x2F;rpc_pipefs type rpc_pipefs (rw,relatime)</span><br><span class="line">0badf4a571-jye50.cn-hangzhou.nas.aliyuncs.com:&#x2F; on &#x2F;data&#x2F;nfs type nfs4 (rw,relatime,vers&#x3D;4.0,rsize&#x3D;1048576,wsize&#x3D;1048576,namlen&#x3D;255,hard,proto&#x3D;tcp,timeo&#x3D;600,retrans&#x3D;2,sec&#x3D;sys,clientaddr&#x3D;10.80.226.189,local_lock&#x3D;none,addr&#x3D;100.99.252.70)</span><br><span class="line">tmpfs on &#x2F;run&#x2F;user&#x2F;0 type tmpfs (rw,nosuid,nodev,relatime,size&#x3D;6580616k,mode&#x3D;700)</span><br><span class="line">systemd-1 on &#x2F;proc&#x2F;sys&#x2F;fs&#x2F;binfmt_misc type autofs (rw,relatime,fd&#x3D;36,pgrp&#x3D;1,timeout&#x3D;0,minproto&#x3D;5,maxproto&#x3D;5,direct,pipe_ino&#x3D;517790550)</span><br><span class="line">tmpfs on &#x2F;run&#x2F;user&#x2F;1003 type tmpfs (rw,nosuid,nodev,relatime,size&#x3D;6580616k,mode&#x3D;700,uid&#x3D;1003,gid&#x3D;1003)</span><br></pre></td></tr></table></figure>
<p>由于阿里云nas地址已释放，故出现不能读取的情况</p>
<p>执行强制umount</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aly-99 bin]# umount -f &#x2F;data&#x2F;nfs</span><br><span class="line">umount.nfs4: &#x2F;data&#x2F;nfs: device is busy</span><br></pre></td></tr></table></figure>

<p>执行失败，kill掉进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aly-99 bin]# fuser -m -k &#x2F;data&#x2F;nfs</span><br></pre></td></tr></table></figure>

<p><strong>注：</strong><br>单单执行以下命令，依然会卡住不动，原因不明</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fuser -m &#x2F;data&#x2F;nfs</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>df</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos7配置本地的yum源</title>
    <url>/posts/c2afc167.html</url>
    <content><![CDATA[<p>准备以下几个文件：</p>
<p><a href="http://mirrors.163.com/centos/7.7.1908/os/x86_64/Packages/deltarpm-3.6-3.el7.x86_64.rpm">http://mirrors.163.com/centos/7.7.1908/os/x86_64/Packages/deltarpm-3.6-3.el7.x86_64.rpm</a><br><a href="http://mirrors.163.com/centos/7.7.1908/os/x86_64/Packages/python-deltarpm-3.6-3.el7.x86_64.rpm">http://mirrors.163.com/centos/7.7.1908/os/x86_64/Packages/python-deltarpm-3.6-3.el7.x86_64.rpm</a><br><a href="http://mirrors.163.com/centos/7.7.1908/os/x86_64/Packages/createrepo-0.9.9-28.el7.noarch.rpm">http://mirrors.163.com/centos/7.7.1908/os/x86_64/Packages/createrepo-0.9.9-28.el7.noarch.rpm</a><br><a href="http://mirrors.163.com/centos/7.7.1908/os/x86_64/Packages/libxml2-python-2.9.1-6.el7_2.3.x86_64.rpm">http://mirrors.163.com/centos/7.7.1908/os/x86_64/Packages/libxml2-python-2.9.1-6.el7_2.3.x86_64.rpm</a></p>
<a id="more"></a>
<p>本地安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum localinstall createrepo-0.9.9-28.el7.noarch.rpm</span><br><span class="line">yum localinstall libxml2-python-2.9.1-6.el7_2.3.x86_64.rpm</span><br></pre></td></tr></table></figure>

<p>创建本地yum地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;yum.repos.d&#x2F;local-yum.repo</span><br><span class="line"></span><br><span class="line">[local-yum]</span><br><span class="line">name&#x3D;local-yum</span><br><span class="line">baseurl&#x3D;file:&#x2F;&#x2F;&#x2F;opt&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br></pre></td></tr></table></figure>

<p>创建iso文件夹及cdrom文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;mnt&#x2F;&#123;iso,cdrom&#125;</span><br></pre></td></tr></table></figure>

<p>上传iso文件到/mnt/iso</p>
<p>将/mnt/iso/CentOS-7-x86_64-DVD-1908.iso挂载到/mnt/cdrom/下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mount -o loop &#x2F;mnt&#x2F;iso&#x2F;CentOS-7-x86_64-DVD-1908.iso &#x2F;mnt&#x2F;cdron&#x2F;</span><br></pre></td></tr></table></figure>

<p>将/mnt/cdron/目录下的所用文件复制到/opt/下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp -rv &#x2F;mnt&#x2F;cdrom&#x2F;* &#x2F;opt&#x2F;</span><br></pre></td></tr></table></figure>

<p>切换到/opt/目录下：删除*.html、删除CentOS/repodata/TRANS.TBL</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;</span><br><span class="line">rm -rf *.html</span><br><span class="line">rm -rf repodata&#x2F;TRANS.TBL</span><br></pre></td></tr></table></figure>

<p>执行createrepo命令生成YUM通用数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">createrepo -g &#x2F;opt&#x2F;repodata&#x2F;521f322f05f9802f2438d8bb7d97558c64ff3ff74c03322d77787ade9152d8bb-c7-x86_64-comps.xml &#x2F;opt&#x2F;</span><br></pre></td></tr></table></figure>

<p>清除缓存</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum clean all</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS7 升级内核</title>
    <url>/posts/fc409f42.html</url>
    <content><![CDATA[<p>Linux 内核官网： <a href="https://www.kernel.org/">https://www.kernel.org/</a><br>Linux 内核各个版本的支持时间： <a href="https://www.kernel.org/category/releases.html">https://www.kernel.org/category/releases.html</a></p>
<a id="more"></a>

<h1 id="查看当前的内核版本"><a href="#查看当前的内核版本" class="headerlink" title="查看当前的内核版本"></a>查看当前的内核版本</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Linux 只表示内核。各大 Linux 发行版（RedHat、Ubuntu、CentOS 等）在内核基础上集成了其他的一系列软件，按照各自的版本规则发布。</p>
<h2 id="常用的查看内核信息的命令"><a href="#常用的查看内核信息的命令" class="headerlink" title="常用的查看内核信息的命令"></a>常用的查看内核信息的命令</h2><h3 id="uname"><a href="#uname" class="headerlink" title="uname"></a>uname</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uname</span><br><span class="line"></span><br><span class="line">打印指定的系统信息。不带参数时，默认使用 -s 参数。 </span><br><span class="line">参数： </span><br><span class="line">-a, –all：按照下面的顺序打印所有信息，如果 -p 和 -i 未知时排除掉。 </span><br><span class="line">-s, –kernel-name：打印内核名字，一般就是 Linux。 </span><br><span class="line">-n, –nodename：打印网络节点的主机名。 </span><br><span class="line">-r, –kernel-release：打印内核发行版的版本。常用。3.10.0-514.26.2.el7.x86_64 </span><br><span class="line">-v, –kernel-version：打印内核的版本。#1 SMP Tue Jul 4 15:04:05 UTC 2017 </span><br><span class="line">-m, –machine：打印机器硬件名。 </span><br><span class="line">-p, –processor：打印处理器名字或“unknown”。 </span><br><span class="line">-i, –hardware-platform：打印硬件平台或“unknown”。 </span><br><span class="line">-o, –operating-system：打印操作系统。 </span><br><span class="line">–help：显示这个帮助并退出。 </span><br><span class="line">–version：显示这版本信息并退出。</span><br></pre></td></tr></table></figure>

<h3 id="proc-虚拟文件系统"><a href="#proc-虚拟文件系统" class="headerlink" title="proc 虚拟文件系统"></a>proc 虚拟文件系统</h3><p>内核空间和用户空间通过 /proc 虚拟文件系统可以通信。<br>/proc 目录中包含一些目录和虚拟文件，这些虚拟文件可以向用户呈现内核信息或者从用户空间向内核发送信息。<br>常用文件：</p>
<ul>
<li><p>cpuinfo：标识了处理器的类型和速度</p>
</li>
<li><p>pci：显示在 PCI 总线上找到的设备</p>
</li>
<li><p>modules：当前加载到内核中的模块</p>
</li>
<li><p>version：系统版本及内核版本</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@VM_20_6_centos ~]# cat &#x2F;proc&#x2F;version </span><br><span class="line">Linux version 4.4.158-1.el7.elrepo.x86_64 (mockbuild@Build64R7) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-28) (GCC) ) #1 SMP Wed Sep 26 14:58:11 EDT 2018</span><br></pre></td></tr></table></figure>

<h1 id="升级内核"><a href="#升级内核" class="headerlink" title="升级内核"></a>升级内核</h1><p>更新仓库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y update</span><br></pre></td></tr></table></figure>
<h2 id="启用-ELRepo-仓库"><a href="#启用-ELRepo-仓库" class="headerlink" title="启用 ELRepo 仓库"></a>启用 ELRepo 仓库</h2><p>ELRepo 仓库是基于社区的用于企业级 Linux 仓库，提供对 RedHat Enterprise (RHEL) 和 其他基于 RHEL的 Linux 发行版（CentOS、Scientific、Fedora 等）的支持。<br>ELRepo 聚焦于和硬件相关的软件包，包括文件系统驱动、显卡驱动、网络驱动、声卡驱动和摄像头驱动等。</p>
<p>启用 ELRepo 仓库：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm --import https:&#x2F;&#x2F;www.elrepo.org&#x2F;RPM-GPG-KEY-elrepo.org</span><br><span class="line">rpm -Uvh http:&#x2F;&#x2F;www.elrepo.org&#x2F;elrepo-release-7.0-3.el7.elrepo.noarch.rpm</span><br></pre></td></tr></table></figure>

<h2 id="查看可用的系统内核包"><a href="#查看可用的系统内核包" class="headerlink" title="查看可用的系统内核包"></a>查看可用的系统内核包</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum --disablerepo&#x3D;&quot;*&quot; --enablerepo&#x3D;&quot;elrepo-kernel&quot; list available</span><br></pre></td></tr></table></figure>

<h2 id="安装最新内核"><a href="#安装最新内核" class="headerlink" title="安装最新内核"></a>安装最新内核</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum --enablerepo&#x3D;elrepo-kernel install kernel-lt</span><br></pre></td></tr></table></figure>

<h2 id="设置-grub2"><a href="#设置-grub2" class="headerlink" title="设置 grub2"></a>设置 grub2</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看系统上的所有可以内核</span><br><span class="line">sudo awk -F\&#39; &#39;$1&#x3D;&#x3D;&quot;menuentry &quot; &#123;print i++ &quot; : &quot; $2&#125;&#39; &#x2F;etc&#x2F;grub2.cfg</span><br><span class="line"></span><br><span class="line"># 设置 grub2</span><br><span class="line">grub2-set-default 0 命令或编辑 &#x2F;etc&#x2F;default&#x2F;grub </span><br><span class="line"></span><br><span class="line"># vi &#x2F;etc&#x2F;default&#x2F;grub </span><br><span class="line"># 设置 GRUB_DEFAULT&#x3D;0，表示使用上一步的 awk 命令显示的编号为 0 的内核作为默认内核</span><br><span class="line">&gt; GRUB_TIMEOUT&#x3D;5</span><br><span class="line">&gt; GRUB_DISTRIBUTOR&#x3D;&quot;$(sed &#39;s, release .*$,,g&#39; &#x2F;etc&#x2F;system-release)&quot;</span><br><span class="line">&gt; GRUB_DEFAULT&#x3D;0</span><br><span class="line">&gt; GRUB_DISABLE_SUBMENU&#x3D;true</span><br><span class="line">&gt; GRUB_TERMINAL_OUTPUT&#x3D;&quot;console&quot;</span><br><span class="line">&gt; GRUB_CMDLINE_LINUX&#x3D;&quot;crashkernel&#x3D;auto console&#x3D;ttyS0 console&#x3D;tty0 panic&#x3D;5&quot;</span><br><span class="line">&gt; GRUB_DISABLE_RECOVERY&#x3D;&quot;true&quot;</span><br><span class="line">&gt; GRUB_TERMINAL&#x3D;&quot;serial console&quot;</span><br><span class="line">&gt; GRUB_TERMINAL_OUTPUT&#x3D;&quot;serial console&quot;</span><br><span class="line">&gt; GRUB_SERIAL_COMMAND&#x3D;&quot;serial --speed&#x3D;9600 --unit&#x3D;0 --word&#x3D;8 --parity&#x3D;no --stop&#x3D;1&quot;</span><br></pre></td></tr></table></figure>

<h2 id="生成-grub-配置文件并重启"><a href="#生成-grub-配置文件并重启" class="headerlink" title="生成 grub 配置文件并重启"></a>生成 grub 配置文件并重启</h2><p>通过 gurb2-mkconfig 命令创建 grub2 的配置文件，然后重启</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo grub2-mkconfig -o &#x2F;boot&#x2F;grub2&#x2F;grub.cfg</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure>

<h1 id="删除旧内核（可选）"><a href="#删除旧内核（可选）" class="headerlink" title="删除旧内核（可选）"></a>删除旧内核（可选）</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -qa | grep kernel</span><br><span class="line"></span><br><span class="line"># 删除旧版本工具包</span><br><span class="line">yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64</span><br><span class="line"># 安装新版本工具包</span><br><span class="line">yum --disablerepo&#x3D;\* --enablerepo&#x3D;elrepo-kernel install -y kernel-ml-tools.x86_64</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>kernel</tag>
      </tags>
  </entry>
  <entry>
    <title>Ceph RBD介绍与使用</title>
    <url>/posts/58d7ce25.html</url>
    <content><![CDATA[<h1 id="RBD介绍"><a href="#RBD介绍" class="headerlink" title="RBD介绍"></a>RBD介绍</h1><p>RBD即RADOS Block Device的简称，RBD块存储是最稳定且最常用的存储类型。RBD块设备类似磁盘可以被挂载。 RBD块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在Ceph集群的多个OSD中。如下是对Ceph RBD的理解。</p>
<a id="more"></a>
<p>RBD 就是 Ceph 里的块设备，一个 4T 的块设备的功能和一个 4T 的 SATA 类似，挂载的 RBD 就可以当磁盘用；<br>resizable：这个块可大可小；<br>data striped：这个块在Ceph里面是被切割成若干小块来保存，不然 1PB 的块怎么存的下；<br>thin-provisioned：精简置备，1TB 的集群是能创建无数 1PB 的块的。其实就是块的大小和在 Ceph 中实际占用大小是没有关系的，刚创建出来的块是不占空间，今后用多大空间，才会在 Ceph 中占用多大空间。举例：你有一个 32G 的 U盘，存了一个2G的电影，那么 RBD 大小就类似于 32G，而 2G 就相当于在 Ceph 中占用的空间 ；<br>块存储本质就是将裸磁盘或类似裸磁盘(lvm)设备映射给主机使用，主机可以对其进行格式化并存储和读取数据，块设备读取速度快但是不支持共享。</p>
<p>ceph可以通过内核模块和librbd库提供块设备支持。客户端可以通过内核模块挂载rbd使用，客户端使用rbd块设备就像使用普通硬盘一样，可以对其就行格式化然后使用；客户应用也可以通过librbd使用ceph块，典型的是云平台的块存储服务（如下图），云平台可以使用rbd作为云的存储后端提供镜像存储、volume块或者客户的系统引导盘等。</p>
<img src="/images/20191231193020.png" width="100%" height="100%">


<h1 id="RBD常用命令"><a href="#RBD常用命令" class="headerlink" title="RBD常用命令"></a>RBD常用命令</h1><p>命令    功能<br>rbd create    创建块设备映像<br>rbd ls    列出 rbd 存储池中的块设备<br>rbd info    查看块设备信息<br>rbd diff    可以统计 rbd 使用量<br>rbd map    映射块设备<br>rbd showmapped    查看已映射块设备<br>rbd remove    删除块设备<br>rbd resize    更改块设备的大小</p>
<h1 id="RBD配置操作"><a href="#RBD配置操作" class="headerlink" title="RBD配置操作"></a>RBD配置操作</h1><h2 id="RBD挂载到操作系统"><a href="#RBD挂载到操作系统" class="headerlink" title="RBD挂载到操作系统"></a>RBD挂载到操作系统</h2><p>创建rbd使用的pool</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ceph osd pool create rbd  32 32</span><br><span class="line"># ceph osd pool application enable rbd rbd</span><br></pre></td></tr></table></figure>

<p>创建一个块设备</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd create --size 10240 image01 </span><br><span class="line"></span><br><span class="line">RBD的创建</span><br><span class="line"></span><br><span class="line">执行命令rbd create rbd_write --size 1024 --pool data或者rbd create rbd_write --size 1024 -p data创建rbd块设备</span><br><span class="line"></span><br><span class="line">rbd_write代表所创建rbd块设备的名字</span><br><span class="line">--size后接rbd块设备的大小，单位MB</span><br><span class="line">--pool后接该rbd块设备所在存储池名称</span><br></pre></td></tr></table></figure>

<p>查看块设备</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd ls</span><br><span class="line"># rbd info image01</span><br><span class="line"></span><br><span class="line">执行命令rbd ls -p data查看存储池中已创建的块设备</span><br></pre></td></tr></table></figure>

<p>将块设备映射到系统内核</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd map image01</span><br></pre></td></tr></table></figure>

<p>禁用当前系统内核不支持的feature</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd feature disable image01 exclusive-lock, object-map, fast-diff, deep-flatten</span><br></pre></td></tr></table></figure>

<p>再次映射</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd map image01</span><br></pre></td></tr></table></figure>

<p>格式化块设备镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mkfs.xfs &#x2F;dev&#x2F;rbd0</span><br></pre></td></tr></table></figure>

<p>mount到本地</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mount &#x2F;dev&#x2F;rbd0 &#x2F;mnt</span><br><span class="line"># umount &#x2F;mnt</span><br></pre></td></tr></table></figure>

<p>取消块设备和内核映射</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd unmap image01</span><br></pre></td></tr></table></figure>

<p>删除RBD块设备</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd rm image01</span><br></pre></td></tr></table></figure>

<h2 id="快照配置"><a href="#快照配置" class="headerlink" title="快照配置"></a>快照配置</h2><p>创建快照</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rbd create --size 10240 image02</span><br><span class="line">rbd snap create image02@image02_snap01</span><br></pre></td></tr></table></figure>

<p>列出创建的快照</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd snap list image02</span><br><span class="line">或</span><br><span class="line"># rbd ls -l</span><br></pre></td></tr></table></figure>

<p>查看快照详细信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd info image02@image02_snap01</span><br></pre></td></tr></table></figure>

<p>克隆快照（快照必须处于被保护状态才能被克隆）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd snap protect image02@image02_snap01</span><br><span class="line"># rbd clone rbd&#x2F;image02@image02_snap01 kube&#x2F;image02_clone01</span><br></pre></td></tr></table></figure>

<p>查看快照的children</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd children image02</span><br></pre></td></tr></table></figure>

<p>去掉快照的parent</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd flatten kube&#x2F;image02_clone01</span><br></pre></td></tr></table></figure>

<p>恢复快照</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd snap rollback image02@image02_snap01</span><br></pre></td></tr></table></figure>

<p>删除快照</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd snap unprotect image02@image02_snap01</span><br><span class="line"># rbd snap remove image02@image02_snap01</span><br></pre></td></tr></table></figure>

<h2 id="导出导入RBD镜像"><a href="#导出导入RBD镜像" class="headerlink" title="导出导入RBD镜像"></a>导出导入RBD镜像</h2><p>导出RBD镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd export image02 &#x2F;tmp&#x2F;image02</span><br></pre></td></tr></table></figure>

<p>导出RBD镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># rbd import &#x2F;tmp&#x2F;image02 rbd&#x2F;image02 --image-format 2</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title>ceph通过ceph-deploy部署分布式存储</title>
    <url>/posts/eb41373f.html</url>
    <content><![CDATA[<h1 id="Ceph介绍"><a href="#Ceph介绍" class="headerlink" title="Ceph介绍"></a>Ceph介绍</h1><h2 id="ceph架构哲学"><a href="#ceph架构哲学" class="headerlink" title="ceph架构哲学"></a>ceph架构哲学</h2><ul>
<li><p>每个组件必须是可扩展的</p>
</li>
<li><p>不存在单点故障</p>
</li>
<li><p>解决方案必须是基于软件的、开源的、适应力强的</p>
</li>
<li><p>可以运行在常规硬件上的</p>
</li>
<li><p>任何可能的一切都必须自我管理</p>
</li>
</ul>
<p>意义：帮助企业摆脱昂贵的专属硬件</p>
<a id="more"></a>
<h2 id="ceph目标"><a href="#ceph目标" class="headerlink" title="ceph目标"></a>ceph目标</h2><ul>
<li><p>轻松扩展到PB级别</p>
</li>
<li><p>不同负荷下的高性能（IOPS）</p>
</li>
<li><p>高可靠</p>
</li>
</ul>
<h2 id="ceph现状"><a href="#ceph现状" class="headerlink" title="ceph现状"></a>ceph现状</h2><ul>
<li><p>作为云存储解决方案</p>
</li>
<li><p>作为软件定义的解决方案</p>
</li>
<li><p>作为统一的存储解决方案</p>
</li>
</ul>
<h1 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h1><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>硬件配置： 每个节点配置两个网卡，一内一外<br>操作系统： Centos 7.X<br>节点准备：<br>1、3个节点，配置3个OSD，1个MON<br>2、每个节点运行2个ceph daemon（OSD和MON）<br>3、每OSD节点1个SSD日志盘，一个SATA 1T数据盘</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# systemctl stop firewalld</span><br><span class="line">[root@localhost ~]# systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<h3 id="关闭selinux"><a href="#关闭selinux" class="headerlink" title="关闭selinux"></a>关闭selinux</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# sed	-i	&#39;s&#x2F;enforcing&#x2F;disabled&#x2F;&#39;	&#x2F;etc&#x2F;selinux&#x2F;config </span><br><span class="line">[root@localhost ~]# setenforce	0</span><br></pre></td></tr></table></figure>

<h3 id="设置主机名"><a href="#设置主机名" class="headerlink" title="设置主机名"></a>设置主机名</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# hostnamectl	set-hostname node4</span><br><span class="line"></span><br><span class="line">修改&#x2F;etc&#x2F;hosts</span><br><span class="line">192.168.1.65 node4</span><br><span class="line">192.168.1.66 node5</span><br><span class="line">192.168.1.67 node6</span><br></pre></td></tr></table></figure>

<h3 id="设置时间同步"><a href="#设置时间同步" class="headerlink" title="设置时间同步"></a>设置时间同步</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# yum install chrony -y</span><br><span class="line">[root@localhost ~]# systemctl enable chronyd.service</span><br><span class="line">[root@localhost ~]# systemctl restart chronyd.service</span><br></pre></td></tr></table></figure>

<h3 id="配置163源"><a href="#配置163源" class="headerlink" title="配置163源"></a>配置163源</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.163.com&#x2F;.help&#x2F;CentOS7-Base-163.repo</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# vim &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</span><br><span class="line">[Ceph]</span><br><span class="line">name&#x3D;Ceph packages for $basearch</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ceph&#x2F;rpm-nautilus&#x2F;el7&#x2F;$basearch</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line">priority&#x3D;1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name&#x3D;Ceph noarch packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ceph&#x2F;rpm-nautilus&#x2F;el7&#x2F;noarch</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line">priority&#x3D;1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name&#x3D;Ceph source packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ceph&#x2F;rpm-nautilus&#x2F;el7&#x2F;SRPMS</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line">priority&#x3D;1</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# yum clean all &amp;&amp; yum makecache</span><br></pre></td></tr></table></figure>

<h3 id="创建一个CEPH部署用户"><a href="#创建一个CEPH部署用户" class="headerlink" title="创建一个CEPH部署用户"></a>创建一个CEPH部署用户</h3><h4 id="在每个Ceph节点上创建一个新用户"><a href="#在每个Ceph节点上创建一个新用户" class="headerlink" title="在每个Ceph节点上创建一个新用户"></a>在每个Ceph节点上创建一个新用户</h4><p>不可创建ceph， ceph用户名为Ceph守护程序保留</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh user@ceph-server</span><br><span class="line">sudo useradd -d &#x2F;home&#x2F;&#123;username&#125; -m &#123;username&#125;</span><br><span class="line">sudo passwd &#123;username&#125;</span><br></pre></td></tr></table></figure>

<h4 id="对于您添加到每个Ceph节点的新用户，请确保该用户具有-sudo特权"><a href="#对于您添加到每个Ceph节点的新用户，请确保该用户具有-sudo特权" class="headerlink" title="对于您添加到每个Ceph节点的新用户，请确保该用户具有 sudo特权"></a>对于您添加到每个Ceph节点的新用户，请确保该用户具有 sudo特权</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;&#123;username&#125; ALL &#x3D; (root) NOPASSWD:ALL&quot; | sudo tee &#x2F;etc&#x2F;sudoers.d&#x2F;&#123;username&#125;</span><br><span class="line">sudo chmod 0440 &#x2F;etc&#x2F;sudoers.d&#x2F;&#123;username&#125;</span><br></pre></td></tr></table></figure>

<h3 id="配置免密"><a href="#配置免密" class="headerlink" title="配置免密"></a>配置免密</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node4 ~]# ssh-key                #三次回车做成空密码密钥</span><br><span class="line">[root@node4 ~]# ssh-copy-id node4</span><br><span class="line">[root@node4 ~]# ssh-copy-id node5</span><br><span class="line">[root@node4 ~]# ssh-copy-id node6</span><br></pre></td></tr></table></figure>


<h2 id="开始部署"><a href="#开始部署" class="headerlink" title="开始部署"></a>开始部署</h2><h3 id="管理节点上安装部署工具"><a href="#管理节点上安装部署工具" class="headerlink" title="管理节点上安装部署工具"></a>管理节点上安装部署工具</h3><p>注意：只在node4上安装，其他节点不用安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install ceph-deploy -y</span><br></pre></td></tr></table></figure>

<h3 id="管理节点上创建一个目录"><a href="#管理节点上创建一个目录" class="headerlink" title="管理节点上创建一个目录"></a>管理节点上创建一个目录</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir my-cluster</span><br><span class="line">cd my-cluster</span><br></pre></td></tr></table></figure>

<h3 id="重新部署-管理节点上"><a href="#重新部署-管理节点上" class="headerlink" title="重新部署,管理节点上"></a>重新部署,管理节点上</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph-deploy purge &#123;ceph-node&#125; [&#123;ceph-node&#125;]</span><br><span class="line">ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;]</span><br><span class="line">ceph-deploy forgetkeys</span><br><span class="line">rm ceph.*</span><br></pre></td></tr></table></figure>

<h3 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node4 ceph]# ceph-deploy new node4</span><br><span class="line">[root@node4 ceph]# ls</span><br><span class="line">ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring</span><br><span class="line"></span><br><span class="line">说明：</span><br><span class="line">ceph.conf               集群配置文件</span><br><span class="line">ceph-deploy-ceph.log    使用ceph-deploy部署的日志记录</span><br><span class="line">ceph.mon.keyring        验证key文件</span><br></pre></td></tr></table></figure>

<h3 id="如果您有多个网络接口，请在Ceph配置文件添加设置"><a href="#如果您有多个网络接口，请在Ceph配置文件添加设置" class="headerlink" title="如果您有多个网络接口，请在Ceph配置文件添加设置"></a>如果您有多个网络接口，请在Ceph配置文件添加设置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在[global]配置段里添加</span><br><span class="line">[root@node4 ~]# vim &#x2F;data&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line"></span><br><span class="line">public network &#x3D; 192.168.1.0&#x2F;24</span><br></pre></td></tr></table></figure>

<h3 id="安装Ceph软件包"><a href="#安装Ceph软件包" class="headerlink" title="安装Ceph软件包"></a>安装Ceph软件包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node4 my-cluster]# ceph-deploy install node2 node3 node4</span><br></pre></td></tr></table></figure>

<h3 id="部署初始化监控以及密钥"><a href="#部署初始化监控以及密钥" class="headerlink" title="部署初始化监控以及密钥"></a>部署初始化监控以及密钥</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node4 my-cluster]# ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure>

<h3 id="使用ceph-deploy复制配置文件和管理密钥到管理节点和其他节点"><a href="#使用ceph-deploy复制配置文件和管理密钥到管理节点和其他节点" class="headerlink" title="使用ceph-deploy复制配置文件和管理密钥到管理节点和其他节点"></a>使用ceph-deploy复制配置文件和管理密钥到管理节点和其他节点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node4 my-cluster]# ceph-deploy admin node2 node3 node4</span><br></pre></td></tr></table></figure>

<h3 id="部署管理器守护程序mgr"><a href="#部署管理器守护程序mgr" class="headerlink" title="部署管理器守护程序mgr"></a>部署管理器守护程序mgr</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node4 my-cluster]# ceph-deploy mgr create node4</span><br></pre></td></tr></table></figure>

<p>监控节点初始化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node4 ~]# ceph-deploy mon create-initial</span><br><span class="line">[root@node4 ~]# ceph health</span><br><span class="line">HEALTH_OK</span><br></pre></td></tr></table></figure>

<p>将配置文件信息同步到所有集群节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node4 ceph]# ceph-deploy admin node4 node5 node6</span><br></pre></td></tr></table></figure>

<p>验证ceph集群状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node4 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     b134f3bc-ab58-45c4-9d22-5d9841c56c55</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum node4 (age 16m)</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B &#x2F; 0 B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>

<p>为了防止mon单点故障，可以加多个mon节点（非必要）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ceph-deploy mon add node5</span><br><span class="line">ceph-deploy mon add node6</span><br></pre></td></tr></table></figure>

















<p>–&gt; Finished Dependency Resolution<br>Error: Package: 2:ceph-base-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liboath.so.0(LIBOATH_1.2.0)(64bit)<br>Error: Package: 2:ceph-base-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liblttng-ust.so.0()(64bit)<br>Error: Package: 2:ceph-common-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liboath.so.0(LIBOATH_1.2.0)(64bit)<br>Error: Package: 2:ceph-common-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liboath.so.0(LIBOATH_1.10.0)(64bit)<br>Error: Package: 2:ceph-common-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: libleveldb.so.1()(64bit)<br>Error: Package: 2:librgw2-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liboath.so.0()(64bit)<br>Error: Package: 2:librgw2-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liblttng-ust.so.0()(64bit)<br>Error: Package: 2:ceph-mon-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: libleveldb.so.1()(64bit)<br>Error: Package: 2:ceph-radosgw-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liboath.so.0()(64bit)<br>Error: Package: 2:ceph-base-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liboath.so.0(LIBOATH_1.12.0)(64bit)<br>Error: Package: 2:ceph-common-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liboath.so.0()(64bit)<br>Error: Package: 2:ceph-base-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liboath.so.0()(64bit)<br>Error: Package: 2:librbd1-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liblttng-ust.so.0()(64bit)<br>Error: Package: 2:ceph-common-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: libbabeltrace-ctf.so.1()(64bit)<br>Error: Package: 2:ceph-mgr-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: python-pecan<br>Error: Package: 2:ceph-base-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: libleveldb.so.1()(64bit)<br>Error: Package: 2:ceph-osd-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: libleveldb.so.1()(64bit)<br>Error: Package: 2:ceph-common-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: libbabeltrace.so.1()(64bit)<br>Error: Package: 2:librados2-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liblttng-ust.so.0()(64bit)<br>Error: Package: 2:ceph-base-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: liboath.so.0(LIBOATH_1.10.0)(64bit)<br>Error: Package: 2:ceph-mgr-14.2.5-0.el7.x86_64 (Ceph)<br>           Requires: python-bcrypt<br> You could try using –skip-broken to work around the problem<br> You could try running: rpm -Va –nofiles –nodigest</p>
<p>执行以下命令解决</p>
<p>yum install epel-release   -y</p>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title>Ceph luminous 安装配置</title>
    <url>/posts/f010e57.html</url>
    <content><![CDATA[<p><strong>备注：</strong><br>简易安装ceph luminous版本，用于kubernetes PV，详情移至Ceph官网，部署Ceph至少一个MON，三个OSD。Ceph 分布式存储集群有三大组件组成，分为：Ceph Monitor、Ceph OSD、Ceph MDS，后边使用对象存储和块存储时，MDS 非必须安装，只有当使用 Cephfs 文件存储时，才需要安装。</p>
<ul>
<li><p>自动部署osd可以采用 ceph-deploy ，也可以采用 ceph-disk 命令</p>
</li>
<li><p>luminous版本中，删除OSD可以直接使用 ceph osd purge 命令，比较省心</p>
</li>
<li><p>生产环境中磁盘的挂载尽量采用uuid/partuuid</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;docs.ceph.com&#x2F;docs&#x2F;master&#x2F;start&#x2F;quick-ceph-deploy&#x2F;</span><br></pre></td></tr></table></figure>


</li>
</ul>
<a id="more"></a>

<table>
<thead>
<tr>
<th>ip</th>
<th>hosts</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>172.16.2.30</td>
<td>server01</td>
<td>mon</td>
</tr>
<tr>
<td>172.16.2.31</td>
<td>server02</td>
<td>mon、osd，一块硬盘sdb</td>
</tr>
<tr>
<td>172.16.2.32</td>
<td>server03</td>
<td>mon、osd，三块硬盘sdb、sbc、sbd</td>
</tr>
<tr>
<td>172.16.2.33</td>
<td>server04</td>
<td>osd，一块硬盘sdb</td>
</tr>
</tbody></table>
<h1 id="环境准备-无必要说明，基本在172-16-2-30-server01操作"><a href="#环境准备-无必要说明，基本在172-16-2-30-server01操作" class="headerlink" title="环境准备(无必要说明，基本在172.16.2.30 server01操作)"></a>环境准备(无必要说明，基本在172.16.2.30 server01操作)</h1><h2 id="基础阿里源"><a href="#基础阿里源" class="headerlink" title="基础阿里源"></a>基础阿里源</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#使用阿里源</span><br><span class="line">mv &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo &#x2F;home&#x2F;CentOS-Base.repo.bak</span><br><span class="line">wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br></pre></td></tr></table></figure>

<h2 id="ceph阿里源"><a href="#ceph阿里源" class="headerlink" title="ceph阿里源"></a>ceph阿里源</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#创建ceph源</span><br><span class="line">echo &#39;</span><br><span class="line">[ceph]</span><br><span class="line">name&#x3D;ceph</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-luminous&#x2F;el7&#x2F;x86_64&#x2F;</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name&#x3D;cephnoarch</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-luminous&#x2F;el7&#x2F;noarch&#x2F;</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">[ceph-source]</span><br><span class="line">name&#x3D;ceph-source</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-luminous&#x2F;el7&#x2F;SRPMS&#x2F;</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">&#39;&gt;&#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</span><br><span class="line">#生成缓存</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br></pre></td></tr></table></figure>

<h2 id="关闭selinux、防火墙-每台服务器运行"><a href="#关闭selinux、防火墙-每台服务器运行" class="headerlink" title="关闭selinux、防火墙(每台服务器运行)"></a>关闭selinux、防火墙(每台服务器运行)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld.service</span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line">firewall-cmd --state</span><br><span class="line">sed -i &#39;&#x2F;^SELINUX&#x3D;.*&#x2F;c SELINUX&#x3D;disabled&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUXTYPE&#x3D;.*&#x2F;SELINUXTYPE&#x3D;disabled&#x2F;g&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">grep --color&#x3D;auto &#39;^SELINUX&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure>

<h2 id="时间同步（每台服务器运行）"><a href="#时间同步（每台服务器运行）" class="headerlink" title="时间同步（每台服务器运行）"></a>时间同步（每台服务器运行）</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install ntp -y</span><br><span class="line">&#x2F;usr&#x2F;sbin&#x2F;ntpdate ntp6.aliyun.com </span><br><span class="line">echo &quot;*&#x2F;3 * * * * &#x2F;usr&#x2F;sbin&#x2F;ntpdate ntp6.aliyun.com  &amp;&gt; &#x2F;dev&#x2F;null&quot; &gt; &#x2F;tmp&#x2F;crontab</span><br><span class="line">crontab &#x2F;tmp&#x2F;crontab</span><br></pre></td></tr></table></figure>

<h2 id="hosts（每台服务器运行）"><a href="#hosts（每台服务器运行）" class="headerlink" title="hosts（每台服务器运行）"></a>hosts（每台服务器运行）</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &#39;#ceph</span><br><span class="line">172.16.2.30 server01</span><br><span class="line">172.16.2.31 server02</span><br><span class="line">172.16.2.32 server03</span><br><span class="line">172.16.2.33 server03</span><br><span class="line">&#39;&gt;&gt;&#x2F;etc&#x2F;hosts</span><br></pre></td></tr></table></figure>

<h2 id="添加硬盘、查看状态，无需分区、格式化"><a href="#添加硬盘、查看状态，无需分区、格式化" class="headerlink" title="添加硬盘、查看状态，无需分区、格式化"></a>添加硬盘、查看状态，无需分区、格式化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如在server04、server03、server02添加硬盘，如server03</span><br><span class="line">[root@server03 ~]# lsblk </span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">fd0               2:0    1    4K  0 disk </span><br><span class="line">sda               8:0    0  100G  0 disk </span><br><span class="line">├─sda1            8:1    0    1G  0 part &#x2F;boot</span><br><span class="line">└─sda2            8:2    0   99G  0 part </span><br><span class="line">  ├─centos-root 253:0    0   50G  0 lvm  &#x2F;</span><br><span class="line">  ├─centos-swap 253:1    0  7.9G  0 lvm  </span><br><span class="line">  └─centos-home 253:2    0 41.1G  0 lvm  &#x2F;home</span><br><span class="line">sdb               8:16   0   10G  0 disk </span><br><span class="line">sdc               8:32   0   10G  0 disk </span><br><span class="line">sdd               8:48   0   10G  0 disk </span><br><span class="line">sr0              11:0    1  4.2G  0 rom</span><br></pre></td></tr></table></figure>

<h2 id="免密访问"><a href="#免密访问" class="headerlink" title="免密访问"></a>免密访问</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-kengen</span><br><span class="line">ssh-copy-id root@172.16.2.31</span><br><span class="line">ssh-copy-id root@172.16.2.32</span><br></pre></td></tr></table></figure>

<h1 id="安装Ceph"><a href="#安装Ceph" class="headerlink" title="安装Ceph"></a>安装Ceph</h1><h2 id="更新仓库并安装ceph-deploy"><a href="#更新仓库并安装ceph-deploy" class="headerlink" title="更新仓库并安装ceph-deploy"></a>更新仓库并安装ceph-deploy</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum update &amp;&amp; yum install ceph-deploy -y</span><br></pre></td></tr></table></figure>

<h2 id="创建集群配置"><a href="#创建集群配置" class="headerlink" title="创建集群配置"></a>创建集群配置</h2><p>ceph-deploy 工具部署集群前需要创建一些集群配置信息，其保存在 ceph.conf 文件中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建集群配置目录</span><br><span class="line">mkdir ceph-cluster &amp;&amp; cd ceph-cluster</span><br><span class="line"># 创建 monitor-node</span><br><span class="line">ceph-deploy new server01 server02 server03</span><br><span class="line"># 追加 OSD 副本数量(测试虚拟机总共有3台)</span><br><span class="line">echo &quot;osd pool default size &#x3D; 3&quot; &gt;&gt; ceph.conf</span><br></pre></td></tr></table></figure>

<h2 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h2><p>创建集群使用 ceph-deploy 工具即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装 ceph</span><br><span class="line">ceph-deploy install server01 server02 server03 server04</span><br><span class="line">#yum install -y ceph-radosgw ceph ceph-release ceph-common</span><br><span class="line"># 初始化 monitor node 和 秘钥文件</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line"># 部署 ceph cli 工具和秘钥文件,以便在各个 Node 上使用 ceph 命令时，无需指定 monitor 地址和 ceph.client.admin.keyring 密钥</span><br><span class="line">ceph-deploy admin server01 server02 server03 server04</span><br><span class="line"># 部署管理守护进程</span><br><span class="line">ceph-deploy mgr create server01 server02 server03</span><br><span class="line"># 添加osd存储，添加硬盘，生产环境需要在OSD节点添加</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;sdb server02</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;sdb server03</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;sdc server03</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;sdd server03</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;sdb server04</span><br><span class="line"></span><br><span class="line"># 假如重新部署ceph，删ceph之前没有删除osd，可以使用以下命令</span><br><span class="line">lvscan</span><br><span class="line">vgscan</span><br><span class="line">pvscan</span><br><span class="line"></span><br><span class="line">lvremove</span><br><span class="line">vgremove</span><br><span class="line">pvremove</span><br><span class="line"></span><br><span class="line"># 检测集群状态</span><br><span class="line">ceph health</span><br><span class="line"># 查看集群状态</span><br><span class="line">ceph -s</span><br><span class="line"></span><br><span class="line">#启动dashboard</span><br><span class="line">ceph mgr module enable dashboard</span><br><span class="line">ceph config-key put mgr&#x2F;dashboard&#x2F;server_addr 172.16.2.30</span><br><span class="line">ceph config-key put mgr&#x2F;dashboard&#x2F;server_port 7000</span><br><span class="line"></span><br><span class="line"># 重置集群重新部署</span><br><span class="line">ceph-deploy purge &#123;ceph-node&#125; [&#123;ceph-node&#125;]</span><br><span class="line">ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;]</span><br><span class="line">ceph-deploy forgetkeys</span><br><span class="line">rm ceph.*</span><br></pre></td></tr></table></figure>

<h2 id="新增组件-如mon"><a href="#新增组件-如mon" class="headerlink" title="新增组件,如mon"></a>新增组件,如mon</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建 MDS</span><br><span class="line">ceph-deploy mds create server01</span><br><span class="line"># 创建 RGW</span><br><span class="line">ceph-deploy rgw create server01</span><br><span class="line"># 增加 monitor</span><br><span class="line">echo &quot;public network &#x3D; 172.16.2.0&#x2F;24&quot; &gt;&gt; ceph.conf</span><br><span class="line">ceph-deploy --overwrite-conf mon add server02</span><br><span class="line">ceph-deploy --overwrite-conf mon add server03</span><br><span class="line"># 查看仲裁信息</span><br><span class="line">ceph quorum_status --format json-pretty</span><br></pre></td></tr></table></figure>

<h2 id="创建块设备"><a href="#创建块设备" class="headerlink" title="创建块设备"></a>创建块设备</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建存储池</span><br><span class="line">rados mkpool data</span><br><span class="line"># 创建 image</span><br><span class="line">rbd create data --size 10240 -p data</span><br><span class="line"># 关闭不支持特性</span><br><span class="line">rbd feature disable data exclusive-lock, object-map, fast-diff, deep-flatten -p data</span><br><span class="line"># 映射(每个节点都要映射)</span><br><span class="line">rbd map data --name client.admin -p data</span><br><span class="line"># 格式化块设备(单节点即可)</span><br><span class="line">mkfs.xfs &#x2F;dev&#x2F;rbd0</span><br></pre></td></tr></table></figure>

<h2 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h2><p>```<br>#增加pool<br>ceph osd pool create kube 128<br>#删除 pool,先在ceph.conf 增加下面:<br>mon_allow_pool_delete = true<br>#重启ceph-mon：<br>systemctl restart ceph-mon@server01<br>#删除<br>ceph osd pool rm kube kube –yes-i-really-really-mean-it</p>
]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title>清除kubernetes pod产生的日志文件logs</title>
    <url>/posts/8cdb5793.html</url>
    <content><![CDATA[<p>随着系统运行的时间越长，Log日志所占的空间也越来越大，但是磁盘空间是一定的，这时就需要清理一下这些无用的日志文件。</p>
<a id="more"></a>
<h1 id="手动清理"><a href="#手动清理" class="headerlink" title="手动清理"></a>手动清理</h1><p>进入 /var/lib/docker/containers 中找到较大的 .log 文件，在管理员权限下(sudo su root)使用命令 cat /dev/null &gt; xxx.log 清空文件。 注意直接删除文件可能导致后续log无法写入。</p>
<h1 id="对日志设置自动回滚写入以及清除"><a href="#对日志设置自动回滚写入以及清除" class="headerlink" title="对日志设置自动回滚写入以及清除"></a>对日志设置自动回滚写入以及清除</h1><p>通过logrotate服务实现日志定期清理和回卷</p>
<p>logrotate是个十分有用的工具，它可以自动对日志进行截断（或轮循）、压缩以及删除旧的日志文件。例如，你可以设置logrotate，让/var/log/foo日志文件每30天轮循，并删除超过6个月的日志。配置完后，logrotate的运作完全自动化，不必进行任何进一步的人为干预。</p>
<p>但如果按照之前的部署方式，需要手动在每个节点上都安装和配置对应logrotate工具。如果通过Kubernetes容器服务编排的能力，将logrotate通过Kubernetes中服务的方式部署到各个节点上，这样既可以实现只需要一次部署，部署到所有节点。并且通过容器的方式保证了logrotate配置的一致性。</p>
<p>方案的具体实现是在Kubernetes集群中，创建DaemonSet资源实现。DaemonSet资源会在每个Node节点上都部署一个logrotate的容器实例，并且在容器实例中设置映射主机的log日志目录，从而实现日志的定时清理和回卷。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: logrotate</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: logging</span><br><span class="line">        id: logrotate</span><br><span class="line">      name: logrotate</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: logrotate-es</span><br><span class="line">        image: blacklabelops&#x2F;logrotate</span><br><span class="line">        securityContext:</span><br><span class="line">          privileged: true</span><br><span class="line">        volumeMounts:</span><br><span class="line">         - name: containers</span><br><span class="line">           mountPath: &#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers</span><br><span class="line">         - name: varlog</span><br><span class="line">           mountPath: &#x2F;var&#x2F;log&#x2F;docker</span><br><span class="line">         - name: logs</span><br><span class="line">           mountPath: &#x2F;logs</span><br><span class="line">        env:</span><br><span class="line">        - name: LOGS_DIRECTORIES</span><br><span class="line">          value: &quot;&#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers &#x2F;var&#x2F;log&#x2F;docker&quot;</span><br><span class="line">        - name: LOGROTATE_INTERVAL</span><br><span class="line">          value: &quot;hourly&quot;</span><br><span class="line">        - name: LOGROTATE_OLDDIR</span><br><span class="line">          value: &quot;&#x2F;logs&quot;</span><br><span class="line">      volumes:</span><br><span class="line">         - hostPath:</span><br><span class="line">             path: &#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers</span><br><span class="line">           name: containers</span><br><span class="line">         - hostPath:</span><br><span class="line">             path: &#x2F;var&#x2F;log&#x2F;docker</span><br><span class="line">           name: varlog</span><br><span class="line">         - hostPath:</span><br><span class="line">             path: &#x2F;var&#x2F;log&#x2F;containers&#x2F;</span><br><span class="line">           name: logs</span><br></pre></td></tr></table></figure>

<p>在示例的yaml文件中，logrotate服务将按照定时(1小时)的对日志进行回卷，回卷超过5个副本后则会对日志进行清理。如果有需要，可以修改相应的参数，设置不同的回卷规则和清理规则。详细的参数说明可以参考：<a href="https://github.com/blacklabelops/logrotate">https://github.com/blacklabelops/logrotate</a> </p>
<p>在kubernetes执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kubectl create -f logrotate_ds.yaml</span><br><span class="line">daemonset &quot;logrotate&quot; created</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows 环境部署可视化管理系统</title>
    <url>/posts/33e91186.html</url>
    <content><![CDATA[<h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><p>系统： windows server 2016<br>内网环境，无外网</p>
<a id="more"></a>
<h1 id="JDK1-8"><a href="#JDK1-8" class="headerlink" title="JDK1.8"></a>JDK1.8</h1><h2 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h2><p>官网下载：<a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html">https://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p>
<h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><p>配置环境变量：此电脑右击属性–高级系统设置–环境变量</p>
<p>添加 JAVA_HOME 环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Program Files\Java\jdk1.8.0_221</span><br></pre></td></tr></table></figure>

<p>添加 JRE_HOME 环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Program Files\Java\jdk1.8.0_221\jre</span><br></pre></td></tr></table></figure>

<p>将变量添加到系统的path目录中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">%JAVA_HOME%\bin</span><br><span class="line">%JRE_HOME%\bin</span><br></pre></td></tr></table></figure>

<h1 id="Tomcat8"><a href="#Tomcat8" class="headerlink" title="Tomcat8"></a>Tomcat8</h1><h2 id="下载安装-1"><a href="#下载安装-1" class="headerlink" title="下载安装"></a>下载安装</h2><p>官网下载：<a href="https://tomcat.apache.org/download-80.cgi">https://tomcat.apache.org/download-80.cgi</a></p>
<p>添加 TOMCAT_HOME 环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\apache-tomcat-8.5.43</span><br></pre></td></tr></table></figure>

<p>将变量添加到系统的path目录中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">%TOMCAT_HOME%\bin</span><br></pre></td></tr></table></figure>

<h2 id="添加Tomcat服务"><a href="#添加Tomcat服务" class="headerlink" title="添加Tomcat服务"></a>添加Tomcat服务</h2><p>cmd命令行界面进入Tomcat解压目录的bin目录 安装tomcat服务运行 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service.bat install</span><br></pre></td></tr></table></figure>
<p>如果要清除掉，之前添加的tamcat </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service remove tomcat8</span><br></pre></td></tr></table></figure>

<h1 id="Mysql5-7"><a href="#Mysql5-7" class="headerlink" title="Mysql5.7"></a>Mysql5.7</h1><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>5.7版本和之前的不一样： </p>
<ul>
<li><p>文件夹中没有DATA目录</p>
</li>
<li><p>没有mysql默认库</p>
</li>
<li><p>没有默认的my.ini或参考的my-default.ini</p>
</li>
</ul>
<p>Windows的路径使用了反斜杠（\），因此，配置中使用时尽量合乎规范将反斜杠改为双反斜杠()或直接使用斜杠（/）。（也有直接使用\而不受影响的）</p>
<h2 id="下载mysql5-7-zip安装包"><a href="#下载mysql5-7-zip安装包" class="headerlink" title="下载mysql5.7 zip安装包"></a>下载mysql5.7 zip安装包</h2><ul>
<li>官网下载压缩包链接： <a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads">https://dev.mysql.com/downloads/mysql/5.7.html#downloads</a></li>
</ul>
<p>根据电脑配置选择32/64位版本，将下载回来的mysql压缩包解压至适当路径，也就是你打算以后使用的工作目录</p>
<h2 id="配置环境变量-1"><a href="#配置环境变量-1" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><p>配置环境变量：此电脑右击属性–高级系统设置–环境变量–系统变量–Path</p>
<p>双击path，新建mysql/bin 所在目录，如C:\mysql-5.7.27-winx64\bin</p>
<h2 id="配置my-ini"><a href="#配置my-ini" class="headerlink" title="配置my.ini"></a>配置my.ini</h2><p>mysql的目录先新建my.ini</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"># 服务端配置</span><br><span class="line"></span><br><span class="line"># 设置mysql的工作目录，安装包解压后的路径</span><br><span class="line">basedir&#x3D;E:\\mysql</span><br><span class="line"></span><br><span class="line"># 数据存放目录data，需要自行新建</span><br><span class="line"># 也可以使用mysqld --initialize-insecure 命令后也会自动在根目录中生成data目录</span><br><span class="line">datadir&#x3D;E:\\mysql\data</span><br><span class="line"></span><br><span class="line"># 默认连接端口3306，正式环境一般都会修改</span><br><span class="line">port&#x3D;3306</span><br><span class="line"></span><br><span class="line"># 设置mysql默认字符集为utf-8</span><br><span class="line">character-set-server&#x3D;utf8</span><br><span class="line"></span><br><span class="line"># 限制server接受的数据包的大小</span><br><span class="line">max_allowed_packet&#x3D;500M</span><br><span class="line"></span><br><span class="line"># 设置的最大连接数</span><br><span class="line">max_connections&#x3D;1000</span><br><span class="line"></span><br><span class="line">skip-host-cache</span><br><span class="line">skip-name-resolve</span><br><span class="line"></span><br><span class="line">[client]</span><br><span class="line"># 客户端配置</span><br><span class="line"></span><br><span class="line">default-character-set&#x3D;utf8</span><br><span class="line">#设置mysql默认字符集为utf-8</span><br></pre></td></tr></table></figure>

<h2 id="初始化mysql"><a href="#初始化mysql" class="headerlink" title="初始化mysql"></a>初始化mysql</h2><ul>
<li>以管理员身份运行cmd命令</li>
</ul>
<ul>
<li>mysql初始化</li>
</ul>
<ol>
<li><p>输入cd 对应mysql\bin目录,例如我的是E:\mysql\bin，进入bin目录下</p>
</li>
<li><p>输入mysqld –initialize（初始化）</p>
</li>
<li><p>或mysqld –initialize-insecure（不安全的选项）</p>
</li>
<li><p>两者取决于你是否希望服务器生成一个拥有随机初始密码的root@localhost的账户。后者直接是空密码创建。</p>
</li>
<li><p>为了能够方便查看初始化过程中的信息，可以追加 –console 参数使mysqld将输出信息写到控制台。</p>
</li>
<li><p>一般linux系统才需要追加–user=mysql之类来指定用户（事先设好读写权限）</p>
</li>
</ol>
<h2 id="安装（到windwos）服务"><a href="#安装（到windwos）服务" class="headerlink" title="安装（到windwos）服务"></a>安装（到windwos）服务</h2><ul>
<li><p>同样在管理员权限的cmd中操作，安装成服务更便利开机启动。</p>
</li>
<li><p>输入mysqld –install</p>
</li>
<li><p>成功会如图显示Servers Successfully installed </p>
</li>
<li><p>若需要指定配置文件（mysql多实例的）则可以在–install后面跟自定义的服务名和–defaults-file选项来指定配置文件。 mysqld –install MySQL –defaults-file=E:\mysql\my.ini </p>
</li>
<li><p>上述命令可以在安装时指定服务名为 MySQL 以及指定配置文件路径，需要注意的是：–install必须是第一个参数， 且服务名（若需指定的话）必须紧跟其后。</p>
</li>
<li><p>如果不想让MySQL服务每次开机都自动启动，可以使用–install-manual 参数代替 –install 参数。</p>
</li>
<li><p>控制台下输入net start mysql 启动mysql服务。</p>
</li>
<li><p>卸载mysql服务使用的命令是mysqld –remove</p>
</li>
<li><p>正常退出和关闭mysql服务使用net stop mysql </p>
</li>
<li><p>也有使用mysqladmin自带的管理工具来关闭的，前提是进入到bin目录下运行：mysqladmin -u root -p shutdown </p>
</li>
<li><p>查看相关进程号使用的是tasklist | findstr mysql</p>
</li>
<li><p>杀死进程（不推荐）的是taskkill /F /PID 进程号 </p>
</li>
</ul>
<h2 id="登录和重设密码"><a href="#登录和重设密码" class="headerlink" title="登录和重设密码"></a>登录和重设密码</h2><ul>
<li><p>mysql -uroot -p输入前面记下的随机密码登入。 </p>
</li>
<li><p>若之前你使用–initialize-insecure 参数初始化，则使用如下命令来连接MySQL：mysql -uroot –skip-password或同样使用mysql -uroot -p在提示输入密码时直接回车即可进入。</p>
</li>
<li><p>一般第一次可用mysqladmin方式重设root密码，mysqladmin -uroot -p password </p>
</li>
<li><p>重新正常登录数据库后，也可直接在mysql内直接修改用户权限或user表方式修改密码。mysql&gt; update mysql.user set authentication_string=password(‘’) where User=”root “ and host=”localhost”;</p>
</li>
<li><p>5.7以前的版本是使用password字段保存密码的，5.7改成了authentication_string，不容易记忆。修改密码后需要输入flush privileges;命令来刷新生效。</p>
</li>
<li><p>由于authentication_string不太容易记忆，也有直接改权限的，ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘123456’ </p>
</li>
</ul>
<h2 id="强制跳过密码登录"><a href="#强制跳过密码登录" class="headerlink" title="强制跳过密码登录"></a>强制跳过密码登录</h2><ul>
<li>第一种，临时跳过密码。</li>
</ul>
<p>执行 mysqld –skip_grant_tables 启动服务<br>注意此时应再开多一个cmd窗口来作为客户端连接服务端，登录的时候直接回车无需密码。<br>然后就是重设密码了。设置成功后记得停止mysqld服务，重新启动正常需密码的服务。</p>
<ul>
<li>第二种，需长期跳过密码（所有用户都不用密码即可连接）。</li>
</ul>
<p>在配置文件，my.ini中的[mysqld]下添加一行<br>skip_grant_tables表示跳过权限表。<br>再执行mysql服务启动，就是无权限管理的连接了。极不安全，只适用于个人测试或学习环境。<br>此配置一成功后，客户端连接mysql只需敲mysql就直接进mysql了。</p>
<h2 id="设置友好提示符"><a href="#设置友好提示符" class="headerlink" title="设置友好提示符"></a>设置友好提示符</h2><p>连接上去，使用的时候，你会发觉MySQL 客户端的默认提示符是 “mysql&gt;”，基本上没什么实际作用。 修改这个提示符，让它显示一些有用的信息，例如当前所在的数据库等。 修改方法有四种，其中前两种只对当前连接有效，后两种则对所有连接有效。</p>
<ol>
<li><p>连接客户端时通过参数指定。<br>mysql –prompt=”(\u@\h) [\d]&gt; “<br>这样提示符就会变成 (user@host) [database]&gt;<br>其中常用的字符参数有：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\D 完整的日期 </span><br><span class="line">\d 当前数据库 </span><br><span class="line">\h 服务器地址 </span><br><span class="line">\u 用户名</span><br></pre></td></tr></table></figure></li>
<li><p>连接上客户端后，通过 prompt命令 PROMPT (\u@\h) [\d]&gt;修改。<br>例：<br>mysql&gt; PROMPT (\u@\h)[\d]&gt;<br>PROMPT set to ‘(\u@\h)[\d]&gt;’</p>
</li>
<li><p>在 MySQL 的配置文件中配置。<br>[mysql]<br>prompt=\u@\h [\d]&gt;\</p>
</li>
<li><p>通过环境变量配置。<br>export MYSQL_PS1=”\u@\h [\d]&gt; “</p>
</li>
</ol>
<h2 id="开启远程连接"><a href="#开启远程连接" class="headerlink" title="开启远程连接"></a>开启远程连接</h2><p>开启远程连接，输入命令GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’%’ IDENTIFIED BY ‘密码’ WITH GRANT OPTION;<br>刷新权限，命令为：FLUSH PRIVILEGES;</p>
<h2 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h2><p>问题1：<br>在windows 2016 服务器上执行mysql提示MSVCR120.dll缺少的错误</p>
<p>解决方案：<br>官网<a href="https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads">https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads</a> 下载Visual Studio 2013 (VC++ 12.0)安装即可</p>
<p>问题2：<br>重启mysql时出现</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\Administrator&gt;net start mysql</span><br><span class="line">MySQL 服务正在启动 ....................</span><br><span class="line">MySQL 服务无法启动。</span><br><span class="line"></span><br><span class="line">请键入 NET HELPMSG 3523 以获得更多的帮助。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\Administrator&gt;net start mysql</span><br><span class="line">服务正在启动或停止中，请稍候片刻后再试一次。</span><br></pre></td></tr></table></figure>

<p>解决方案:<br>最终发现是由于编码原因导致的。因为在这个过程中用记事本打开过配置文件my.ini，不小心把编码变为了utf-8，所以解决方案就是把my.ini编码重新改回ANSI编码就解决了。</p>
<h1 id="项目部署"><a href="#项目部署" class="headerlink" title="项目部署"></a>项目部署</h1><h2 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h2><p>1、导入数据库<br>2、修改数据库，dbglobal***<br>3、由于是本地环境，需创建表t_cmm_server_info，插入字段alter table t_file add column <code>f_dir</code> varchar(128) CHARACTER SET utf8mb4 DEFAULT NULL COMMENT ‘文件所在目录’;</p>
<h2 id="web"><a href="#web" class="headerlink" title="web"></a>web</h2><p>修改配置表，如mysql、mongodb</p>
<h2 id="文件服务"><a href="#文件服务" class="headerlink" title="文件服务"></a>文件服务</h2><p>修改配置表</p>
<h2 id="通信服务"><a href="#通信服务" class="headerlink" title="通信服务"></a>通信服务</h2><p>修改配置表</p>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>JAVA</tag>
        <tag>TOMCAT</tag>
        <tag>MYSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>搭建FastDFS分布式文件系统</title>
    <url>/posts/9de8b011.html</url>
    <content><![CDATA[<h2 id="FastDFS介绍"><a href="#FastDFS介绍" class="headerlink" title="FastDFS介绍"></a>FastDFS介绍</h2><p>FastDFS是一款类Google FS的开源分布式文件系统，它用纯C语言实现，支持Linux、FreeBSD、AIX等UNIX系统。它只能通过专有API对文件进行存取访问，不支持POSIX接口方式，不能mount使用。准确地讲，Google FS以及FastDFS、mogileFS、HDFS、TFS等类Google FS都不是系统级的分布式文件系统，而是应用级的分布式文件存储服务。</p>
<a id="more"></a>
<p>FastDFS是一个开源的，高性能的的分布式文件系统，他主要的功能包括：文件存储，同步和访问，设计基于高可用和负载均衡，fastfd非常适用于基于文件服务的站点，例如图片分享和视频分享网站。<br>FastDFS有两个角色：跟踪服务和存储服务，跟踪服务控制，调度文件以负载均衡的方式访问；存储服务包括：文件存储，文件同步，提供文件访问接口，同时以key value的方式管理文件的元数据。<br>跟踪和存储服务可以由1台或者多台服务器组成，同时可以动态的添加，删除跟踪和存储服务而不会对在线的服务产生影响，在集群中，tracker服务是对等的。<br>存储系统由一个或多个卷组成，卷与卷之间的文件是相互独立的，所有卷的文件容量累加就是整个存储系统中的文件容量。一个卷可以由一台或多台存储服务器组成，一个卷下的存储服务器中的文件都是相同的，卷中的多台存储服务器起到了冗余备份和负载均衡的作用。在卷中增加服务器时，同步已有的文件由系统自动完成，同步完成后，系统自动将新增服务器切换到线上提供服务。当存储空间不足或即将耗尽时，可以动态添加卷。只需要增加一台或多台服务器，并将它们配置为一个新的卷，这样就扩大了存储系统的容量。</p>
<h2 id="FastDFS架构"><a href="#FastDFS架构" class="headerlink" title="FastDFS架构"></a>FastDFS架构</h2><p><img src="http://imglf1.nosdn.127.net/img/R3hDdlA4YitONFpPSXdaUEVIeXNxd20wOGd2UFBkR0NIRGkwTzN3RjlOcklzS3NzTzlFY0pRPT0.gif" alt=""><br>客户端和Storage server主动连接Tracker server。Storage server主动向Tracker server报告其状态信息，包括磁盘剩余空间、文件同步状况、文件上传下载次数等统计信息。Storage server会连接集群中所有的Tracker server，向他们报告自己的状态。Storage server启动一个单独的线程来完成对一台Tracker server的连接和定时报告。需要说明的是，一个组包含的Storage server不是通过配置文件设定的，而是通过Tracker server获取到的。<br>不同组的Storage server之间不会相互通信，同组内的Storage server之间会相互连接进行文件同步。<br>Storage server采用binlog文件记录文件上传、删除等更新操作。binlog中只记录文件名，不记录文件内容。<br>文件同步只在同组内的Storage server之间进行，采用push方式，即源头服务器同步给目标服务器。只有源头数据才需要同步，备份数据并不需要再次同步，否则就构成环路了。有个例外，就是新增加一台Storage server时，由已有的一台Storage server将已有的所有数据（包括源头数据和备份数据）同步给该新增服务器。<br>Storage server中由专门的线程根据binlog进行文件同步。为了最大程度地避免相互影响以及出于系统简洁性考虑，Storage server对组内除自己以外的每台服务器都会启动一个线程来进行文件同步。<br>文件同步采用增量同步方式，系统记录已同步的位置（binlog文件偏移量）到标识文件中。标识文件名格式：{dest storage IP}_{port}.mark，例如：192.168.1.14_23000.mark。</p>
<h2 id="FastDFS文件上传下载交互过程"><a href="#FastDFS文件上传下载交互过程" class="headerlink" title="FastDFS文件上传下载交互过程"></a>FastDFS文件上传下载交互过程</h2><p><img src="http://imglf.nosdn.127.net/img/R3hDdlA4YitONFpPSXdaUEVIeXNxNjRjV2FWQjZZMHovREprSGx6ZnBNSmFoUXp3WDdVZ0p3PT0.gif" alt=""></p>
<h3 id="文件下载流程"><a href="#文件下载流程" class="headerlink" title="文件下载流程"></a>文件下载流程</h3><ol>
<li>Client询问Tracker server可以下载指定文件的Storage server，参数为文件ID（包含组名和文件名）；</li>
<li>Tracker server返回一台可用的Storage server；</li>
<li>Client直接和该Storage server建立连接，完成文件下载。<br><img src="http://imglf2.nosdn.127.net/img/R3hDdlA4YitONFpPSXdaUEVIeXNxekFWSXFDQW5HOHFTUklySk5adlNHbVJZRDVMMEpkMDBnPT0.gif" alt=""><h3 id="文件上传流程"><a href="#文件上传流程" class="headerlink" title="文件上传流程"></a>文件上传流程</h3></li>
<li>Client询问Tracker server上传到的Storage server；</li>
<li>Tracker server返回一台可用的Storage server，返回的数据为该Storage server的IP地址和端口；</li>
<li>Client直接和该Storage server建立连接，进行文件上传，Storage server返回新生成的文件ID，文件上传结束。</li>
</ol>
<h2 id="FastDFS安装"><a href="#FastDFS安装" class="headerlink" title="FastDFS安装"></a>FastDFS安装</h2><h3 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CentOS Linux release 7.2.1511 (Core)</span><br></pre></td></tr></table></figure>

<h3 id="下载并安装FastDFS依赖包libfastcommon"><a href="#下载并安装FastDFS依赖包libfastcommon" class="headerlink" title="下载并安装FastDFS依赖包libfastcommon"></a>下载并安装FastDFS依赖包libfastcommon</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# wget https:&#x2F;&#x2F;codeload.github.com&#x2F;happyfish100&#x2F;libfastcommon&#x2F;zip&#x2F;master</span><br><span class="line">[root@object1 ~]# unzip master</span><br><span class="line">[root@object1 ~]# cd libfastcommon-master&#x2F;</span><br><span class="line">[root@object1 libfastcommon-master]# .&#x2F;make.sh</span><br><span class="line">[root@object1 libfastcommon-master]# .&#x2F;make.sh install</span><br></pre></td></tr></table></figure>

<h3 id="下载并安装FastDFS"><a href="#下载并安装FastDFS" class="headerlink" title="下载并安装FastDFS"></a>下载并安装FastDFS</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# wget https:&#x2F;&#x2F;nchc.dl.sourceforge.net&#x2F;project&#x2F;fastdfs&#x2F;FastDFS%20Server%20Source%20Code&#x2F;FastDFS%20Server%20with%20PHP%20Extension%20Source%20Code%20V5.08&#x2F;FastDFS_v5.08.tar.gz</span><br><span class="line">[root@object1 ~]# tar zxvf FastDFS_v5.08.tar.gz </span><br><span class="line">[root@object1 ~]# cd FastDFS</span><br><span class="line">[root@object1 FastDFS]# .&#x2F;make.sh &amp;&amp; .&#x2F;make.sh install</span><br></pre></td></tr></table></figure>

<h3 id="默认脚本目录"><a href="#默认脚本目录" class="headerlink" title="默认脚本目录"></a>默认脚本目录</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# ll &#x2F;etc&#x2F;init.d&#x2F; | grep fdfs</span><br></pre></td></tr></table></figure>

<h3 id="样例配置文件"><a href="#样例配置文件" class="headerlink" title="样例配置文件"></a>样例配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# ll &#x2F;etc&#x2F;fdfs&#x2F;</span><br></pre></td></tr></table></figure>

<p><strong>注意：虽然FastDFS区分tracker和storage服务器，但是安装的软件及步骤均相同，只是不同的配置文件而已，因此以上安装适用tracker server和storage server</strong></p>
<h3 id="配置跟踪服务器（tracker-server）"><a href="#配置跟踪服务器（tracker-server）" class="headerlink" title="配置跟踪服务器（tracker server）"></a>配置跟踪服务器（tracker server）</h3><h4 id="拷贝tracker-server和client端样例配置文件并重命名"><a href="#拷贝tracker-server和client端样例配置文件并重命名" class="headerlink" title="拷贝tracker server和client端样例配置文件并重命名"></a>拷贝tracker server和client端样例配置文件并重命名</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# cp &#x2F;etc&#x2F;fdfs&#x2F;tracker.conf.sample &#x2F;etc&#x2F;fdfs&#x2F;tracker.conf</span><br><span class="line">[root@object1 ~]# cp &#x2F;etc&#x2F;fdfs&#x2F;client.conf.sample &#x2F;etc&#x2F;fdfs&#x2F;client.conf</span><br></pre></td></tr></table></figure>

<h4 id="修改client-conf"><a href="#修改client-conf" class="headerlink" title="修改client.conf"></a>修改client.conf</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第14行</span><br><span class="line"> tracker_server&#x3D;192.168.1.226:22122</span><br></pre></td></tr></table></figure>

<h4 id="启动tracker-server"><a href="#启动tracker-server" class="headerlink" title="启动tracker server"></a>启动tracker server</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# &#x2F;etc&#x2F;init.d&#x2F;fdfs_trackerd start</span><br><span class="line">[root@object1 ~]# ss -tunlp | grep 22122</span><br></pre></td></tr></table></figure>

<h3 id="配置存储服务器（storage-server）"><a href="#配置存储服务器（storage-server）" class="headerlink" title="配置存储服务器（storage server）"></a>配置存储服务器（storage server）</h3><h4 id="拷贝storage-server样例配置文件并重命名"><a href="#拷贝storage-server样例配置文件并重命名" class="headerlink" title="拷贝storage server样例配置文件并重命名"></a>拷贝storage server样例配置文件并重命名</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# cp &#x2F;etc&#x2F;fdfs&#x2F;storage.conf.sample &#x2F;etc&#x2F;fdfs&#x2F;storage.conf</span><br></pre></td></tr></table></figure>

<h4 id="修改storage-conf"><a href="#修改storage-conf" class="headerlink" title="修改storage.conf"></a>修改storage.conf</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第118行</span><br><span class="line">tracker_server&#x3D;192.168.1.226:22122</span><br></pre></td></tr></table></figure>

<h4 id="启动storage-server（启动storage-server的前提是tracker-server必须事先已启动）"><a href="#启动storage-server（启动storage-server的前提是tracker-server必须事先已启动）" class="headerlink" title="启动storage server（启动storage server的前提是tracker server必须事先已启动）"></a>启动storage server（启动storage server的前提是tracker server必须事先已启动）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# &#x2F;etc&#x2F;init.d&#x2F;fdfs_storaged start</span><br><span class="line">[root@object1 ~]# ss -tunlp | grep 23000</span><br></pre></td></tr></table></figure>

<h3 id="文件上传测试"><a href="#文件上传测试" class="headerlink" title="文件上传测试"></a>文件上传测试</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# &#x2F;usr&#x2F;bin&#x2F;fdfs_upload_file &#x2F;etc&#x2F;fdfs&#x2F;client.conf &#x2F;root&#x2F;test.jpg </span><br><span class="line">group1&#x2F;M00&#x2F;00&#x2F;00&#x2F;wKgB4lkJlSSADZazAAMuQxWPTP8989.jpg</span><br></pre></td></tr></table></figure>

<h2 id="存储服务器（storage-server）安装并配置nginx"><a href="#存储服务器（storage-server）安装并配置nginx" class="headerlink" title="存储服务器（storage server）安装并配置nginx"></a>存储服务器（storage server）安装并配置nginx</h2><h3 id="下载并安装fastdfs-nginx-module模块"><a href="#下载并安装fastdfs-nginx-module模块" class="headerlink" title="下载并安装fastdfs-nginx-module模块"></a>下载并安装fastdfs-nginx-module模块</h3><p>注：FastDFS通过Tracker服务器,将文件放在Storage服务器存储，但是同组存储服务器之间需要进入文件复制，有同步延迟的问题。假设Tracker服务器将文件上传到了192.168.1.226，上传成功后文件ID已经返回给客户端。此时FastDFS存储集群机制会将这个文件同步到同组存储192.168.1.227，在文件还没有复制完成的情况下，客户端如果用这个文件ID在192.168.1.227上取文件,就会出现文件无法访问的错误。而fastdfs-nginx-module可以重定向文件连接到源服务器取文件,避免客户端由于复制延迟导致的文件无法访问错误。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# wget http:&#x2F;&#x2F;nchc.dl.sourceforge.net&#x2F;project&#x2F;fastdfs&#x2F;FastDFS%20Nginx%20Module%20Source%20Code&#x2F;fastdfs-nginx-module_v1.16.tar.gz</span><br><span class="line">[root@object1 ~]# tar zxvf fastdfs-nginx-module_v1.16.tar.gz </span><br><span class="line">[root@object1 ~]# cd fastdfs-nginx-module&#x2F;src&#x2F;</span><br><span class="line">[root@object1 src]# vim config </span><br><span class="line">编辑config文件，执行如下命令进行批量替换并保存退出</span><br><span class="line">:%s+&#x2F;usr&#x2F;local&#x2F;+&#x2F;usr&#x2F;+g</span><br><span class="line">[root@object1 src]# cp mod_fastdfs.conf &#x2F;etc&#x2F;fdfs&#x2F;</span><br><span class="line">修改mod_fastdfs.conf</span><br><span class="line">第40行</span><br><span class="line">tracker_server&#x3D;192.168.1.226:22122</span><br></pre></td></tr></table></figure>

<h3 id="安装nginx依赖库"><a href="#安装nginx依赖库" class="headerlink" title="安装nginx依赖库"></a>安装nginx依赖库</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# yum install -y pcre-devel zlib-devel nginx</span><br></pre></td></tr></table></figure>

<h3 id="安装nginx"><a href="#安装nginx" class="headerlink" title="安装nginx"></a>安装nginx</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.13.0.tar.gz</span><br><span class="line">[root@object1 ~]# tar zxvf nginx-1.13.0.tar.gz</span><br><span class="line">[root@object1 ~]# cd nginx-1.13.0</span><br><span class="line">[root@object1 nginx-1.13.0]# .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --add-module&#x3D;&#x2F;root&#x2F;fastdfs-nginx-module&#x2F;src&#x2F;</span><br><span class="line">[root@object1 nginx-1.13.0]# make &amp;&amp; make install</span><br></pre></td></tr></table></figure>

<h3 id="拷贝FastDFS中的部分配置文件到-etc-fdfs目录中"><a href="#拷贝FastDFS中的部分配置文件到-etc-fdfs目录中" class="headerlink" title="拷贝FastDFS中的部分配置文件到/etc/fdfs目录中"></a>拷贝FastDFS中的部分配置文件到/etc/fdfs目录中</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 nginx-1.13.0]# cp &#x2F;root&#x2F;FastDFS&#x2F;conf&#x2F;http.conf &#x2F;etc&#x2F;fdfs&#x2F;</span><br><span class="line">[root@object1 nginx-1.13.0]# cp &#x2F;root&#x2F;FastDFS&#x2F;conf&#x2F;mime.types &#x2F;etc&#x2F;fdfs&#x2F;</span><br></pre></td></tr></table></figure>

<h3 id="配置nginx"><a href="#配置nginx" class="headerlink" title="配置nginx"></a>配置nginx</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# vim &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf</span><br><span class="line">修改1行</span><br><span class="line">user root;  #解决下载操作时报404的问题</span><br><span class="line">修改36行</span><br><span class="line">listen       8888;  #storage.conf配置文件一致</span><br><span class="line">添加</span><br><span class="line">location ~&#x2F;group[0-9]&#x2F; &#123;</span><br><span class="line">                ngx_fastdfs_module;</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>

<h3 id="拷贝nginx服务到-etc-init-d-目录下并启动"><a href="#拷贝nginx服务到-etc-init-d-目录下并启动" class="headerlink" title="拷贝nginx服务到/etc/init.d/目录下并启动"></a>拷贝nginx服务到/etc/init.d/目录下并启动</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# cp &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx &#x2F;etc&#x2F;init.d&#x2F;</span><br><span class="line">[root@object1 ~]# &#x2F;etc&#x2F;init.d&#x2F;nginx</span><br><span class="line">[root@object1 ~]# ss -tunlp | grep 8888</span><br></pre></td></tr></table></figure>

<h3 id="通过浏览器访问之前已经上传的文件"><a href="#通过浏览器访问之前已经上传的文件" class="headerlink" title="通过浏览器访问之前已经上传的文件"></a>通过浏览器访问之前已经上传的文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;192.168.1.226:8888&#x2F;group1&#x2F;M00&#x2F;00&#x2F;00&#x2F;wKgB4lkJlSSADZazAAMuQxWPTP8989.jpg</span><br><span class="line">访问出现400 Bad Request</span><br><span class="line">查看日志</span><br><span class="line">[root@object1 ~]# vim &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;error.log </span><br><span class="line"></span><br><span class="line">报错信息</span><br><span class="line">[2017-05-03 17:00:38] ERROR - file: ..&#x2F;common&#x2F;fdfs_global.c, line: 52, the format of filename &quot;group1&#x2F;M00&#x2F;00&#x2F;00&#x2F;wKgB4lkJlSSADZazAAMuQxWPTP8989.jpg&quot; is invalid</span><br><span class="line"></span><br><span class="line">解决方法：</span><br><span class="line">[root@object1 ~]# vim &#x2F;etc&#x2F;fdfs&#x2F;mod_fastdfs.conf</span><br><span class="line">修改53行</span><br><span class="line">url_have_group_name &#x3D; true</span><br></pre></td></tr></table></figure>


]]></content>
      <tags>
        <tag>FastDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>部署Gitolite、Centos7记录</title>
    <url>/posts/d973517f.html</url>
    <content><![CDATA[<h1 id="Gitolite架构"><a href="#Gitolite架构" class="headerlink" title="Gitolite架构"></a>Gitolite架构</h1><img src="/images/15174104_13760152809DNT.png" width="100%" height="100%">

<a id="more"></a>

<h1 id="安装Gitolie-服务端操作"><a href="#安装Gitolie-服务端操作" class="headerlink" title="安装Gitolie(服务端操作)"></a>安装Gitolie(服务端操作)</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># install from EPEL</span><br><span class="line">[root@server01 ~]# yum --enablerepo&#x3D;epel -y install gitolite3</span><br><span class="line">[root@server01 ~]# su - gitolite3</span><br><span class="line"></span><br><span class="line">-sh-4.2$ ssh-keygen -f ~&#x2F;.ssh&#x2F;gitadmin </span><br><span class="line">-sh-4.2$ gitolite setup -pk ~&#x2F;.ssh&#x2F;gitadmin.pub</span><br><span class="line">-sh-4.2$ vi ~&#x2F;.ssh&#x2F;config</span><br><span class="line"></span><br><span class="line"># create new</span><br><span class="line"># any name you like</span><br><span class="line">host GitServer</span><br><span class="line">    user gitolite3</span><br><span class="line">    # Git server&#39;s hostname or IP address</span><br><span class="line">    hostname 172.16.2.30</span><br><span class="line">    port 22</span><br><span class="line">    # secret key</span><br><span class="line">    identityfile ~&#x2F;.ssh&#x2F;gitadmin</span><br><span class="line"></span><br><span class="line">-sh-4.2$ chmod 600 ~&#x2F;.ssh&#x2F;config </span><br><span class="line">-sh-4.2$ git config --global user.name &quot;gitolite3&quot; </span><br><span class="line">-sh-4.2$ git config --global user.email &quot;ywthings@qq.com&quot; </span><br><span class="line">-sh-4.2$ git config --global push.default simple</span><br><span class="line">-sh-4.2$ git clone ssh:&#x2F;&#x2F;GitServer&#x2F;gitolite-admin</span><br><span class="line"></span><br><span class="line">至此，gitolite已经安装完成</span><br></pre></td></tr></table></figure>

<h1 id="生成SSH-Key（客户端操作，以下linux操作，windows使用git工具）"><a href="#生成SSH-Key（客户端操作，以下linux操作，windows使用git工具）" class="headerlink" title="生成SSH Key（客户端操作，以下linux操作，windows使用git工具）"></a>生成SSH Key（客户端操作，以下linux操作，windows使用git工具）</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@server02 ~]# ssh-keygen</span><br><span class="line"></span><br><span class="line">#一路回车键即可，不要输入密码，如输入密码，git clone时会提示输入密码</span><br></pre></td></tr></table></figure>

<h1 id="将生成的SSH-public-key拷贝到服务器（客户端操作）"><a href="#将生成的SSH-public-key拷贝到服务器（客户端操作）" class="headerlink" title="将生成的SSH public key拷贝到服务器（客户端操作）"></a>将生成的SSH public key拷贝到服务器（客户端操作）</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@server02 ~]# scp &#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub gitolite3@172.16.2.30:&#x2F;var&#x2F;lib&#x2F;gitolite3&#x2F;gitolite-admin&#x2F;keydir</span><br><span class="line"></span><br><span class="line">#可以直接复制密钥内容即可，以上方式注意权限问题</span><br></pre></td></tr></table></figure>

<h1 id="修改gitolite配置文件（服务端操作）"><a href="#修改gitolite配置文件（服务端操作）" class="headerlink" title="修改gitolite配置文件（服务端操作）"></a>修改gitolite配置文件（服务端操作）</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#使用gitolite3用户登录服务器</span><br><span class="line">#客户端的公钥保存在&#x2F;var&#x2F;lib&#x2F;gitolite3&#x2F;gitolite-admin&#x2F;keydir</span><br><span class="line">#修改权限&#x2F;var&#x2F;lib&#x2F;gitolite3&#x2F;gitolite-admin&#x2F;conf&#x2F;gitolite.conf</span><br><span class="line">#上传代码即可</span><br></pre></td></tr></table></figure>

<h1 id="增加仓库"><a href="#增加仓库" class="headerlink" title="增加仓库"></a>增加仓库</h1><p>只需要在/var/lib/gitolite3/gitolite-admin/conf/gitolite.conf文件修改即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">repo    ljs-bis</span><br><span class="line">        RW+     &#x3D;  @developer</span><br></pre></td></tr></table></figure>
<p>然后提交代码，自动创建git仓库</p>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>gitolite</tag>
      </tags>
  </entry>
  <entry>
    <title>云服务器CentOS 7 安装谷歌浏览器</title>
    <url>/posts/fb797d1b.html</url>
    <content><![CDATA[<p>因项目迁移至腾讯云的TSF，项目只能通过对方提供的web页面上传docker容器，本地上传速度又慢，因此通过云服务器上传项目。</p>
<a id="more"></a>
<h1 id="配置yum源"><a href="#配置yum源" class="headerlink" title="配置yum源"></a>配置yum源</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;yum.repos.d&#x2F;google-chrome.repo</span><br><span class="line"></span><br><span class="line">[google-chrome]</span><br><span class="line">name&#x3D;google-chrome</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;dl.google.com&#x2F;linux&#x2F;chrome&#x2F;rpm&#x2F;stable&#x2F;$basearch</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;dl-ssl.google.com&#x2F;linux&#x2F;linux_signing_key.pub</span><br></pre></td></tr></table></figure>

<h1 id="安装google-chrome浏览器"><a href="#安装google-chrome浏览器" class="headerlink" title="安装google chrome浏览器"></a>安装google chrome浏览器</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install google-chrome-stable</span><br></pre></td></tr></table></figure>

<p>Google官方源可能在中国无法使用，导致安装失败或者在国内无法更新，可以添加以下参数来安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install google-chrome-stable --nogpgcheck</span><br></pre></td></tr></table></figure>

<p>或者通过谷歌助手下载linux_signing_key.pub,然后导入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm --import linux_signing_key.pub</span><br></pre></td></tr></table></figure>

<h1 id="Xvfb是一个实现了X11显示服务协议的显示服务器"><a href="#Xvfb是一个实现了X11显示服务协议的显示服务器" class="headerlink" title="Xvfb是一个实现了X11显示服务协议的显示服务器"></a>Xvfb是一个实现了X11显示服务协议的显示服务器</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install Xvfb</span><br></pre></td></tr></table></figure>

<p>xshell 配置，转发 X11 即可, 重新打开终端生效</p>
<h1 id="运行chrome"><a href="#运行chrome" class="headerlink" title="运行chrome"></a>运行chrome</h1><p>找到chrome路径，并做个软连接，方便使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">which google-chrome-stable</span><br><span class="line">ln -s &#x2F;usr&#x2F;bin&#x2F;google-chrome-stable &#x2F;usr&#x2F;bin&#x2F;chrome</span><br></pre></td></tr></table></figure>

<p>使用root用户启动chrome示例时会提示添加参数–no-sandbox,普通用户则不需要</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chrome --no-sandbox</span><br></pre></td></tr></table></figure>








]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>chrome</tag>
      </tags>
  </entry>
  <entry>
    <title>应用系统部署文档模板</title>
    <url>/posts/e0983c90.html</url>
    <content><![CDATA[<p>记录某个项目部署文档。</p>
<p>主要包括Oracle、Redis、Nginx、JDK、Tomcat等软件安装配置，以及应用服务器安装启动的基本操作。</p>
<a id="more"></a>

<p>文档内容请查看部署文档。 <a href="/download/deploy-doc.docx">点击下载</a></p>
]]></content>
      <categories>
        <category>日常部署记录</category>
      </categories>
  </entry>
  <entry>
    <title>搭建 RabbitMQ Server 集群</title>
    <url>/posts/1c93567c.html</url>
    <content><![CDATA[<p>官网：<a href="http://www.rabbitmq.com/download.html">http://www.rabbitmq.com/download.html</a></p>
<a id="more"></a>

<h1 id="前期"><a href="#前期" class="headerlink" title="前期"></a>前期</h1><h2 id="修改hosts"><a href="#修改hosts" class="headerlink" title="修改hosts"></a>修改hosts</h2><p>修改每台服务上的hosts文件(路径：/etc/hosts)，设置成如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">172.16.0.9 node1</span><br><span class="line">172.16.0.8 node2</span><br></pre></td></tr></table></figure>

<h2 id="安装RabbitMQ"><a href="#安装RabbitMQ" class="headerlink" title="安装RabbitMQ"></a>安装RabbitMQ</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># erlang</span><br><span class="line">curl -s https:&#x2F;&#x2F;packagecloud.io&#x2F;install&#x2F;repositories&#x2F;rabbitmq&#x2F;erlang&#x2F;script.rpm.sh | sudo bash</span><br><span class="line"></span><br><span class="line"># 导入2018年12月1日（GMT）开始使用的新PackageCloud密钥</span><br><span class="line">rpm --import https:&#x2F;&#x2F;packagecloud.io&#x2F;rabbitmq&#x2F;rabbitmq-server&#x2F;gpgkey</span><br><span class="line"></span><br><span class="line"># 导入2018年12月1日停止使用的旧PackageCloud密钥（GMT）</span><br><span class="line">rpm --import https:&#x2F;&#x2F;packagecloud.io&#x2F;gpg.key</span><br><span class="line"></span><br><span class="line"># 导入RabbitMQ签名密钥</span><br><span class="line">rpm --import https:&#x2F;&#x2F;github.com&#x2F;rabbitmq&#x2F;signing-keys&#x2F;releases&#x2F;download&#x2F;2.0&#x2F;rabbitmq-release-signing-key.asc</span><br><span class="line"></span><br><span class="line"># 添加Yum存储库</span><br><span class="line">vim &#x2F;etc&#x2F;yum.repos.d&#x2F;rabbitmq.repo</span><br><span class="line">[bintray-rabbitmq-server]</span><br><span class="line">name&#x3D;bintray-rabbitmq-rpm</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;dl.bintray.com&#x2F;rabbitmq&#x2F;rpm&#x2F;rabbitmq-server&#x2F;v3.7.x&#x2F;el&#x2F;7&#x2F;</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">repo_gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line"></span><br><span class="line"># 安装rabbitmq</span><br><span class="line">yum install rabbitmq-server</span><br><span class="line"></span><br><span class="line"># 启动并开启开机启动</span><br><span class="line">systemctl enable rabbitmq-server.service</span><br><span class="line">systemctl start rabbitmq-server.service</span><br><span class="line"></span><br><span class="line"># 查看RabbitMQ中用户命令</span><br><span class="line">rabbitmqctl list_users</span><br><span class="line"></span><br><span class="line"># 创建用户命令,RabbitMQ Server 默认guest用户，只能localhost地址访问</span><br><span class="line">rabbitmqctl add_user test 123456</span><br><span class="line"></span><br><span class="line"># 赋予用户权限命令</span><br><span class="line">rabbitmqctl  set_permissions -p &quot;&#x2F;&quot; test &#39;.*&#39; &#39;.*&#39; &#39;.*&#39;</span><br><span class="line"></span><br><span class="line"># 赋予用户角色命令</span><br><span class="line">rabbitmqctl set_user_tags test administrator</span><br><span class="line"></span><br><span class="line"># 查看插件安装情况</span><br><span class="line">rabbitmqctl list           </span><br><span class="line"></span><br><span class="line"># 开启rabbitmq管理控制台命令</span><br><span class="line">rabbitmq-plugins enable rabbitmq_management</span><br></pre></td></tr></table></figure>

<h1 id="RabbitMQ-Server-高可用集群相关概念"><a href="#RabbitMQ-Server-高可用集群相关概念" class="headerlink" title="RabbitMQ Server 高可用集群相关概念"></a>RabbitMQ Server 高可用集群相关概念</h1><h2 id="设计集群的目的"><a href="#设计集群的目的" class="headerlink" title="设计集群的目的"></a>设计集群的目的</h2><p>允许消费者和生产者在 RabbitMQ 节点崩溃的情况下继续运行。<br>通过增加更多的节点来扩展消息通信的吞吐量。</p>
<h2 id="集群配置方式"><a href="#集群配置方式" class="headerlink" title="集群配置方式"></a>集群配置方式</h2><p>cluster：不支持跨网段，用于同一个网段内的局域网；可以随意的动态增加或者减少；节点之间需要运行相同版本的 RabbitMQ 和 Erlang。<br>federation：应用于广域网，允许单台服务器上的交换机或队列接收发布到另一台服务器上交换机或队列的消息，可以是单独机器或集群。federation 队列类似于单向点对点连接，消息会在联盟队列之间转发任意次，直到被消费者接受。通常使用 federation 来连接 internet 上的中间服务器，用作订阅分发消息或工作队列。<br>shovel：连接方式与 federation 的连接方式类似，但它工作在更低层次。可以应用于广域网。<br>节点类型<br>RAM node：内存节点将所有的队列、交换机、绑定、用户、权限和 vhost 的元数据定义存储在内存中，好处是可以使得像交换机和队列声明等操作更加的快速。<br>Disk node：将元数据存储在磁盘中，单节点系统只允许磁盘类型的节点，防止重启 RabbitMQ 的时候，丢失系统的配置信息。<br>问题说明：RabbitMQ 要求在集群中至少有一个磁盘节点，所有其他节点可以是内存节点，当节点加入或者离开集群时，必须要将该变更通知到至少一个磁盘节点。如果集群中唯一的一个磁盘节点崩溃的话，集群仍然可以保持运行，但是无法进行其他操作（增删改查），直到节点恢复。<br>解决方案：设置两个磁盘节点，至少有一个是可用的，可以保存元数据的更改。</p>
<h2 id="Erlang-Cookie"><a href="#Erlang-Cookie" class="headerlink" title="Erlang Cookie"></a>Erlang Cookie</h2><p>Erlang Cookie 是保证不同节点可以相互通信的密钥，要保证集群中的不同节点相互通信必须共享相同的 Erlang Cookie。具体的目录存放在/var/lib/rabbitmq/.erlang.cookie。</p>
<p>说明：这就要从 rabbitmqctl 命令的工作原理说起，RabbitMQ 底层是通过 Erlang 架构来实现的，所以 rabbitmqctl 会启动 Erlang 节点，并基于 Erlang 节点来使用 Erlang 系统连接 RabbitMQ 节点，在连接过程中需要正确的 Erlang Cookie 和节点名称，Erlang 节点通过交换 Erlang Cookie 以获得认证。</p>
<h2 id="镜像队列"><a href="#镜像队列" class="headerlink" title="镜像队列"></a>镜像队列</h2><p>RabbitMQ 的 Cluster 集群模式一般分为两种，普通模式和镜像模式。</p>
<p>普通模式：默认的集群模式，以两个节点（rabbit01、rabbit02）为例来进行说明。对于 Queue 来说，消息实体只存在于其中一个节点 rabbit01（或者 rabbit02），rabbit01 和 rabbit02 两个节点仅有相同的元数据，即队列的结构。当消息进入 rabbit01 节点的 Queue 后，consumer 从 rabbit02 节点消费时，RabbitMQ 会临时在 rabbit01、rabbit02 间进行消息传输，把 A 中的消息实体取出并经过 B 发送给 consumer。所以 consumer 应尽量连接每一个节点，从中取消息。即对于同一个逻辑队列，要在多个节点建立物理 Queue。否则无论 consumer 连 rabbit01 或 rabbit02，出口总在 rabbit01，会产生瓶颈。当 rabbit01 节点故障后，rabbit02 节点无法取到 rabbit01 节点中还未消费的消息实体。如果做了消息持久化，那么得等 rabbit01 节点恢复，然后才可被消费；如果没有持久化的话，就会产生消息丢失的现象。<br>镜像模式：将需要消费的队列变为镜像队列，存在于多个节点，这样就可以实现 RabbitMQ 的 HA 高可用性。作用就是消息实体会主动在镜像节点之间实现同步，而不是像普通模式那样，在 consumer 消费数据时临时读取。缺点就是，集群内部的同步通讯会占用大量的网络带宽。<br>镜像队列实现了 RabbitMQ 的高可用性（HA），具体的实现策略如下所示：</p>
<table>
<thead>
<tr>
<th>ha-mode</th>
<th align="right">ha-params</th>
<th align="center">功能</th>
</tr>
</thead>
<tbody><tr>
<td>all</td>
<td align="right">空</td>
<td align="center">镜像队列将会在整个集群中复制。当一个新的节点加入后，也会在这 个节点上复制一份。</td>
</tr>
<tr>
<td>exactly</td>
<td align="right">count</td>
<td align="center">镜像队列将会在集群上复制 count 份。如果集群数量少于 count 时候，队列会复制到所有节点上。如果大于 Count 集群，有一个节点 crash 后，新进入节点也不会做新的镜像。</td>
</tr>
<tr>
<td>nodes</td>
<td align="right">node name</td>
<td align="center">镜像队列会在 node name 中复制。如果这个名称不是集群中的一个，这不会触发错误。如果在这个 node list 中没有一个节点在线，那么这个 queue 会被声明在 client 连接的节点。</td>
</tr>
</tbody></table>
<p>实例列举：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">queue_args(&quot;x-ha-policy&quot;:&quot;all&quot;) &#x2F;&#x2F;定义字典来设置额外的队列声明参数</span><br><span class="line">channel.queue_declare(queue&#x3D;&quot;hello-queue&quot;,argument&#x3D;queue_args)</span><br></pre></td></tr></table></figure>

<p>如果需要设定特定的节点（以rabbit@localhost为例），再添加一个参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">queue_args(&quot;x-ha-policy&quot;:&quot;nodes&quot;,</span><br><span class="line">           &quot;x-ha-policy-params&quot;:[&quot;rabbit@localhost&quot;])</span><br><span class="line">channel.queue_declare(queue&#x3D;&quot;hello-queue&quot;,argument&#x3D;queue_args)</span><br></pre></td></tr></table></figure>
<p>可以通过命令行查看那个主节点进行了同步：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ rabbitmqctl list_queue name slave_pids synchronised_slave_pids</span><br></pre></td></tr></table></figure>

<h1 id="RabbitMQ-Cluster-配置"><a href="#RabbitMQ-Cluster-配置" class="headerlink" title="RabbitMQ Cluster 配置"></a>RabbitMQ Cluster 配置</h1><h2 id="修改Erlang-cookie"><a href="#修改Erlang-cookie" class="headerlink" title="修改Erlang cookie"></a>修改Erlang cookie</h2><p>要搭建一个集群，必须修改每个集群节点的Erlang cookie为相同的值。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp &#x2F;var&#x2F;lib&#x2F;rabbitmq&#x2F;.erlang.cookie root@node2:&#x2F;var&#x2F;lib&#x2F;rabbitmq</span><br></pre></td></tr></table></figure>

<p>需改后重启RabbitMQ</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl restart rabbitmq-server.service</span><br></pre></td></tr></table></figure>

<h2 id="加入集群"><a href="#加入集群" class="headerlink" title="加入集群"></a>加入集群</h2><p>以node1作为集群中心，在node2上执行加入集群中心命令（节点类型为磁盘节点）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# rabbitmqctl stop_app</span><br><span class="line">[root@node1 ~]# rabbitmqctl reset </span><br><span class="line">[root@node1 ~]# rabbitmqctl join_cluster rabbit@node1</span><br><span class="line">&#x2F;&#x2F;默认是磁盘节点，如果是内存节点的话，需要加--ram参数</span><br><span class="line">[root@node1 ~]# rabbitmqctl start_app</span><br></pre></td></tr></table></figure>

<h2 id="查看集群的状态"><a href="#查看集群的状态" class="headerlink" title="查看集群的状态"></a>查看集群的状态</h2><p>查看集群的状态（包含node1和node2节点）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# rabbitmqctl cluster_status</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title>使用pure-ftpd部署ftp服务</title>
    <url>/posts/50f11f24.html</url>
    <content><![CDATA[<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="yum"><a href="#yum" class="headerlink" title="yum"></a>yum</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# yum install -y epel-release</span><br><span class="line">[root@localhost ~]# yum install -y pure-ftpd lftp</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# yum install -y openssl openssl-devel gcc gcc-c++</span><br><span class="line">[root@localhost ~]# wget https:&#x2F;&#x2F;download.pureftpd.org&#x2F;pub&#x2F;pure-ftpd&#x2F;releases&#x2F;pure-ftpd-1.0.47.tar.gz</span><br><span class="line">[root@localhost ~]# tar zxvf pure-ftpd-1.0.47.tar.gz</span><br><span class="line">[root@localhost ~]# cd pure-ftpd-1.0.47</span><br><span class="line">[root@localhost ~]# .&#x2F;configure \</span><br><span class="line">--prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;pureftpd \</span><br><span class="line">--without-inetd \</span><br><span class="line">--with-altlog \</span><br><span class="line">--with-puredb \</span><br><span class="line">--with-throttling \</span><br><span class="line">--with-peruserlimits \</span><br><span class="line">--with-tls</span><br><span class="line">[root@localhost ~]# make &amp;&amp; make install</span><br></pre></td></tr></table></figure>

<h1 id="配置启动"><a href="#配置启动" class="headerlink" title="配置启动"></a>配置启动</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# vim &#x2F;etc&#x2F;pure-ftpd&#x2F;pure-ftpd.conf</span><br><span class="line">搜索&#x2F;pureftpd.pdb行首的 #号  删除</span><br><span class="line">PureDB                        &#x2F;etc&#x2F;pure-ftpd&#x2F;pureftpd.pdb</span><br></pre></td></tr></table></figure>

<p>启动服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# systemctl start pure-ftpd</span><br></pre></td></tr></table></figure>

<h1 id="配置权限"><a href="#配置权限" class="headerlink" title="配置权限"></a>配置权限</h1><p>创建测试目录，为了给 pure-ftpd 用户使用，再创建用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# mkdir &#x2F;data&#x2F;ftp</span><br><span class="line">[root@localhost ~]# useradd pure-ftp</span><br></pre></td></tr></table></figure>
<p>把 /data/ftp 的属主和属组改成 pure-ftp</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# chown -R pure-ftp:pure-ftp &#x2F;data&#x2F;ftp</span><br></pre></td></tr></table></figure>

<p>用 pure-pw useradd 命令创建一个用户<br>pure-pw useradd [指定用户] -u [指定系统用户] -d [指定虚拟用户的家目录]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# pure-pw useradd ftp_usera -u pure-ftp  -d &#x2F;data&#x2F;ftp</span><br></pre></td></tr></table></figure>

<p>可以使用 pure-pw –help 查看该命令支持那些用法<br>命令 pure-pw mkdb，是用来把密码生成系统，也就是 pure-ftpd 服务所识别的一种文件，不执行这一步是无法登录的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# pure-pw mkdb</span><br></pre></td></tr></table></figure>

<h1 id="测试-pure-ftpd"><a href="#测试-pure-ftpd" class="headerlink" title="测试 pure-ftpd"></a>测试 pure-ftpd</h1><p>先在 /data/ftp/目录下创建一个文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# touch &#x2F;data&#x2F;ftp&#x2F;123.txt</span><br></pre></td></tr></table></figure>

<p>使用 pure-ftpd</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# lftp ftp_usera@127.0.0.1</span><br><span class="line">口令: </span><br><span class="line">lftp ftp_usera@127.0.0.1:~&gt;</span><br></pre></td></tr></table></figure>

<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><p>pure-pw命令的其他参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-t 下载带宽限制</span><br><span class="line">-T 上传带宽限制</span><br><span class="line">-n 最大文件数目</span><br><span class="line">-N 磁盘配额(单位M)</span><br><span class="line">-q 上传速度限制 </span><br><span class="line">-Q 下载速度限制</span><br><span class="line">-r 允许某些ip&#x2F;网段的客户端访问</span><br><span class="line">-R 拒绝某些ip&#x2F;网段的客户端访问</span><br><span class="line">-i 允许本地某些ip&#x2F;网段访问(allow local host)</span><br><span class="line">-I 拒绝本地某些ip&#x2F;网段访问(deny local host)</span><br><span class="line">-y 同时最大连接数目 </span><br><span class="line">-z 允许连接服务器的时间段，格式hhmm-hhmm，如 -z 0412-1618代表用户只能在凌 晨4点12分至下午4点18分连接服务器 </span><br><span class="line">-f passwd_file</span><br><span class="line">-F puredb_file</span><br><span class="line">-m 不必重启Pure-FTPd以及重新生成puredb_file文件</span><br><span class="line">如果进行帐户操作时，没有带-m 参数，那就应该手动更新一下pdb数据：</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;stow&#x2F;pure-ftpd-1.0.21&#x2F;bin&#x2F;pure-pw mkdb pureftpd.pdb </span><br><span class="line">pure-pw useradd 添加用户</span><br><span class="line">pure-pw userdel 删除用户</span><br><span class="line">pure-pw usermod 修改用户</span><br><span class="line">pure-pw show 查看用户详细信息</span><br><span class="line">pure-pw list 查看所有用户设置</span><br><span class="line">pure-pw mkdb 生成数据文件</span><br></pre></td></tr></table></figure>

<h1 id="配置文件详解"><a href="#配置文件详解" class="headerlink" title="配置文件详解"></a>配置文件详解</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># &#x2F;usr&#x2F;local&#x2F;pureftpd&#x2F;sbin&#x2F;pure-config.pl &#x2F;usr&#x2F;local&#x2F;pureftpd&#x2F;etc&#x2F;pure-ftpd.conf </span><br><span class="line"># </span><br><span class="line"># RPM 缺省使用另外一个配置文件： </span><br><span class="line"># &#x2F;etc&#x2F;sysconfig&#x2F;pure-ftpd </span><br><span class="line"># </span><br><span class="line"># 请不要忘了浏览一下 [url]http:&#x2F;&#x2F;www.pureftpd.org&#x2F;documentation.html[&#x2F;url] 的 </span><br><span class="line"># 文档，查看全部的选项列表。 </span><br><span class="line"># 限制所有用户在其主目录中 </span><br><span class="line">   ChrootEveryone              yes </span><br><span class="line"># 如果前一个指令被设置为了 &quot;no&quot;，下面组的成员(GID)就不受主目录的限制了。而其他的用户还是 </span><br><span class="line"># 会被限制在自己的主目录里。如果你不想把任何用户限制在自己的主目录里，只要注释掉 ChrootEveryone </span><br><span class="line"># 和 TrustedGID 就可以了。 </span><br><span class="line"># TrustedGID                    100 </span><br><span class="line"># 兼容ie等比较非正规化的ftp客户端 </span><br><span class="line">   BrokenClientsCompatibility   no </span><br><span class="line"># 服务器总共允许同时连接的最大用户数 </span><br><span class="line">   MaxClientsNumber          50 </span><br><span class="line"># 做为守护(doemon)进程运行(Fork in background) </span><br><span class="line">   Daemonize                yes </span><br><span class="line"># 同一IP允许同时连接的用户数（Maximum number of sim clients with the same IP address） </span><br><span class="line">   MaxClientsPerIP          8 </span><br><span class="line"># 如果你要记录所有的客户命令，设置这个指令为 &quot;yes&quot;。 </span><br><span class="line"># This directive can be duplicated to also log server responses. </span><br><span class="line">   VerboseLog                no </span><br><span class="line"># 即使客户端没有发送 &#39;-a&#39; 选项也列出隐藏文件( dot-files 。 </span><br><span class="line">   DisplayDotFiles          yes </span><br><span class="line"># 不允许认证用户 - 仅作为一个公共的匿名FTP。 </span><br><span class="line">   AnonymousOnly             no </span><br><span class="line"># 不允许匿名连接，仅允许认证用户使用。 </span><br><span class="line">   NoAnonymous                 no </span><br><span class="line"># Syslog facility (auth, authpriv, daemon, ftp, security, user, local*) </span><br><span class="line"># 缺省的功能( facility 是 &quot;ftp&quot;。 &quot;none&quot; 将禁止日志。 </span><br><span class="line">   SyslogFacility              ftp </span><br><span class="line"># 定制用户登陆后的显示信息（Display fortune cookies） </span><br><span class="line"># FortunesFile              &#x2F;usr&#x2F;share&#x2F;fortune&#x2F;zippy </span><br><span class="line"># 在日志文件中不解析主机名。日志没那么详细的话，就使用更少的带宽。在一个访问量很大  </span><br><span class="line"># 的站点中，设置这个指令为 &quot;yes&quot; ，如果你没有一个能工作的DNS的话。 </span><br><span class="line">   DontResolve                 yes </span><br><span class="line"># 客户端允许的最大的空闲时间（分钟，缺省15分钟） </span><br><span class="line">   MaxIdleTime                 15 </span><br><span class="line"># LDAP 配置文件 (参考 README.LDAP) </span><br><span class="line"># LDAPConfigFile             &#x2F;etc&#x2F;pureftpd-ldap.conf </span><br><span class="line"># MySQL 配置文件 (参考 README.MySQL) </span><br><span class="line"># MySQLConfigFile             &#x2F;etc&#x2F;pureftpd-mysql.conf </span><br><span class="line"># Postgres 配置文件 (参考 README.PGSQL) </span><br><span class="line"># PGSQLConfigFile             &#x2F;etc&#x2F;pureftpd-pgsql.conf </span><br><span class="line"># PureDB 用户数据库 (参考 README.Virtual-Users) </span><br><span class="line"># PureDB                      &#x2F;etc&#x2F;pureftpd.pdb </span><br><span class="line"># pure-authd 的socket 路径(参考 README.Authentication-Modules) </span><br><span class="line"># ExtAuth                       &#x2F;var&#x2F;run&#x2F;ftpd.sock </span><br><span class="line"># 如果你要启用 PAM 认证方式, 去掉下面行的注释。 </span><br><span class="line"># PAMAuthentication          yes </span><br><span class="line"># 如果你要启用 简单的 Unix系统 认证方式(&#x2F;etc&#x2F;passwd), 去掉下面行的注释。 </span><br><span class="line"># UnixAuthentication          yes </span><br><span class="line"># 请注意，LDAPConfigFile, MySQLConfigFile, PAMAuthentication 和 </span><br><span class="line"># UnixAuthentication 这些指令只能被使用一次，不过，他们能被混合在一起用。例如：如果你使用了 </span><br><span class="line"># MySQLConfigFile 和 UnixAuthentication，那么 SQL 服务器将被访问。如果因为用户名未找 </span><br><span class="line"># 到而使 SQL 认证失败的话，就会在&#x2F;etc&#x2F;passwd 和 &#x2F;etc&#x2F;shadow 中尝试另外一种认证，如果因 </span><br><span class="line"># 为密码错误而使 SQL 认证失败的话，认证就会在此结束了。认证方式由它们被给出来的顺序而被链 </span><br><span class="line"># 接了起来。 </span><br><span class="line"># &#39;ls&#39; 命令的递归限制。第一个参数给出文件显示的最大数目。第二个参数给出最大的子目录深度。 </span><br><span class="line">   LimitRecursion              2000 8 </span><br><span class="line"># 允许匿名用户创建新目录？ </span><br><span class="line">   AnonymousCanCreateDirs    no </span><br><span class="line"># 如果系统被 loaded 超过下面的值，匿名用户会被禁止下载。 </span><br><span class="line">   MaxLoad                   4 </span><br><span class="line"># 被动连接响应的端口范围。- for firewalling. </span><br><span class="line"># PassivePortRange       30000 50000 </span><br><span class="line"># 强制一个IP地址使用被动响应（ PASV&#x2F;EPSV&#x2F;SPSV replies）。 - for NAT. </span><br><span class="line"># Symbolic host names are also accepted for gateways with dynamic IP </span><br><span class="line"># addresses. </span><br><span class="line"># ForcePassiveIP             192.168.0.1 </span><br><span class="line"># 匿名用户的上传&#x2F;下载的比率。 </span><br><span class="line"># AnonymousRatio             1 10 </span><br><span class="line"># 所有用户的上传&#x2F;下载的比率。 </span><br><span class="line"># This directive superscedes the previous one. </span><br><span class="line"># UserRatio                 1 10 </span><br><span class="line"># 不接受所有者为 &quot;ftp&quot; 的文件的下载。例如：那些匿名用户上传后未被本地管理员验证的文件。 </span><br><span class="line">   AntiWarez                yes </span><br><span class="line"># 服务监听的IP 地址和端口。(缺省是所有IP地址和21端口) </span><br><span class="line"># Bind                   127.0.0.1,21 </span><br><span class="line"># 匿名用户的最大带宽（KB&#x2F;s）。 </span><br><span class="line"># AnonymousBandwidth          8 </span><br><span class="line"># 所有用户的最大带宽（KB&#x2F;s），包括匿名用户。 </span><br><span class="line"># Use AnonymousBandwidth *or* UserBandwidth, both makes no sense. </span><br><span class="line"># UserBandwidth          8 </span><br><span class="line"># 新建目录及文件的属性掩码值。&lt;文件掩码&gt;:&lt;目录掩码&gt; . </span><br><span class="line"># 177:077 if you feel paranoid. </span><br><span class="line">   Umask                       133:022 </span><br><span class="line"># 认证用户允许登陆的最小组ID（UID） 。 </span><br><span class="line">   MinUID                   100 </span><br><span class="line"># 仅允许认证用户进行 FXP 传输。 </span><br><span class="line">   AllowUserFXP             yes </span><br><span class="line"># 对匿名用户和非匿名用户允许进行匿名 FXP 传输。 </span><br><span class="line">   AllowAnonymousFXP           no </span><br><span class="line"># 用户不能删除和写点文件（文件名以 &#39;.&#39; 开头的文件），即使用户是文件的所有者也不行。 </span><br><span class="line"># 如果 TrustedGID 指令是 enabled ，文件所属组用户能够访问点文件(dot-files)。 </span><br><span class="line">   ProhibitDotFilesWrite    no </span><br><span class="line"># 禁止读点文件（文件名以 &#39;.&#39; 开头的文件） (.history, .ssh...) </span><br><span class="line">   ProhibitDotFilesRead        no </span><br><span class="line"># 永不覆盖文件。当上传的文件，其文件名已经存在时，自动重命名，如： file.1, file.2, file.3, ... </span><br><span class="line">   AutoRename                no </span><br><span class="line"># 不接受匿名用户上传新文件( no &#x3D; 允许上传) </span><br><span class="line">   AnonymousCantUpload       no </span><br><span class="line"># 仅允许来自以下IP地址的非匿名用户连接。你可以使用这个指令来打开几个公网IP来提供匿名FTP， </span><br><span class="line"># 而保留一个私有的防火墙保护的IP来进行远程管理。你还可以只允许一内网地址进行认证，而在另外 </span><br><span class="line"># 一个IP上提供纯匿名的FTP服务。 </span><br><span class="line">#TrustedIP                10.1.1.1 </span><br><span class="line"># 如果你要为日志每一行添加 PID   去掉下面行的注释。 </span><br><span class="line"># LogPID                   yes </span><br><span class="line"># 使用类似于Apache的格式创建一个额外的日志文件，如： </span><br><span class="line"># fw.c9x.org - jedi [13&#x2F;Dec&#x2F;1975:19:36:39] &quot;GET &#x2F;ftp&#x2F;linux.tar.bz2&quot; 200 21809338 </span><br><span class="line"># 这个日志文件能被 www 流量分析器处理。 </span><br><span class="line"># AltLog                   clf:&#x2F;var&#x2F;log&#x2F;pureftpd.log </span><br><span class="line"># 使用优化过的格式为统计报告创建一个额外的日志文件。 </span><br><span class="line"># AltLog                   stats:&#x2F;var&#x2F;log&#x2F;pureftpd.log </span><br><span class="line"># 使用标准的W3C格式创建一个额外的日志文件。（与大部分的商业日志分析器兼容） </span><br><span class="line"># AltLog                   w3c:&#x2F;var&#x2F;log&#x2F;pureftpd.log </span><br><span class="line"># 不接受 CHMOD 命令。用户不能更改他们文件的属性。 </span><br><span class="line"># NoChmod                   yes </span><br><span class="line"># 允许用户恢复和上传文件，却不允许删除他们。 </span><br><span class="line"># KeepAllFiles             yes </span><br><span class="line"># 用户主目录不存在的话，自动创建。 </span><br><span class="line"># CreateHomeDir             yes </span><br><span class="line"># 启用虚拟的磁盘限额。第一个数字是最大的文件数。 </span><br><span class="line"># 第二个数字是最大的总的文件大小(单位：Mb)。 </span><br><span class="line"># 所以，1000:10 就限制每一个用户只能使用 1000 个文件，共10Mb。 </span><br><span class="line"># Quota                       1000:10 </span><br><span class="line"># 如果你的 pure-ftpd 编译时加入了独立服务器( standalone 支持，你能够改变 pid 文件 </span><br><span class="line"># 的位置。缺省位置是 &#x2F;var&#x2F;run&#x2F;pure-ftpd.pid 。 </span><br><span class="line"># PIDFile                   &#x2F;var&#x2F;run&#x2F;pure-ftpd.pid </span><br><span class="line"># 如果你的 pure-ftpd 编译时加入了 pure-uploadscrīpt 支持，这个指令将会使 pure-ftpd </span><br><span class="line"># 发送关于新上传的情况信息到 &#x2F;var&#x2F;run&#x2F;pure-ftpd.upload.pipe，这样 pure-uploadscrīpt </span><br><span class="line"># 就能读然后调用一个脚本去处理新的上传。 </span><br><span class="line"># CallUploadscrīpt yes </span><br><span class="line"># 这个选项对允许匿名上传的服务器是有用的。当 &#x2F;var&#x2F;ftp 在 &#x2F;var 里时，需要保留一定磁盘空间 </span><br><span class="line"># 来保护日志文件。当所在磁盘分区使用超过百分之 X 时，将不在接受新的上传。 </span><br><span class="line">   MaxDiskUsage             99 </span><br><span class="line"># 如果你不想要你的用户重命名文件的话，就设置为 &#39;yes&#39; 。 </span><br><span class="line"># NoRename yes </span><br><span class="line"># 是 &#39;customer proof&#39; : 工作区(workaround)反对普通的客户错误，类似于：&#39;chmod 0 public_html&#39; 的错误。 </span><br><span class="line"># 那是一个有效的命令，不过，将导致无知的客户所定他们自己的文件，将使你的技术支持忙于愚蠢的的问题中。 </span><br><span class="line"># 如果你确信你所有的用户都有基本的Unix知识的话，这个特性将没什么用了。不过，如果你是一个主机提供商 </span><br><span class="line"># 的话，启用它。 </span><br><span class="line">CustomerProof yes </span><br><span class="line"># 每一个用户的并发限制。只有在添加了 --with-peruserlimits 编译选项进行编译后，这个指令才起 </span><br><span class="line"># 作用。(大部分的二进制的发布版本就是例子) </span><br><span class="line"># 格式是 : &lt;每一个用户最大允许的进程&gt;:&lt;最大的匿名用户进程&gt; </span><br><span class="line"># 例如： 3:20 意思是同一个认证用户最大可以有3个同时活动的进程。而且同时最多只能有20个匿名用户进程。 </span><br><span class="line"># PerUserLimits 3:20</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title>清理Docker仓库Registry</title>
    <url>/posts/48c1bd74.html</url>
    <content><![CDATA[<h1 id="下载清理工具"><a href="#下载清理工具" class="headerlink" title="下载清理工具"></a>下载清理工具</h1><p>github 官网：<a href="https://github.com/burnettk/delete-docker-registry-image">https://github.com/burnettk/delete-docker-registry-image</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;burnettk&#x2F;delete-docker-registry-image&#x2F;master&#x2F;delete_docker_registry_image.py | sudo tee &#x2F;usr&#x2F;local&#x2F;bin&#x2F;delete_docker_registry_image &gt;&#x2F;dev&#x2F;null</span><br><span class="line">sudo chmod a+x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;delete_docker_registry_image</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>安装python依赖</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果是python2.x，直接在命令提示符中用pip install futures语句安装。</span><br><span class="line">如果是python3，不用安装也可以用，自带了</span><br></pre></td></tr></table></figure>
<h1 id="添加Registry环境"><a href="#添加Registry环境" class="headerlink" title="添加Registry环境"></a>添加Registry环境</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export REGISTRY_DATA_DIR&#x3D;&#x2F;data&#x2F;docker_registry&#x2F;docker&#x2F;registry&#x2F;v2</span><br></pre></td></tr></table></figure>

<h1 id="查看repositories"><a href="#查看repositories" class="headerlink" title="查看repositories"></a>查看repositories</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;192.168.100.2:30100&#x2F;v2&#x2F;_catalog</span><br></pre></td></tr></table></figure>

<h1 id="根据repositories进行清理"><a href="#根据repositories进行清理" class="headerlink" title="根据repositories进行清理"></a>根据repositories进行清理</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">delete_docker_registry_image --image tbl</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>registry</tag>
      </tags>
  </entry>
  <entry>
    <title>docker通过代理进行构建dockerfile</title>
    <url>/posts/7cc4a6d7.html</url>
    <content><![CDATA[<p>进行Dockerfile进行构建时候，由于无法进行直连外网连接，只能通过代理进行联网构建。相关配置如下以及遇到的几个问题。</p>
<a id="more"></a>

<h3 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ENV http_proxy http:&#x2F;&#x2F;x.x.x.x:3128</span><br><span class="line">ENV https_proxy https:&#x2F;&#x2F;x.x.x.x:3128</span><br><span class="line">ENV no_proxy x.x.x.x</span><br></pre></td></tr></table></figure>

<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p><strong>问题1：</strong>curl: (35) error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">将Dockerfil里面的curl请求更改为wget</span><br><span class="line">原：</span><br><span class="line">RUN curl https:&#x2F;&#x2F;packages.microsoft.com&#x2F;keys&#x2F;microsoft.asc | apt-key add -</span><br><span class="line">新：</span><br><span class="line">RUN wget -qO - https:&#x2F;&#x2F;packages.microsoft.com&#x2F;keys&#x2F;microsoft.asc | apt-key add</span><br></pre></td></tr></table></figure>

<p><strong>问题2：</strong>  Could not handshake: An unexpected TLS packet was received. [IP: x.x.x.x 3218]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">修改代理配置</span><br><span class="line">cat &#x2F;etc&#x2F;apt&#x2F;apt.conf</span><br><span class="line">Acquire::http::proxy &quot;http:&#x2F;&#x2F;x.x.x.x:3128&quot;;</span><br><span class="line">Acquire::https::proxy &quot;https:&#x2F;&#x2F;x.x.x.x:3128&quot;;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>部署Elasticsearch集群</title>
    <url>/posts/148d0252.html</url>
    <content><![CDATA[<h1 id="什么是Elasticsearch集群？"><a href="#什么是Elasticsearch集群？" class="headerlink" title="什么是Elasticsearch集群？"></a>什么是Elasticsearch集群？</h1><p>顾名思义，Elasticsearch集群是一组连接在一起的一个或多个Elasticsearch节点实例。Elasticsearch集群的强大之处在于跨集群中所有节点的任务分配，搜索和索引。</p>
<p>可以为Elasticsearch集群中的节点分配不同的作业或职责：</p>
<ul>
<li><p>Data nodes 存储数据并执行与搜索和聚合等数据相关的操作</p>
</li>
<li><p>Master nodes 负责集群范围的管理和配置操作，例如添加和删除节点</p>
</li>
<li><p>Client nodes 将集群请求转发到主节点，并将与数据相关的请求转发给数据节点</p>
</li>
<li><p>Ingest nodes  用于在编制索引之前预处理文档</p>
</li>
</ul>
<p>默认情况下，会为每个节点自动分配一个唯一标识符或名称，用于管理目的，在多节点或群集环境中变得更加重要。</p>
<h1 id="安装Elasticsearch"><a href="#安装Elasticsearch" class="headerlink" title="安装Elasticsearch"></a>安装Elasticsearch</h1><h2 id="安装Java"><a href="#安装Java" class="headerlink" title="安装Java"></a>安装Java</h2><p>Elasticsearch构建于Java之上，至少需要运行Java 8（1.8.0_131或更高版本）。因此，我们的第一步是在集群中的所有节点上安装Java 8。请注意，应在群集中的所有Elasticsearch节点上安装相同的版本。</p>
<h2 id="安装Elasticsearch节点"><a href="#安装Elasticsearch节点" class="headerlink" title="安装Elasticsearch节点"></a>安装Elasticsearch节点</h2><p>添加Elastic的签名密钥</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm --import https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;GPG-KEY-elasticsearch</span><br></pre></td></tr></table></figure>

<p>添加仓库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vi &#x2F;etc&#x2F;yum.repos.d&#x2F;elasticsearch.repo</span><br><span class="line"></span><br><span class="line">[elasticsearch-6.x]</span><br><span class="line">name&#x3D;Elasticsearch repository for 6.x packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;packages&#x2F;6.x&#x2F;yum</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;GPG-KEY-elasticsearch</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">autorefresh&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br></pre></td></tr></table></figure>

<p>安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install elasticsearch</span><br><span class="line">sudo &#x2F;bin&#x2F;systemctl daemon-reload</span><br><span class="line">sudo &#x2F;bin&#x2F;systemctl enable elasticsearch.service</span><br><span class="line">#sudo systemctl start elasticsearch.service</span><br><span class="line">#sudo systemctl stop elasticsearch.service</span><br></pre></td></tr></table></figure>

<h1 id="配置Elasticsearch集群"><a href="#配置Elasticsearch集群" class="headerlink" title="配置Elasticsearch集群"></a>配置Elasticsearch集群</h1><p>对于每个节点，请打开Elasticsearch配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;elasticsearch&#x2F;elasticsearch.yml</span><br></pre></td></tr></table></figure>

<p>然后输入以下配置（用您的节点IP替换IP）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#give your cluster a name.</span><br><span class="line">cluster.name: my-cluster</span><br><span class="line"></span><br><span class="line">#give your nodes a name (change node number from node to node).</span><br><span class="line">node.name: &quot;es-node-1&quot;</span><br><span class="line"></span><br><span class="line">#define node 1 as master-eligible:</span><br><span class="line">node.master: true</span><br><span class="line"></span><br><span class="line">#define nodes 2 and 3 as data nodes:</span><br><span class="line">node.data: true</span><br><span class="line"></span><br><span class="line">#enter the private IP and port of your node:</span><br><span class="line">network.host: 172.11.61.27</span><br><span class="line">http.port: 9200</span><br><span class="line"></span><br><span class="line">#detail the private IPs of your nodes:</span><br><span class="line">discovery.zen.ping.unicast.hosts: [&quot;172.11.61.27&quot;, &quot;172.31.22.131&quot;,&quot;172.31.32.221&quot;]</span><br></pre></td></tr></table></figure>

<h1 id="运行Elasticsearch集群"><a href="#运行Elasticsearch集群" class="headerlink" title="运行Elasticsearch集群"></a>运行Elasticsearch集群</h1><p>对于每个实例，运行以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo service elasticsearch start</span><br></pre></td></tr></table></figure>

<h1 id="用于生产的Elasticsearch集群配置"><a href="#用于生产的Elasticsearch集群配置" class="headerlink" title="用于生产的Elasticsearch集群配置"></a>用于生产的Elasticsearch集群配置</h1><h2 id="避免脑裂"><a href="#避免脑裂" class="headerlink" title="避免脑裂"></a>避免脑裂</h2><p>脑裂情况是当集群中的节点之间的通信由于网络故障或其中一个节点的内部故障而失败时。在这种情况下，多个节点可能认为它是主节点，导致数据不一致状态。</p>
<p>为了避免这种情况，我们可以更改Elasticsearch配置文件中的discovery.zen.minimum_master_nodes指令，该指令确定需要通信的多少个节点（仲裁）来选择主节点。</p>
<p>确定此数字的最佳做法是使用以下公式来确定此数字：N/2+1,N是群集中符合条件的主节点数。然后，将结果向下舍入到最接近的整数。<br>对于具有三个节点的集群，则</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">discovery.zen.minimum_master_nodes: 2</span><br></pre></td></tr></table></figure>

<h2 id="调整JVM堆大小"><a href="#调整JVM堆大小" class="headerlink" title="调整JVM堆大小"></a>调整JVM堆大小</h2><p>为确保Elasticsearch具有足够的操作余地，应调整默认的JVM堆大小<br>根据经验，最大堆大小应设置为RAM的50％，但不超过32GB（由于Java指针在较大的堆中效率低下）。Elastic还建议最大和最小堆大小的值相同。<br>可以使用jvm.options文件中的Xmx和Xms设置来配置这些值.</p>
<h2 id="禁用交换"><a href="#禁用交换" class="headerlink" title="禁用交换"></a>禁用交换</h2><p>交换未使用的内存是一种已知的行为，但在Elasticsearch的上下文中可能导致断开连接，性能不佳以及一般情况下 - 一个不稳定的集群。</p>
<p>为了避免交换，您可以禁用所有交换（如果Elasticsearch是服务器上运行的唯一服务，则建议使用），或者您可以使用mlockall将Elasticsearch进程锁定到RAM。</p>
<p>为此，请在群集中的所有节点上打开Elasticsearch配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;elasticsearch&#x2F;elasticsearch.yml</span><br><span class="line"></span><br><span class="line">取消注释以下行：</span><br><span class="line">bootstrap.mlockall: true</span><br><span class="line"></span><br><span class="line">vim &#x2F;etc&#x2F;default&#x2F;elasticsearch</span><br><span class="line">进行以下配置</span><br><span class="line">MAX_LOCKED_MEMORY&#x3D;unlimited</span><br></pre></td></tr></table></figure>

<h2 id="调整虚拟内存"><a href="#调整虚拟内存" class="headerlink" title="调整虚拟内存"></a>调整虚拟内存</h2><p>为避免耗尽虚拟内存，请增加mmap计数的限制</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vim &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line"></span><br><span class="line">vm.max_map_count&#x3D;262144</span><br><span class="line">在DEB&#x2F;RPM上，自动配置此设置。</span><br></pre></td></tr></table></figure>
<h2 id="增加打开文件描述符限制"><a href="#增加打开文件描述符限制" class="headerlink" title="增加打开文件描述符限制"></a>增加打开文件描述符限制</h2><p>另一个重要配置是打开文件描述符的限制。由于Elasticsearch使用了大量文件描述符，因此必须确保定义的限制足够，否则最终可能会丢失数据。</p>
<p>此设置的常见建议是65,536或更高。在DEB/RPM上，默认设置已经配置为满足此要求，但您当然可以对其进行微调。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim  &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br><span class="line"></span><br><span class="line">设置限制</span><br><span class="line">- nofile 65536</span><br></pre></td></tr></table></figure>

<h1 id="Elasticsearch集群API"><a href="#Elasticsearch集群API" class="headerlink" title="Elasticsearch集群API"></a>Elasticsearch集群API</h1><p>查询当前所有的索引</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl &#39;localhost:9200&#x2F;_cat&#x2F;indices?v&#39;</span><br></pre></td></tr></table></figure>

<p>查看node情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl &#39;localhost:9200&#x2F;_cat&#x2F;nodes?v&#39;</span><br></pre></td></tr></table></figure>

<p>查看群集的常规信息并衡量其运行状况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -XGET &#39;localhost:9200&#x2F;_cluster&#x2F;health?pretty&#39;</span><br><span class="line">curl &#39;localhost:9200&#x2F;_cat&#x2F;health?v&#39;</span><br><span class="line">绿色表示一切正常, 黄色表示所有的数据可用但是部分副本还没有分配,红色表示部分数据因为某些原因不可用</span><br></pre></td></tr></table></figure>

<p>查看整个群集的详细状态报告</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -XGET &#39;localhost:9200&#x2F;_cluster&#x2F;state?pretty&#39;</span><br></pre></td></tr></table></figure>

<p>监控整个群集的性能指标</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -XGET &#39;localhost:9200&#x2F;_cluster&#x2F;stats?human&amp;pretty&#39;</span><br></pre></td></tr></table></figure>

<p>检查群集中特定节点的度量标准<br>所有节点：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -XGET &#39;localhost:9200&#x2F;_nodes&#x2F;stats?pretty&#39;</span><br></pre></td></tr></table></figure>
<p>特定节点：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -XGET &#39;localhost:9200&#x2F;_nodes&#x2F;node-1&#x2F;stats?pretty&#39;</span><br></pre></td></tr></table></figure>
<p>仅有索引的统计数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -XGET &#39;localhost:9200&#x2F;_nodes&#x2F;stats&#x2F;indices?pretty&#39;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Docker Swarm 构建MongoDB分片副本集群</title>
    <url>/posts/8b167821.html</url>
    <content><![CDATA[<h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><p>两台服务器，建立 Docker Swarm 集群，一个 Manager，一个 Worker。</p>
<p>docker 版本：18-06<br>mongo 版本：3.6<br>172.16.2.30 server01<br>172.16.2.31 server02</p>
<a id="more"></a>
<h1 id="MongoDB-集群架构设计"><a href="#MongoDB-集群架构设计" class="headerlink" title="MongoDB 集群架构设计"></a>MongoDB 集群架构设计</h1><img src="/images/docker-swarm-mongodb.png" width="100%" height="100%">

<h1 id="搭建集群"><a href="#搭建集群" class="headerlink" title="搭建集群"></a>搭建集群</h1><h2 id="Manager节点创建集群网络"><a href="#Manager节点创建集群网络" class="headerlink" title="Manager节点创建集群网络"></a>Manager节点创建集群网络</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker network create -d overlay --attachable mongo</span><br></pre></td></tr></table></figure>

<h2 id="所有节点创建相关文件夹"><a href="#所有节点创建相关文件夹" class="headerlink" title="所有节点创建相关文件夹"></a>所有节点创建相关文件夹</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;root&#x2F;mongo&#x2F;config &#x2F;root&#x2F;mongo&#x2F;shard1 &#x2F;root&#x2F;mongo&#x2F;shard2 &#x2F;root&#x2F;mongo&#x2F;shard3</span><br></pre></td></tr></table></figure>
<h2 id="Manager节点创建-stack-yml"><a href="#Manager节点创建-stack-yml" class="headerlink" title="Manager节点创建 stack.yml"></a>Manager节点创建 stack.yml</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">version: &#39;3.3&#39;</span><br><span class="line">services:</span><br><span class="line">  mongors1n1:</span><br><span class="line">    # docker 中国的镜像加速地址</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --shardsvr --replSet shard1 --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;shard1:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        # 指定在服务器 server01 上启动</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server01</span><br><span class="line">  mongors2n1:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --shardsvr --replSet shard2 --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;shard2:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server01</span><br><span class="line">  mongors3n1:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --shardsvr --replSet shard3 --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;shard3:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server01</span><br><span class="line">  mongors1n2:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --shardsvr --replSet shard1 --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;shard1:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server01</span><br><span class="line">  mongors2n2:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --shardsvr --replSet shard2 --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;shard2:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server01</span><br><span class="line">  mongors3n2:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --shardsvr --replSet shard3 --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;shard3:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server01</span><br><span class="line">  mongors1n3:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --shardsvr --replSet shard1 --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;shard1:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server02</span><br><span class="line">  mongors2n3:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --shardsvr --replSet shard2 --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;shard2:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server02</span><br><span class="line">  mongors3n3:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --shardsvr --replSet shard3 --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;shard3:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server02</span><br><span class="line">  cfg1:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --configsvr --replSet cfgrs --smallfiles --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;config:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server01</span><br><span class="line">  cfg2:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --configsvr --replSet cfgrs --smallfiles --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;config:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server01</span><br><span class="line">  cfg3:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    command: mongod --configsvr --replSet cfgrs --smallfiles --dbpath &#x2F;data&#x2F;db --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">      - &#x2F;root&#x2F;mongo&#x2F;config:&#x2F;data&#x2F;db</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      replicas: 1</span><br><span class="line">      placement:</span><br><span class="line">        constraints:</span><br><span class="line">          - node.hostname&#x3D;&#x3D;server01</span><br><span class="line">  mongos:</span><br><span class="line">    image: registry.docker-cn.com&#x2F;library&#x2F;mongo</span><br><span class="line">    # mongo 3.6 版默认绑定IP为 127.0.0.1，此处绑定 0.0.0.0 是允许其他容器或主机可以访问</span><br><span class="line">    command: mongos --configdb cfgrs&#x2F;cfg1:27017,cfg2:27017,cfg3:27017 --bind_ip 0.0.0.0 --port 27017</span><br><span class="line">    networks:</span><br><span class="line">      - mongo</span><br><span class="line">    # 映射宿主机的 27017 端口</span><br><span class="line">    ports:</span><br><span class="line">      - 27017:27017</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime</span><br><span class="line">    depends_on:</span><br><span class="line">      - cfg1</span><br><span class="line">      - cfg2</span><br><span class="line">      - cfg3</span><br><span class="line">    deploy:</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">      # 在集群内的每一台服务器上都启动一个容器</span><br><span class="line">      mode: global</span><br><span class="line">networks:</span><br><span class="line">  mongo:</span><br><span class="line">    external: true</span><br></pre></td></tr></table></figure>

<h2 id="启动服务，在Manager节点上执行"><a href="#启动服务，在Manager节点上执行" class="headerlink" title="启动服务，在Manager节点上执行"></a>启动服务，在Manager节点上执行</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker stack deploy -c stack.yml mongo</span><br></pre></td></tr></table></figure>

<h2 id="Manager节点查看服务的启动情况"><a href="#Manager节点查看服务的启动情况" class="headerlink" title="Manager节点查看服务的启动情况"></a>Manager节点查看服务的启动情况</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker service ls</span><br><span class="line"></span><br><span class="line">正常情况下，会出现如下结果：</span><br><span class="line"></span><br><span class="line">[root@server01 mongo]# docker service ls</span><br><span class="line">ID                  NAME                      MODE                REPLICAS            IMAGE                                         PORTS</span><br><span class="line">itmtld6o6ldv        mongo_cfg1                replicated          0&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">brlsgen2xluh        mongo_cfg2                replicated          0&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">yx4rg2p21pyz        mongo_cfg3                replicated          1&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">kn4zlkff55ly        mongo_mongors1n1          replicated          1&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">8siujq2xe4mt        mongo_mongors1n2          replicated          0&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">tv6epypyk65n        mongo_mongors1n3          replicated          0&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">ri7zzhp8v5c6        mongo_mongors2n1          replicated          1&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">g9v90ifpupns        mongo_mongors2n2          replicated          0&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">t5ircpm49ojl        mongo_mongors2n3          replicated          0&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">59o5883orn01        mongo_mongors3n1          replicated          1&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">wvemmmjv7mx6        mongo_mongors3n2          replicated          0&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">5307by34ep0j        mongo_mongors3n3          replicated          0&#x2F;1                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   </span><br><span class="line">bs0ols6tbijc        mongo_mongos              global              3&#x2F;3                 registry.docker-cn.com&#x2F;library&#x2F;mongo:latest   *:27017-&gt;27017&#x2F;tcp</span><br></pre></td></tr></table></figure>

<h1 id="初始化集群"><a href="#初始化集群" class="headerlink" title="初始化集群"></a>初始化集群</h1><h2 id="Manager节点初始化-Mongo-配置集群"><a href="#Manager节点初始化-Mongo-配置集群" class="headerlink" title="Manager节点初始化 Mongo 配置集群"></a>Manager节点初始化 Mongo 配置集群</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker exec -it $(docker ps | grep &quot;cfg1&quot; | awk &#39;&#123; print $1 &#125;&#39;) bash -c &quot;echo &#39;rs.initiate(&#123;_id: \&quot;cfgrs\&quot;,configsvr: true, members: [&#123; _id : 0, host : \&quot;cfg1\&quot; &#125;,&#123; _id : 1, host : \&quot;cfg2\&quot; &#125;, &#123; _id : 2, host : \&quot;cfg3\&quot; &#125;]&#125;)&#39; | mongo&quot;</span><br></pre></td></tr></table></figure>

<h2 id="Manager节点初始化三个-Mongo-数据集群"><a href="#Manager节点初始化三个-Mongo-数据集群" class="headerlink" title="Manager节点初始化三个 Mongo 数据集群"></a>Manager节点初始化三个 Mongo 数据集群</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker exec -it $(docker ps | grep &quot;mongors1n1&quot; | awk &#39;&#123; print $1 &#125;&#39;) bash -c &quot;echo &#39;rs.initiate(&#123;_id : \&quot;shard1\&quot;, members: [&#123; _id : 0, host : \&quot;mongors1n1\&quot; &#125;,&#123; _id : 1, host : \&quot;mongors1n2\&quot; &#125;,&#123; _id : 2, host : \&quot;mongors1n3\&quot;, arbiterOnly: true &#125;]&#125;)&#39; | mongo&quot;</span><br><span class="line"></span><br><span class="line">docker exec -it $(docker ps | grep &quot;mongors2n1&quot; | awk &#39;&#123; print $1 &#125;&#39;) bash -c &quot;echo &#39;rs.initiate(&#123;_id : \&quot;shard2\&quot;, members: [&#123; _id : 0, host : \&quot;mongors2n1\&quot; &#125;,&#123; _id : 1, host : \&quot;mongors2n2\&quot; &#125;,&#123; _id : 2, host : \&quot;mongors2n3\&quot;, arbiterOnly: true &#125;]&#125;)&#39; | mongo&quot;</span><br><span class="line"></span><br><span class="line">docker exec -it $(docker ps | grep &quot;mongors3n1&quot; | awk &#39;&#123; print $1 &#125;&#39;) bash -c &quot;echo &#39;rs.initiate(&#123;_id : \&quot;shard3\&quot;, members: [&#123; _id : 0, host : \&quot;mongors3n1\&quot; &#125;,&#123; _id : 1, host : \&quot;mongors3n2\&quot; &#125;,&#123; _id : 2, host : \&quot;mongors3n3\&quot;, arbiterOnly: true &#125;]&#125;)&#39; | mongo&quot;</span><br></pre></td></tr></table></figure>

<h2 id="Manager节点将三个数据集群当做分片加入-mongos"><a href="#Manager节点将三个数据集群当做分片加入-mongos" class="headerlink" title="Manager节点将三个数据集群当做分片加入 mongos"></a>Manager节点将三个数据集群当做分片加入 mongos</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker exec -it $(docker ps | grep &quot;mongos&quot; | awk &#39;&#123; print $1 &#125;&#39;) bash -c &quot;echo &#39;sh.addShard(\&quot;shard1&#x2F;mongors1n1:27017,mongors1n2:27017,mongors1n3:27017\&quot;)&#39; | mongo &quot;</span><br><span class="line"></span><br><span class="line">docker exec -it $(docker ps | grep &quot;mongos&quot; | awk &#39;&#123; print $1 &#125;&#39;) bash -c &quot;echo &#39;sh.addShard(\&quot;shard2&#x2F;mongors2n1:27017,mongors2n3:27017,mongors2n3:27017\&quot;)&#39; | mongo &quot;</span><br><span class="line"></span><br><span class="line">docker exec -it $(docker ps | grep &quot;mongos&quot; | awk &#39;&#123; print $1 &#125;&#39;) bash -c &quot;echo &#39;sh.addShard(\&quot;shard3&#x2F;mongors3n1:27017,mongors3n2:27017,mongors3n3:27017\&quot;)&#39; | mongo &quot;</span><br></pre></td></tr></table></figure>

<h2 id="连接集群"><a href="#连接集群" class="headerlink" title="连接集群"></a>连接集群</h2><p>内部：在 mongo 网络下的容器，通过 mongos:27017 连接</p>
<p>外部：通过 IP:27017 连接，IP 可以为两台服务的中的一个的 IP</p>
<p>后期可以添加mongdodb可视化工具：mongodb-express</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>swarm</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title>Dockerfile 最佳实践准则</title>
    <url>/posts/99efb1a6.html</url>
    <content><![CDATA[<p>原文链接：<br><a href="https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/">https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/</a></p>
<p><strong>注意</strong></p>
<p>本文使用一个基于 Maven 的 Java 项目作为示例，然后不断改进 Dockerfile 的写法，直到最后写出一个最优雅的 Dockerfile。中间的所有步骤都是为了说明某一方面的最佳实践。</p>
<a id="more"></a>
<h1 id="减少构建时间"><a href="#减少构建时间" class="headerlink" title="减少构建时间"></a>减少构建时间</h1><p>一个开发周期包括构建 Docker 镜像，更改代码，然后重新构建 Docker 镜像。在构建镜像的过程中，如果能够利用缓存，可以减少不必要的重复构建步骤。</p>
<h2 id="构建顺序影响缓存的利用率"><a href="#构建顺序影响缓存的利用率" class="headerlink" title="构建顺序影响缓存的利用率"></a>构建顺序影响缓存的利用率</h2><img src="/images/2019-07-09-063907.jpg" width="100%" height="100%">

<p>镜像的构建顺序很重要，当你向 Dockerfile 中添加文件，或者修改其中的某一行时，那一部分的缓存就会失效，该缓存的后续步骤都会中断，需要重新构建。所以优化缓存的最佳方法是把不需要经常更改的行放到最前面，更改最频繁的行放到最后面。</p>
<h2 id="只拷贝需要的文件，防止缓存溢出"><a href="#只拷贝需要的文件，防止缓存溢出" class="headerlink" title="只拷贝需要的文件，防止缓存溢出"></a>只拷贝需要的文件，防止缓存溢出</h2><img src="/images/2019-07-09-070604.jpg" width="100%" height="100%">

<p>当拷贝文件到镜像中时，尽量只拷贝需要的文件，切忌使用 COPY . 指令拷贝整个目录。如果被拷贝的文件内容发生了更改，缓存就会被破坏。在上面的示例中，镜像中只需要构建好的 jar 包，因此只需要拷贝这个文件就行了，这样即使其他不相关的文件发生了更改也不会影响缓存。</p>
<h2 id="最小化可缓存的执行层"><a href="#最小化可缓存的执行层" class="headerlink" title="最小化可缓存的执行层"></a>最小化可缓存的执行层</h2><img src="/images/2019-07-09-071520.jpg" width="100%" height="100%">

<p>每一个 RUN 指令都会被看作是可缓存的执行单元。太多的 RUN 指令会增加镜像的层数，增大镜像体积，而将所有的命令都放到同一个 RUN 指令中又会破坏缓存，从而延缓开发周期。当使用包管理器安装软件时，一般都会先更新软件索引信息，然后再安装软件。推荐将更新索引和安装软件放在同一个 RUN 指令中，这样可以形成一个可缓存的执行单元，否则你可能会安装旧的软件包。</p>
<h1 id="减小镜像体积"><a href="#减小镜像体积" class="headerlink" title="减小镜像体积"></a>减小镜像体积</h1><p>镜像的体积很重要，因为镜像越小，部署的速度更快，攻击范围越小。</p>
<h2 id="删除不必要依赖"><a href="#删除不必要依赖" class="headerlink" title="删除不必要依赖"></a>删除不必要依赖</h2><img src="/images/2019-07-09-073718.jpg" width="100%" height="100%">

<p>删除不必要的依赖，不要安装调试工具。如果实在需要调试工具，可以在容器运行之后再安装。某些包管理工具（如 apt）除了安装用户指定的包之外，还会安装推荐的包，这会无缘无故增加镜像的体积。apt 可以通过添加参数 -–no-install-recommends 来确保不会安装不需要的依赖项。如果确实需要某些依赖项，请在后面手动添加。</p>
<h2 id="删除包管理工具的缓存"><a href="#删除包管理工具的缓存" class="headerlink" title="删除包管理工具的缓存"></a>删除包管理工具的缓存</h2><img src="/images/2019-07-09-073800.jpg" width="100%" height="100%">

<p>包管理工具会维护自己的缓存，这些缓存会保留在镜像文件中，推荐的处理方法是在每一个 RUN 指令的末尾删除缓存。如果你在下一条指令中删除缓存，不会减小镜像的体积。</p>
<p>当然了，还有其他更高级的方法可以用来减小镜像体积，如下文将会介绍的多阶段构建。接下来我们将探讨如何优化 Dockerfile 的可维护性、安全性和可重复性。</p>
<h1 id="可维护性"><a href="#可维护性" class="headerlink" title="可维护性"></a>可维护性</h1><h2 id="尽量使用官方镜像"><a href="#尽量使用官方镜像" class="headerlink" title="尽量使用官方镜像"></a>尽量使用官方镜像</h2><img src="/images/2019-07-09-074136.jpg" width="100%" height="100%">

<p>使用官方镜像可以节省大量的维护时间，因为官方镜像的所有安装步骤都使用了最佳实践。如果你有多个项目，可以共享这些镜像层，因为他们都可以使用相同的基础镜像。</p>
<h2 id="使用更具体的标签"><a href="#使用更具体的标签" class="headerlink" title="使用更具体的标签"></a>使用更具体的标签</h2><img src="/images/2019-07-09-081644.jpg" width="100%" height="100%">

<p>基础镜像尽量不要使用 latest 标签。虽然这很方便，但随着时间的推移，latest 镜像可能会发生重大变化。因此在 Dockerfile 中最好指定基础镜像的具体标签。我们使用 openjdk 作为示例，指定标签为 8。其他更多标签请查看官方仓库。</p>
<h2 id="使用体积最小的基础镜像"><a href="#使用体积最小的基础镜像" class="headerlink" title="使用体积最小的基础镜像"></a>使用体积最小的基础镜像</h2><img src="/images/2019-07-09-082549.jpg" width="100%" height="100%">

<p>基础镜像的标签风格不同，镜像体积就会不同。slim 风格的镜像是基于 Debian 发行版制作的，而 alpine 风格的镜像是基于体积更小的 Alpine Linux 发行版制作的。其中一个明显的区别是：Debian 使用的是 GNU 项目所实现的 C 语言标准库，而 Alpine 使用的是 Musl C 标准库，它被设计用来替代 GNU C 标准库（glibc）的替代品，用于嵌入式操作系统和移动设备。因此使用 Alpine 在某些情况下会遇到兼容性问题。 以 openjdk 为例，jre 风格的镜像只包含 Java 运行时，不包含 SDK，这么做也可以大大减少镜像体积。</p>
<h1 id="重复利用"><a href="#重复利用" class="headerlink" title="重复利用"></a>重复利用</h1><p>到目前为止，我们一直都在假设你的 jar 包是在主机上构建的，这还不是理想方案，因为没有充分利用容器提供的一致性环境。例如，如果你的 Java 应用依赖于某一个特定的操作系统的库，就可能会出现问题，因为环境不一致（具体取决于构建 jar 包的机器）。</p>
<h2 id="在一致的环境中从源代码构建"><a href="#在一致的环境中从源代码构建" class="headerlink" title="在一致的环境中从源代码构建"></a>在一致的环境中从源代码构建</h2><p>源代码是你构建 Docker 镜像的最终来源，Dockerfile 里面只提供了构建步骤。</p>
<img src="/images/2019-07-09-085148.jpg" width="100%" height="100%">

<p>首先应该确定构建应用所需的所有依赖，本文的示例 Java 应用很简单，只需要 Maven 和 JDK，所以基础镜像应该选择官方的体积最小的 maven 镜像，该镜像也包含了 JDK。如果你需要安装更多依赖，可以在 RUN 指令中添加。pom.xml 文件和 src 文件夹需要被复制到镜像中，因为最后执行 mvn package 命令（-e 参数用来显示错误，-B 参数表示以非交互式的“批处理”模式运行）打包的时候会用到这些依赖文件。</p>
<p>虽然现在我们解决了环境不一致的问题，但还有另外一个问题：每次代码更改之后，都要重新获取一遍 pom.xml 中描述的所有依赖项。下面我们来解决这个问题。</p>
<h2 id="在单独的步骤中获取依赖项"><a href="#在单独的步骤中获取依赖项" class="headerlink" title="在单独的步骤中获取依赖项"></a>在单独的步骤中获取依赖项</h2><img src="/images/2019-07-09-091331.jpg" width="100%" height="100%">

<p>结合前面提到的缓存机制，我们可以让获取依赖项这一步变成可缓存单元，只要 pom.xml 文件的内容没有变化，无论代码如何更改，都不会破坏这一层的缓存。上图中两个 COPY 指令中间的 RUN 指令用来告诉 Maven 只获取依赖项。</p>
<p>现在又遇到了一个新问题：跟之前直接拷贝 jar 包相比，镜像体积变得更大了，因为它包含了很多运行应用时不需要的构建依赖项。</p>
<h2 id="使用多阶段构建来删除构建时的依赖项"><a href="#使用多阶段构建来删除构建时的依赖项" class="headerlink" title="使用多阶段构建来删除构建时的依赖项"></a>使用多阶段构建来删除构建时的依赖项</h2><img src="/images/2019-07-09-093630.jpg" width="100%" height="100%">

<p>多阶段构建可以由多个 FROM 指令识别，每一个 FROM 语句表示一个新的构建阶段，阶段名称可以用 AS 参数指定。本例中指定第一阶段的名称为 builder，它可以被第二阶段直接引用。两个阶段环境一致，并且第一阶段包含所有构建依赖项。</p>
<p>第二阶段是构建最终镜像的最后阶段，它将包括应用运行时的所有必要条件，本例是基于 Alpine 的最小 JRE 镜像。上一个构建阶段虽然会有大量的缓存，但不会出现在第二阶段中。为了将构建好的 jar 包添加到最终的镜像中，可以使用 COPY –from=STAGE_NAME 指令，其中 STAGE_NAME 是上一构建阶段的名称。</p>
<img src="/images/2019-07-09-094739.jpg" width="100%" height="100%">

<p>多阶段构建是删除构建依赖的首选方案。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Elasticsearch系列---基本概念及工作原理</title>
    <url>/posts/d9404faf.html</url>
    <content><![CDATA[<h1 id="Elasticsearch是什么？"><a href="#Elasticsearch是什么？" class="headerlink" title="Elasticsearch是什么？"></a>Elasticsearch是什么？</h1><p>Elasticsearch简称ES，是一个基于Lucene构建的开源、分布式、Restful接口的全文搜索引擎，还是一个分布式文档数据库。天生就是分布式、高可用、可扩展的，可以在很短的时间内存储、搜索和分析大量的数据。</p>
<a id="more"></a>
<h2 id="什么是全文搜索？"><a href="#什么是全文搜索？" class="headerlink" title="什么是全文搜索？"></a>什么是全文搜索？</h2><p>全文搜索也叫全文检索，是指扫描文章中的每一个词，对每一个词进建立一个索引，指明该词在文章中出现的次数和位置，当前端用户输入的关键词发起查询请求后，搜索引擎就会根据事先建立的索引进行查找，并将查询的结果响应给用户。<br>这里有两个关键字：分词和索引，Elasticsearch内部会完成这两件事情，对保存的文本内容按规则进行分词，并对这些分词后的词条建立索引，供用户查询。</p>
<h2 id="什么是倒排索引？"><a href="#什么是倒排索引？" class="headerlink" title="什么是倒排索引？"></a>什么是倒排索引？</h2><p>全文搜索过程根据关键词创建的索引叫倒排索引，顾名思义，建立正向关系“文本内容-关键词”叫正排索引，后续会介绍，倒排索引就是把原有关系倒过来，建立成“关键词-文本内容”的关系，这样的关系非常利于搜索。</p>
<h1 id="Elasticsearch什么场景适用？"><a href="#Elasticsearch什么场景适用？" class="headerlink" title="Elasticsearch什么场景适用？"></a>Elasticsearch什么场景适用？</h1><h2 id="常见场景"><a href="#常见场景" class="headerlink" title="常见场景"></a>常见场景</h2><h3 id="搜索类场景"><a href="#搜索类场景" class="headerlink" title="搜索类场景"></a>搜索类场景</h3><p>常见的搜索场景比如说电商网站、招聘网站、新闻资讯类网站、各种app内的搜索。</p>
<h3 id="日志分析类场景"><a href="#日志分析类场景" class="headerlink" title="日志分析类场景"></a>日志分析类场景</h3><p>经典的ELK组合（Elasticsearch/Logstash/Kibana），可以完成日志收集，日志存储，日志分析查询界面基本功能，目前该方案的实现很普及，大部分企业日志分析系统都是使用该方案。</p>
<h3 id="数据预警平台及数据分析场景"><a href="#数据预警平台及数据分析场景" class="headerlink" title="数据预警平台及数据分析场景"></a>数据预警平台及数据分析场景</h3><p>例如电商价格预警，在支持的电商平台设置价格预警，当优惠的价格低于某个值时，触发通知消息，通知用户购买。<br>数据分析常见的比如分析电商平台销售量top 10的品牌，分析博客系统、头条网站top 10关注度、评论数、访问量的内容等等。</p>
<h3 id="商业BI系统"><a href="#商业BI系统" class="headerlink" title="商业BI系统"></a>商业BI系统</h3><p>比大型零售超市，需要分析上一季度用户消费金额，年龄段，每天各时间段到店人数分布等信息，输出相应的报表数据，并预测下一季度的热卖商品，根据年龄段定向推荐适宜产品。Elasticsearch执行数据分析和挖掘，Kibana做数据可视化。</p>
<h2 id="常见案例"><a href="#常见案例" class="headerlink" title="常见案例"></a>常见案例</h2><ul>
<li>维基百科、百度百科：有全文检索、高亮、搜索推荐功能</li>
<li>stack overflow：有全文检索，可以根据报错关键信息，去搜索解决方法。</li>
<li>github：从上千亿行代码中搜索你想要的关键代码。</li>
<li>日志分析系统：各企业内部搭建的ELK平台</li>
</ul>
<h1 id="Elasticsearch的架构图"><a href="#Elasticsearch的架构图" class="headerlink" title="Elasticsearch的架构图"></a>Elasticsearch的架构图</h1><img src="/images/39521071.png" width="100%" height="100%">

<p>架构各组件简单释义:</p>
<ul>
<li>gateway 底层存储系统，一般为文件系统，支持多种类型。</li>
<li>distributed lucence directory 基于lucence的分布式框架，封装了建立倒排索引、数据存储、translog、segment等实现。</li>
<li>模块层 ES的主要模块，包含索引模块、搜索模块、映射模块。</li>
<li>Discovery 集群node发现模块，用于集群node之间的通信，选举coordinate node操作，支持多种发现机制，如zen，ec2等。</li>
<li>script 脚本解析模块，用来支持在查询语句中编写的脚本，如painless，groovy，python等。</li>
<li>plugins 第三方插件，各种高级功能可由插件提供，支持定制。</li>
<li>transport/jmx 通信模块，数据传输，底层使用netty框架</li>
<li>restful/node 对外提供的访问Elasticsearch集群的接口</li>
<li>x-pack elasticsearch的一个扩展包，集成安全、警告、监视、图形和报告功能，无缝接入，可插拔设计。</li>
</ul>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="NRT"><a href="#NRT" class="headerlink" title="NRT"></a>NRT</h3><p>Near Realtime，近实时，有两个层面的含义，一是从写入一条数据到这条数据可以被搜索，有一段非常小的延迟（大约1秒左右），二是基于Elasticsearch的搜索和分析操作，耗时可以达到秒级。</p>
<h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><p>集群，对外提供索引和搜索的服务，包含一个或多个节点，每个节点属于哪个集群是通过集群名称来决定的（默认名称是elasticsearch），集群名称搞错了后果很严重。命名建议是研发、测试环境、准生产、生产环境用不同的名称增加区分度，例如研发使用es-dev，测试使用es-test，准生产使用es-stg，生产环境使用es-pro这样的名字来区分。如果是中小型应用，集群可以只有一个节点。</p>
<h3 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h3><p>单独一个Elasticsearch服务器实例称为一个node，node是集群的一部分，每个node有独立的名称，默认是启动时获取一个UUID作为名称，也可以自行配置，node名称特别重要，Elasticsearch集群是通过node名称进行管理和通信的，一个node只能加入一个Elasticsearch集群当中，集群提供完整的数据存储，索引和搜索的功能，它下面的每个node分摊上述功能（每条数据都会索引到node上）。</p>
<h3 id="shard"><a href="#shard" class="headerlink" title="shard"></a>shard</h3><p>分片，是单个Lucene索引，由于单台机器的存储容量是有限的（如1TB），而Elasticsearch索引的数据可能特别大（PB级别，并且30GB/天的写入量），单台机器无法存储全部数据，就需要将索引中的数据切分为多个shard，分布在多台服务器上存储。利用shard可以很好地进行横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升集群整体的吞吐量和性能。<br>shard在使用时比较简单，只需要在创建索引时指定shard的数量即可，剩下的都交给Elasticsearch来完成，只是创建索引时一旦指定shard数量，后期就不能再更改了。</p>
<h3 id="replica"><a href="#replica" class="headerlink" title="replica"></a>replica</h3><p>索引副本，完全拷贝shard的内容，shard与replica的关系可以是一对多，同一个shard可以有一个或多个replica，并且同一个shard下的replica数据完全一样，replica作为shard的数据拷贝，承担以下三个任务：</p>
<ol>
<li>shard故障或宕机时，其中一个replica可以升级成shard。</li>
<li>replica保证数据不丢失（冗余机制），保证高可用。</li>
<li>replica可以分担搜索请求，提升整个集群的吞吐量和性能。</li>
</ol>
<p>shard的全称叫primary shard，replica全称叫replica shard，primary shard数量在创建索引时指定，后期不能修改，replica shard后期可以修改。默认每个索引的primary shard值为5，replica shard值为5，含义是5个primary shard，5个replica shard，共10个shard。</p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><p>索引，具有相同结构的文档集合，类似于关系型数据库的数据库实例（6.0.0版本type废弃后，索引的概念下降到等同于数据库表的级别）。一个集群里可以定义多个索引，如客户信息索引、商品分类索引、商品索引、订单索引、评论索引等等，分别定义自己的数据结构。<br>索引命名要求全部使用小写，建立索引、搜索、更新、删除操作都需要用到索引名称。</p>
<h3 id="type"><a href="#type" class="headerlink" title="type"></a>type</h3><p>类型，原本是在索引(Index)内进行的逻辑细分，但后来发现企业研发为了增强可阅读性和可维护性，制订的规范约束，同一个索引下很少还会再使用type进行逻辑拆分（如同一个索引下既有订单数据，又有评论数据），因而在6.0.0版本之后，此定义废弃。</p>
<h3 id="Document"><a href="#Document" class="headerlink" title="Document"></a>Document</h3><p>文档，Elasticsearch最小的数据存储单元，JSON数据格式，类似于关系型数据库的表记录（一行数据），结构定义多样化，同一个索引下的document，结构尽可能相同。</p>
<h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><h3 id="启动过程"><a href="#启动过程" class="headerlink" title="启动过程"></a>启动过程</h3><p>当Elasticsearch的node启动时，默认使用广播寻找集群中的其他node，并与之建立连接，如果集群已经存在，其中一个节点角色特殊一些，叫coordinate node（协调者，也叫master节点），负责管理集群node的状态，有新的node加入时，会更新集群拓扑信息。如果当前集群不存在，那么启动的node就自己成为coordinate node。</p>
<h3 id="应用程序与集群通信过程"><a href="#应用程序与集群通信过程" class="headerlink" title="应用程序与集群通信过程"></a>应用程序与集群通信过程</h3><p>虽然Elasticsearch设置了Coordinate Node用来管理集群，但这种设置对客户端（应用程序）来说是透明的，客户端可以请求任何一个它已知的node，如果该node是集群当前的Coordinate，那么它会将请求转发到相应的Node上进行处理，如果该node不是Coordinate，那么该node会先将请求转交给Coordinate Node，再由Coordinate进行转发，搓着各node返回的数据全部交由Coordinate Node进行汇总，最后返回给客户端。</p>
<h3 id="集群内node有效性检测"><a href="#集群内node有效性检测" class="headerlink" title="集群内node有效性检测"></a>集群内node有效性检测</h3><p>正常工作时，Coordinate Node会定期与拓扑结构中的Node进行通信，检测实例是否正常工作，如果在指定的时间周期内，Node无响应，那么集群会认为该Node已经宕机。集群会重新进行均衡：</p>
<ol>
<li>重新分配宕机的Node，其他Node中有该Node的replica shard，选出一个replica shard，升级成为primary shard。</li>
<li>重新安置新的shard。</li>
<li>拓扑更新，分发给该Node的请求重新映射到目前正常的Node上。</li>
</ol>
<h1 id="Restful-API"><a href="#Restful-API" class="headerlink" title="Restful API"></a>Restful API</h1><p>Kibana界面的Dev Tools菜单，可以发送Elasticsearch的Restful请求。</p>
<h2 id="检查集群的健康状况"><a href="#检查集群的健康状况" class="headerlink" title="检查集群的健康状况"></a>检查集群的健康状况</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;_cat&#x2F;health?v</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">epoch      timestamp cluster        status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent </span><br><span class="line">1587087676 09:41:16  my-application green           2         2     70  35    0    0        0             0                  -                100.0%</span><br></pre></td></tr></table></figure>

<p>集群的状态有green、yellow、red三种，定义如下：</p>
<ul>
<li>green：每个索引的primary shard和replica shard都是active状态的</li>
<li>yellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态</li>
<li>red：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了</li>
</ul>
<h2 id="查看集群索引"><a href="#查看集群索引" class="headerlink" title="查看集群索引"></a>查看集群索引</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;_cat&#x2F;indices?v</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">health status index                 pri rep docs.count docs.deleted store.size pri.store.size </span><br><span class="line">green  open   depart_person_statics   5   1       7015            0      2.6mb          1.3mb </span><br><span class="line">green  open   depart_person           5   1      22794         7536      8.2mb            4mb </span><br><span class="line">green  open   enterprise_unit_es      5   1    3274712            1        2gb            1gb </span><br><span class="line">green  open   duty_statistics         5   1          0            0      1.5kb           780b </span><br><span class="line">green  open   conference-index-2      5   1          0            0      1.5kb           780b </span><br><span class="line">green  open   test                    5   1          2            0      8.3kb          4.1kb </span><br><span class="line">green  open   statistics              5   1    8428549        17812      1.6gb          834mb</span><br></pre></td></tr></table></figure>

<h2 id="查看node信息"><a href="#查看node信息" class="headerlink" title="查看node信息"></a>查看node信息</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;_cat&#x2F;nodes?v</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">host         ip           heap.percent ram.percent load node.role master name  </span><br><span class="line">192.168.1.63 192.168.1.63           66          83 0.30 d         *      node1 </span><br><span class="line">192.168.1.62 192.168.1.62           52          98 0.59 d         m      node2</span><br></pre></td></tr></table></figure>

<h2 id="创建索引命令"><a href="#创建索引命令" class="headerlink" title="创建索引命令"></a>创建索引命令</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT &#x2F;test?pretty</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;acknowledged&quot; : true,</span><br><span class="line">  &quot;shards_acknowledged&quot; : true,</span><br><span class="line">  &quot;index&quot; : &quot;test&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="删除索引命令"><a href="#删除索引命令" class="headerlink" title="删除索引命令"></a>删除索引命令</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELETE &#x2F;test?pretty</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;acknowledged&quot; : true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title>部署ELK日志分析7.3.x</title>
    <url>/posts/5b135e8d.html</url>
    <content><![CDATA[<h1 id="安装公共签名密钥"><a href="#安装公共签名密钥" class="headerlink" title="安装公共签名密钥"></a>安装公共签名密钥</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo rpm --import https:&#x2F;&#x2F;packages.elastic.co&#x2F;GPG-KEY-elasticsearch</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>在/etc/yum.repos.d/ 目录下新建一个elastic.repo文件，新增如下内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;yum.repos.d&#x2F;elastic.repo</span><br><span class="line"></span><br><span class="line">[elastic-7.x]</span><br><span class="line">name&#x3D;Elastic repository for 7.x packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;packages&#x2F;7.x&#x2F;yum</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;GPG-KEY-elasticsearch</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">autorefresh&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br></pre></td></tr></table></figure>

<h1 id="安装elasticsearch"><a href="#安装elasticsearch" class="headerlink" title="安装elasticsearch"></a>安装elasticsearch</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y elasticsearch</span><br></pre></td></tr></table></figure>

<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置文件都在 &#x2F;etc&#x2F;elasticsearch&#x2F; 目录下</span><br><span class="line">vim &#x2F;etc&#x2F;elasticsearch&#x2F;elasticsearch.yml</span><br><span class="line"></span><br><span class="line"># 集群名称</span><br><span class="line">cluster.name: my-application</span><br><span class="line"># 节点名称</span><br><span class="line">node.name: es</span><br><span class="line"># 数据文件与日志文件存放目录,没有的话新建</span><br><span class="line">path.data: &#x2F;data&#x2F;elasticsearch&#x2F;data</span><br><span class="line">path.logs: &#x2F;data&#x2F;elasticsearch&#x2F;logs</span><br><span class="line"># 网络设置</span><br><span class="line">network.host: 0.0.0.0</span><br><span class="line">http.port: 9200</span><br><span class="line"># 集群设置</span><br><span class="line">cluster.initial_master_nodes: [&quot;es&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 修改配置中目录的用户与用户组，不然无法启动</span><br><span class="line">chown -R elasticsearch:elasticsearch &#x2F;data&#x2F;elasticsearch&#x2F;</span><br></pre></td></tr></table></figure>

<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 刷新服务配置</span><br><span class="line">systemctl daemon-reload</span><br><span class="line"># 启动</span><br><span class="line">systemctl start elasticsearch.service</span><br><span class="line"># 开机自启</span><br><span class="line">systemctl enable elasticsearch.service</span><br><span class="line"># 查看状态</span><br><span class="line">systemctl status elasticsearch.service</span><br></pre></td></tr></table></figure>

<h1 id="安装kibana"><a href="#安装kibana" class="headerlink" title="安装kibana"></a>安装kibana</h1><h2 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y kibana</span><br></pre></td></tr></table></figure>

<h2 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置</span><br><span class="line">vim &#x2F;etc&#x2F;kibana&#x2F;kibana.yml</span><br><span class="line"></span><br><span class="line">server.host: &quot;0.0.0.0&quot;</span><br><span class="line"># 不要用 127.0.0.1，可能会提示 Kibana server is not ready yet</span><br><span class="line">elasticsearch.hosts: [&quot;http:&#x2F;&#x2F;localhost:9200&quot;]</span><br><span class="line">i18n.locale: &quot;zh-CN&quot;</span><br></pre></td></tr></table></figure>

<h2 id="启动-1"><a href="#启动-1" class="headerlink" title="启动"></a>启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 刷新服务配置</span><br><span class="line">systemctl daemon-reload</span><br><span class="line"># 开机自启</span><br><span class="line">systemctl enable kibana.service</span><br><span class="line"># 启动</span><br><span class="line">systemctl start kibana.service</span><br><span class="line"># 查看状态</span><br><span class="line">systemctl status kibana.service</span><br></pre></td></tr></table></figure>

<h1 id="安装logstash"><a href="#安装logstash" class="headerlink" title="安装logstash"></a>安装logstash</h1><h2 id="安装-2"><a href="#安装-2" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y logstash</span><br></pre></td></tr></table></figure>


<h2 id="启动-2"><a href="#启动-2" class="headerlink" title="启动"></a>启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 刷新服务配置</span><br><span class="line">systemctl daemon-reload</span><br><span class="line"># 开机自启</span><br><span class="line">systemctl enable kibana.service</span><br><span class="line"># 启动</span><br><span class="line">systemctl start kibana.service</span><br><span class="line"># centos 6 启动方式</span><br><span class="line">initctl start logstash</span><br><span class="line"># 查看状态</span><br><span class="line">systemctl status kibana.service</span><br></pre></td></tr></table></figure>

<h2 id="启动不了"><a href="#启动不了" class="headerlink" title="启动不了"></a>启动不了</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">问题1：查看&#x2F;var&#x2F;log&#x2F;messages日志，提示logstash: could not find java; set JAVA_HOME or ensure java is in PATH</span><br><span class="line"></span><br><span class="line">解决：创建软连接 </span><br><span class="line">ln -s &#x2F;usr&#x2F;local&#x2F;jdk1.8.0_77&#x2F;bin&#x2F;java &#x2F;usr&#x2F;bin&#x2F;java</span><br><span class="line"></span><br><span class="line">问题2：通过geoip解析ip时，没有geo_point</span><br><span class="line"></span><br><span class="line">解决：索引使用logstash开头</span><br></pre></td></tr></table></figure>

<h2 id="配置模板"><a href="#配置模板" class="headerlink" title="配置模板"></a>配置模板</h2><p>geoip 官方文档：<a href="https://www.elastic.co/cn/blog/geoip-in-the-elastic-stack">https://www.elastic.co/cn/blog/geoip-in-the-elastic-stack</a></p>
<p>原日志类型如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-09-30 10:33:12,432 [HttpTaskSync-1-thread-114],1569810792432,&#x2F;device,11,11,2,6.4.5.1,175.167.138.111,0,0</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Sample Logstash configuration for creating a simple</span><br><span class="line"># Beats -&gt; Logstash -&gt; Elasticsearch pipeline.</span><br><span class="line">input &#123;</span><br><span class="line">   file&#123;</span><br><span class="line">        path &#x3D;&gt; &quot;&#x2F;data&#x2F;apilogs&#x2F;req&#x2F;requestLog.log&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;</span><br><span class="line">	grok &#123;</span><br><span class="line">		match &#x3D;&gt; &#123;</span><br><span class="line">			&quot;message&quot; &#x3D;&gt; [&quot;(?&lt;date&gt;[0-9\-]+\s[0-9\:]+)\W+(?&lt;info&gt;[0-9]+)\s+(?&lt;info2&gt;\[.*\]+)\W+(?&lt;create_time&gt;[0-9]+)\W+(?&lt;url&gt;[\&#x2F;A-Za-z0-9\.]+)\W+(?&lt;user_time&gt;[0-9]+)\W+(?&lt;total_time&gt;[0-9]+)\W+(?&lt;terminal_type&gt;[0-9]+)\W+(?&lt;app_version&gt;[0-9\.]+)\W+(?&lt;remote_ip&gt;[0-9\.]+)\W+(?&lt;user_id&gt;[\d]+)\W++(?&lt;status&gt;[0-9]+)&quot;]</span><br><span class="line">        	&#125;</span><br><span class="line">		remove_field &#x3D;&gt; &quot;message&quot;</span><br><span class="line">		remove_field &#x3D;&gt; &quot;info&quot;</span><br><span class="line">		remove_field &#x3D;&gt; &quot;info2&quot;</span><br><span class="line">	&#125;	</span><br><span class="line">	if [url] &#x3D;&#x3D; &quot;getStoreSalesBarPicId&quot; &#123;</span><br><span class="line">                        drop &#123; &#125;</span><br><span class="line">        &#125;</span><br><span class="line">	geoip &#123;</span><br><span class="line">		source &#x3D;&gt; &quot;remote_ip&quot;</span><br><span class="line">		target &#x3D;&gt; &quot;geoip&quot;</span><br><span class="line">		database &#x3D;&gt; &quot;&#x2F;etc&#x2F;logstash&#x2F;GeoLite2-City_20190924&#x2F;GeoLite2-City.mmdb&quot;	</span><br><span class="line">	&#125;	</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts &#x3D;&gt; &quot;http:&#x2F;&#x2F;10.81.54.16:9200&quot;</span><br><span class="line">    index &#x3D;&gt; &quot;logstash-helper-requestlog&quot;</span><br><span class="line">    #user &#x3D;&gt; &quot;elastic&quot;</span><br><span class="line">    #password &#x3D;&gt; &quot;changeme&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>kibana</tag>
        <tag>logstash</tag>
      </tags>
  </entry>
  <entry>
    <title>备份etcd</title>
    <url>/posts/e64d06ef.html</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>整个k8s诸多组件几乎都是无状态的，所有的数据保存在etcd里，可以说etcd是整个k8s集群的数据库。</p>
<h1 id="具体方案"><a href="#具体方案" class="headerlink" title="具体方案"></a>具体方案</h1><h2 id="etcd-backup"><a href="#etcd-backup" class="headerlink" title="etcd-backup"></a>etcd-backup</h2><p><a href="https://github.com/giantswarm/etcd-backup">https://github.com/giantswarm/etcd-backup</a></p>
<p>将etcdctl 修改为线上实际的版本。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM alpine:3.8</span><br><span class="line"></span><br><span class="line">RUN apk add --no-cache curl</span><br><span class="line"></span><br><span class="line"># Get etcdctl</span><br><span class="line">ENV ETCD_VER&#x3D;v3.2.24</span><br><span class="line">RUN \</span><br><span class="line">  cd &#x2F;tmp &amp;&amp; \</span><br><span class="line">  curl -L https:&#x2F;&#x2F;storage.googleapis.com&#x2F;etcd&#x2F;$&#123;ETCD_VER&#125;&#x2F;etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz | \</span><br><span class="line">  tar xz -C &#x2F;usr&#x2F;local&#x2F;bin --strip-components&#x3D;1</span><br><span class="line"></span><br><span class="line">COPY .&#x2F;etcd-backup &#x2F;</span><br><span class="line">ENTRYPOINT [&quot;&#x2F;etcd-backup&quot;]</span><br><span class="line">CMD [&quot;-h&quot;]</span><br></pre></td></tr></table></figure>

<h2 id="k8s"><a href="#k8s" class="headerlink" title="k8s"></a>k8s</h2><p>选择k8s中的cronjob比较合适，备份策略是每三小时备份一次。</p>
<p>cronjob.yaml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: batch&#x2F;v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: etcd-backup</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  schedule: &quot;0 *&#x2F;4 * * *&quot;</span><br><span class="line">  successfulJobsHistoryLimit: 2</span><br><span class="line">  failedJobsHistoryLimit: 2</span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      # Job timeout</span><br><span class="line">      activeDeadlineSeconds: 300</span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          tolerations:</span><br><span class="line">          # Tolerate master taint</span><br><span class="line">          - key: node-role.kubernetes.io&#x2F;master</span><br><span class="line">            operator: Exists</span><br><span class="line">            effect: NoSchedule</span><br><span class="line">          # Container creates etcd backups.</span><br><span class="line">          # Run container in host network mode on G8s masters</span><br><span class="line">          # to be able to use 127.0.0.1 as etcd address.</span><br><span class="line">          # For etcd v2 backups container should have access</span><br><span class="line">          # to etcd data directory. To achive that,</span><br><span class="line">          # mount &#x2F;var&#x2F;lib&#x2F;etcd3 as a volume.</span><br><span class="line">          nodeSelector:</span><br><span class="line">            node-role.kubernetes.io&#x2F;master: &quot;&quot;</span><br><span class="line">          containers:</span><br><span class="line">          - name: etcd-backup</span><br><span class="line">            image: iyacontrol&#x2F;etcd-backup:0.1</span><br><span class="line">            args:</span><br><span class="line">            # backup guest clusters only on production instalations</span><br><span class="line">            # testing installation can have many broken guest clusters</span><br><span class="line">            - -prefix&#x3D;k8s-prod-1</span><br><span class="line">            - -etcd-v2-datadir&#x3D;&#x2F;var&#x2F;lib&#x2F;etcd</span><br><span class="line">            - -etcd-v3-endpoints&#x3D;https:&#x2F;&#x2F;172.xx.xx.221:2379,https:&#x2F;&#x2F;172.xx.xx.83:2379,https:&#x2F;&#x2F;172.xx.xx.246:2379</span><br><span class="line">            - -etcd-v3-cacert&#x3D;&#x2F;certs&#x2F;ca.crt</span><br><span class="line">            - -etcd-v3-cert&#x3D;&#x2F;certs&#x2F;server.crt</span><br><span class="line">            - -etcd-v3-key&#x3D;&#x2F;certs&#x2F;server.key</span><br><span class="line">            - -aws-s3-bucket&#x3D;mybucket</span><br><span class="line">            - -aws-s3-region&#x3D;us-east-1</span><br><span class="line">            volumeMounts:</span><br><span class="line">            - mountPath: &#x2F;var&#x2F;lib&#x2F;etcd</span><br><span class="line">              name: etcd-datadir</span><br><span class="line">            - mountPath: &#x2F;certs</span><br><span class="line">              name: etcd-certs</span><br><span class="line">            env:</span><br><span class="line">              - name: ETCDBACKUP_AWS_ACCESS_KEY</span><br><span class="line">                valueFrom:</span><br><span class="line">                  secretKeyRef:</span><br><span class="line">                    name: etcd-backup</span><br><span class="line">                    key: ETCDBACKUP_AWS_ACCESS_KEY</span><br><span class="line">              - name: ETCDBACKUP_AWS_SECRET_KEY</span><br><span class="line">                valueFrom:</span><br><span class="line">                  secretKeyRef:</span><br><span class="line">                    name: etcd-backup</span><br><span class="line">                    key: ETCDBACKUP_AWS_SECRET_KEY</span><br><span class="line">              - name: ETCDBACKUP_PASSPHRASE</span><br><span class="line">                valueFrom:</span><br><span class="line">                  secretKeyRef:</span><br><span class="line">                    name: etcd-backup</span><br><span class="line">                    key: ETCDBACKUP_PASSPHRASE</span><br><span class="line">          volumes:</span><br><span class="line">          - name: etcd-datadir</span><br><span class="line">            hostPath:</span><br><span class="line">              path: &#x2F;var&#x2F;lib&#x2F;etcd</span><br><span class="line">          - name: etcd-certs</span><br><span class="line">            hostPath:</span><br><span class="line">              path: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;</span><br><span class="line">          # Do not restart pod, job takes care on restarting failed pod.</span><br><span class="line">          restartPolicy: Never</span><br><span class="line">          hostNetwork: true</span><br></pre></td></tr></table></figure>

<p><strong>注意：容忍 和 nodeselector配合，让pod调度到master节点上。</strong></p>
<p>secret.yaml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: etcd-backup</span><br><span class="line">  namespace: kube-system</span><br><span class="line">type: Opaque</span><br><span class="line">data:</span><br><span class="line">  ETCDBACKUP_AWS_ACCESS_KEY: QUtJTI0TktCT0xQRlEK</span><br><span class="line">  ETCDBACKUP_AWS_SECRET_KEY: aXJ6eThjQnM2MVRaSkdGMGxDeHhoeFZNUDU4ZGRNbgo&#x3D;</span><br><span class="line">  ETCDBACKUP_PASSPHRASE: &quot;&quot;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>使用Ceph RBD持久化存储部署K8S EFK</title>
    <url>/posts/cb5eee23.html</url>
    <content><![CDATA[<h1 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h1><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul>
<li><p>Ceph版本：v14 (nautilus)</p>
</li>
<li><p>k8s:1.17.x</p>
</li>
<li><p>Cenos 7.6</p>
<a id="more"></a>
<h1 id="Ceph"><a href="#Ceph" class="headerlink" title="Ceph"></a>Ceph</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#创建存储池</span><br><span class="line"># ceph osd pool create k8s 128 128</span><br><span class="line">#查看存储池</span><br><span class="line"># ceph osd pool ls</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h1 id="Ceph-创建账号"><a href="#Ceph-创建账号" class="headerlink" title="Ceph 创建账号"></a>Ceph 创建账号</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">可以直接使用Ceph的admin账号，当然生产环境中还是要根据不同功能客户端分配不同的账号：</span><br><span class="line">ceph auth get-or-create client.k8s mon &#39;allow r&#39; osd &#39;allow rwx pool&#x3D;k8s&#39; -o ceph.client.k8s.keyring</span><br><span class="line"></span><br><span class="line">获取账号的密钥：</span><br><span class="line">ceph auth get-key client.admin | base64</span><br></pre></td></tr></table></figure>

<h1 id="K8S-使用-Ceph-RBD存储"><a href="#K8S-使用-Ceph-RBD存储" class="headerlink" title="K8S 使用 Ceph RBD存储"></a>K8S 使用 Ceph RBD存储</h1><h2 id="创建-namespace"><a href="#创建-namespace" class="headerlink" title="创建 namespace"></a>创建 namespace</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;  namespace.yaml  &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: efk</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f namespace.yaml</span><br></pre></td></tr></table></figure>

<h2 id="为存储类提供secret"><a href="#为存储类提供secret" class="headerlink" title="为存储类提供secret"></a>为存储类提供secret</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; ceph-efk-secret.yaml &lt;&lt; EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-admin-secret</span><br><span class="line">  namespace: efk </span><br><span class="line">data:</span><br><span class="line">  key: QVFBRkwyOWU4WUQ3SlJBQVhtajVVRnJ0TUZVVnZaanVMYkRuT3c9PQ&#x3D;&#x3D;</span><br><span class="line">type: kubernetes.io&#x2F;rbd</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-efk-secret</span><br><span class="line">  namespace: efk </span><br><span class="line">data:</span><br><span class="line">  key: QVFBRkwyOWU4WUQ3SlJBQVhtajVVRnJ0TUZVVnZaanVMYkRuT3c9PQ&#x3D;&#x3D;</span><br><span class="line">type: kubernetes.io&#x2F;rbd</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f ceph-efk-secret.yaml</span><br></pre></td></tr></table></figure>


<h2 id="创建动态RBD-StorageClass"><a href="#创建动态RBD-StorageClass" class="headerlink" title="创建动态RBD StorageClass"></a>创建动态RBD StorageClass</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; ceph-efk-storageclass.yaml &lt;&lt; EOF</span><br><span class="line">apiVersion: storage.k8s.io&#x2F;v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-efk</span><br><span class="line">  namespace: efk</span><br><span class="line">  annotations:</span><br><span class="line">    storageclass.kubernetes.io&#x2F;is-default-class: &quot;false&quot;</span><br><span class="line">provisioner: ceph.com&#x2F;rbd</span><br><span class="line">reclaimPolicy: Retain</span><br><span class="line">parameters:</span><br><span class="line">  monitors: 192.168.1.65:6789,192.168.1.66:6789,192.168.1.67:6789</span><br><span class="line">  adminId: admin</span><br><span class="line">  adminSecretName: ceph-admin-secret</span><br><span class="line">  adminSecretNamespace: efk</span><br><span class="line">  pool: k8s</span><br><span class="line">  fsType: ext4</span><br><span class="line">  userId: admin</span><br><span class="line">  userSecretName: ceph-efk-secret</span><br><span class="line">  imageFormat: &quot;2&quot;</span><br><span class="line">  imageFeatures: &quot;layering&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f ceph-efk-storageclass.yaml</span><br></pre></td></tr></table></figure>

<h2 id="为controller-manager提供rbd命令"><a href="#为controller-manager提供rbd命令" class="headerlink" title="为controller-manager提供rbd命令"></a>为controller-manager提供rbd命令</h2><p>使用StorageClass动态创建PV时，controller-manager会自动在Ceph上创建image，所以我们要为其准备好rbd命令。<br>如果集群是用kubeadm部署的，由于controller-manager官方镜像中没有rbd命令，所以我们要导入外部配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; rbd-provisioner.yaml &lt;&lt; EOF</span><br><span class="line">kind: ClusterRole </span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1 </span><br><span class="line">metadata: </span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: efk </span><br><span class="line">rules: </span><br><span class="line">  - apiGroups: [&quot;&quot;] </span><br><span class="line">    resources: [&quot;persistentvolumes&quot;] </span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] </span><br><span class="line">  - apiGroups: [&quot;&quot;] </span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;] </span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] </span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;] </span><br><span class="line">    resources: [&quot;storageclasses&quot;] </span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] </span><br><span class="line">  - apiGroups: [&quot;&quot;] </span><br><span class="line">    resources: [&quot;events&quot;] </span><br><span class="line">    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] </span><br><span class="line">  - apiGroups: [&quot;&quot;] </span><br><span class="line">    resources: [&quot;services&quot;] </span><br><span class="line">    resourceNames: [&quot;kube-dns&quot;,&quot;coredns&quot;] </span><br><span class="line">    verbs: [&quot;list&quot;, &quot;get&quot;] </span><br><span class="line">--- </span><br><span class="line">kind: ClusterRoleBinding </span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1 </span><br><span class="line">metadata: </span><br><span class="line">  name: rbd-provisioner </span><br><span class="line">  namespace: efk</span><br><span class="line">subjects: </span><br><span class="line">  - kind: ServiceAccount </span><br><span class="line">    name: rbd-provisioner </span><br><span class="line">    namespace: default </span><br><span class="line">roleRef: </span><br><span class="line">  kind: ClusterRole </span><br><span class="line">  name: rbd-provisioner </span><br><span class="line">  apiGroup: rbac.authorization.k8s.io </span><br><span class="line">--- </span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1 </span><br><span class="line">kind: Role </span><br><span class="line">metadata: </span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: efk </span><br><span class="line">rules: </span><br><span class="line">- apiGroups: [&quot;&quot;] </span><br><span class="line">  resources: [&quot;secrets&quot;] </span><br><span class="line">  verbs: [&quot;get&quot;] </span><br><span class="line">- apiGroups: [&quot;&quot;] </span><br><span class="line">  resources: [&quot;endpoints&quot;] </span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] </span><br><span class="line">--- </span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1 </span><br><span class="line">kind: RoleBinding </span><br><span class="line">metadata: </span><br><span class="line">  name: rbd-provisioner </span><br><span class="line">  namespace: efk</span><br><span class="line">roleRef: </span><br><span class="line">  apiGroup: rbac.authorization.k8s.io </span><br><span class="line">  kind: Role </span><br><span class="line">  name: rbd-provisioner </span><br><span class="line">subjects: </span><br><span class="line">  - kind: ServiceAccount </span><br><span class="line">    name: rbd-provisioner </span><br><span class="line">    namespace: default </span><br><span class="line">--- </span><br><span class="line">apiVersion: extensions&#x2F;v1beta1 </span><br><span class="line">kind: Deployment </span><br><span class="line">metadata: </span><br><span class="line">  name: rbd-provisioner </span><br><span class="line">  namespace: efk</span><br><span class="line">spec: </span><br><span class="line">  replicas: 1 </span><br><span class="line">  strategy: </span><br><span class="line">    type: Recreate </span><br><span class="line">  template: </span><br><span class="line">    metadata: </span><br><span class="line">      labels: </span><br><span class="line">        app: rbd-provisioner </span><br><span class="line">    spec: </span><br><span class="line">      containers: </span><br><span class="line">      - name: rbd-provisioner </span><br><span class="line">        image: quay.io&#x2F;external_storage&#x2F;rbd-provisioner:latest </span><br><span class="line">        env: </span><br><span class="line">        - name: PROVISIONER_NAME </span><br><span class="line">          value: ceph.com&#x2F;rbd </span><br><span class="line">      serviceAccount: rbd-provisioner </span><br><span class="line">--- </span><br><span class="line">apiVersion: v1 </span><br><span class="line">kind: ServiceAccount </span><br><span class="line">metadata: </span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: efk</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f rbd-provisioner.yaml</span><br></pre></td></tr></table></figure>

<h2 id="为kubelet提供rbd命令"><a href="#为kubelet提供rbd命令" class="headerlink" title="为kubelet提供rbd命令"></a>为kubelet提供rbd命令</h2><p>创建pod时，kubelet需要使用rbd命令去检测和挂载pv对应的ceph image，所以要在所有的worker节点安装ceph客户端ceph-common-14.2.8。<br>并且需要复制ceph.client.admin.keyring、ceph.conf文件到/etc/ceph目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install ceph-common -y</span><br><span class="line"></span><br><span class="line">yum install epel-release -y</span><br></pre></td></tr></table></figure>

<p>无安装epel-release否则会提示一下错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">---&gt; Package python-backports.x86_64 0:1.0-8.el7 will be installed</span><br><span class="line">--&gt; Finished Dependency Resolution</span><br><span class="line">Error: Package: 2:librgw2-14.2.8-0.el7.x86_64 (Ceph)</span><br><span class="line">           Requires: liblttng-ust.so.0()(64bit)</span><br><span class="line">Error: Package: 2:ceph-common-14.2.8-0.el7.x86_64 (Ceph)</span><br><span class="line">           Requires: libleveldb.so.1()(64bit)</span><br><span class="line">Error: Package: 2:ceph-common-14.2.8-0.el7.x86_64 (Ceph)</span><br><span class="line">           Requires: libbabeltrace.so.1()(64bit)</span><br><span class="line">Error: Package: 2:ceph-common-14.2.8-0.el7.x86_64 (Ceph)</span><br><span class="line">           Requires: liboath.so.0(LIBOATH_1.2.0)(64bit)</span><br><span class="line">Error: Package: 2:ceph-common-14.2.8-0.el7.x86_64 (Ceph)</span><br><span class="line">           Requires: libbabeltrace-ctf.so.1()(64bit)</span><br><span class="line">Error: Package: 2:librados2-14.2.8-0.el7.x86_64 (Ceph)</span><br><span class="line">           Requires: liblttng-ust.so.0()(64bit)</span><br><span class="line">Error: Package: 2:librbd1-14.2.8-0.el7.x86_64 (Ceph)</span><br><span class="line">           Requires: liblttng-ust.so.0()(64bit)</span><br><span class="line">Error: Package: 2:ceph-common-14.2.8-0.el7.x86_64 (Ceph)</span><br><span class="line">           Requires: liboath.so.0()(64bit)</span><br><span class="line">Error: Package: 2:ceph-common-14.2.8-0.el7.x86_64 (Ceph)</span><br><span class="line">           Requires: liboath.so.0(LIBOATH_1.10.0)(64bit)</span><br><span class="line">Error: Package: 2:librgw2-14.2.8-0.el7.x86_64 (Ceph)</span><br><span class="line">           Requires: liboath.so.0()(64bit)</span><br><span class="line"> You could try using --skip-broken to work around the problem</span><br><span class="line"> You could try running: rpm -Va --nofiles --nodigest</span><br></pre></td></tr></table></figure>

<h2 id="创建Elasticsearch"><a href="#创建Elasticsearch" class="headerlink" title="创建Elasticsearch"></a>创建Elasticsearch</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; elasticsearch.yaml &lt;&lt; EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  namespace: efk</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: elasticsearch-logging</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">    kubernetes.io&#x2F;name: &quot;Elasticsearch&quot;</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 9200</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: db</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: elasticsearch-logging</span><br><span class="line">---</span><br><span class="line"># RBAC authn and authz</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  namespace: efk</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: elasticsearch-logging</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: elasticsearch-logging</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - &quot;services&quot;</span><br><span class="line">  - &quot;namespaces&quot;</span><br><span class="line">  - &quot;endpoints&quot;</span><br><span class="line">  verbs:</span><br><span class="line">  - &quot;get&quot;</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: efk</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: elasticsearch-logging</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  namespace: efk</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line">---</span><br><span class="line"># Elasticsearch deployment itself</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  namespace: efk</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: elasticsearch-logging</span><br><span class="line">    version: v7.4.2</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  serviceName: elasticsearch-logging</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: elasticsearch-logging</span><br><span class="line">      version: v7.4.2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: elasticsearch-logging</span><br><span class="line">        version: v7.4.2</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: elasticsearch-logging</span><br><span class="line">      containers:</span><br><span class="line">      - image: quay.io&#x2F;fluentd_elasticsearch&#x2F;elasticsearch:v7.4.2 </span><br><span class="line">        name: elasticsearch-logging</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        resources:</span><br><span class="line">          # need more cpu upon initialization, therefore burstable class</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 3Gi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 3Gi</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9200</span><br><span class="line">          name: db</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 9300</span><br><span class="line">          name: transport</span><br><span class="line">          protocol: TCP</span><br><span class="line">        #livenessProbe:</span><br><span class="line">        #  tcpSocket:</span><br><span class="line">        #    port: transport</span><br><span class="line">        #  initialDelaySeconds: 5</span><br><span class="line">        #  timeoutSeconds: 10</span><br><span class="line">        #readinessProbe:</span><br><span class="line">        #  tcpSocket:</span><br><span class="line">        #    port: transport</span><br><span class="line">        #  initialDelaySeconds: 5</span><br><span class="line">        #  timeoutSeconds: 10</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: elasticsearch-logging</span><br><span class="line">          mountPath: &#x2F;data</span><br><span class="line">        env:</span><br><span class="line">        - name: &quot;NAMESPACE&quot;</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.namespace</span><br><span class="line">      volumes:</span><br><span class="line">      - name: elasticsearch-logging</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: ceph-elasticsearch</span><br><span class="line">        #emptyDir: &#123;&#125;</span><br><span class="line">      # Elasticsearch requires vm.max_map_count to be at least 262144.</span><br><span class="line">      # If your OS already sets up this number to a higher value, feel free</span><br><span class="line">      # to remove this init container.</span><br><span class="line">      initContainers:</span><br><span class="line">      - image: alpine:3.6</span><br><span class="line">        command: [&quot;&#x2F;sbin&#x2F;sysctl&quot;, &quot;-w&quot;, &quot;vm.max_map_count&#x3D;262144&quot;]</span><br><span class="line">        name: elasticsearch-logging-init</span><br><span class="line">        securityContext:</span><br><span class="line">          privileged: true</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-elasticsearch</span><br><span class="line">  namespace: efk</span><br><span class="line">  labels:</span><br><span class="line">    app: elasticsearch</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: ceph-efk</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 10Gi</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f elasticsearch.yaml</span><br></pre></td></tr></table></figure>

<h2 id="创建Kibana"><a href="#创建Kibana" class="headerlink" title="创建Kibana"></a>创建Kibana</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; kibana.yaml &lt;&lt; EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kibana-logging</span><br><span class="line">  namespace: efk</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kibana-logging</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">    kubernetes.io&#x2F;name: &quot;Kibana&quot;</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 5601</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: ui</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kibana-logging</span><br><span class="line">---</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: kibana-logging</span><br><span class="line">  namespace: efk</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kibana-logging</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kibana-logging</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kibana-logging</span><br><span class="line">      annotations:</span><br><span class="line">        seccomp.security.alpha.kubernetes.io&#x2F;pod: &#39;docker&#x2F;default&#39;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: kibana-logging</span><br><span class="line">        image: docker.elastic.co&#x2F;kibana&#x2F;kibana-oss:7.2.0</span><br><span class="line">        resources:</span><br><span class="line">          # need more cpu upon initialization, therefore burstable class</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">        env:</span><br><span class="line">          - name: ELASTICSEARCH_HOSTS</span><br><span class="line">            value: http:&#x2F;&#x2F;elasticsearch-logging:9200</span><br><span class="line">          - name: SERVER_NAME</span><br><span class="line">            value: kibana-logging</span><br><span class="line">          #- name: SERVER_BASEPATH</span><br><span class="line">          #  value: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;efk&#x2F;services&#x2F;kibana-logging&#x2F;proxy</span><br><span class="line">          - name: SERVER_REWRITEBASEPATH</span><br><span class="line">            value: &quot;false&quot;</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 5601</span><br><span class="line">          name: ui</span><br><span class="line">          protocol: TCP</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;api&#x2F;status</span><br><span class="line">            port: ui</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          timeoutSeconds: 10</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;api&#x2F;status</span><br><span class="line">            port: ui</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          timeoutSeconds: 10</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f kibana.yaml</span><br></pre></td></tr></table></figure>

<h2 id="创建Fluentd-ConfigMap"><a href="#创建Fluentd-ConfigMap" class="headerlink" title="创建Fluentd ConfigMap"></a>创建Fluentd ConfigMap</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; fluentd-es-config.yaml &lt;&lt; EOF</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-es-config-v0.2.0</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">data:</span><br><span class="line">  system.conf: |-</span><br><span class="line">    &lt;system&gt;</span><br><span class="line">      root_dir &#x2F;tmp&#x2F;fluentd-buffers&#x2F;</span><br><span class="line">    &lt;&#x2F;system&gt;</span><br><span class="line"></span><br><span class="line">  containers.input.conf: |-</span><br><span class="line">    # This configuration file for Fluentd &#x2F; td-agent is used</span><br><span class="line">    # to watch changes to Docker log files. The kubelet creates symlinks that</span><br><span class="line">    # capture the pod name, namespace, container name &amp; Docker container ID</span><br><span class="line">    # to the docker logs for pods in the &#x2F;var&#x2F;log&#x2F;containers directory on the host.</span><br><span class="line">    # If running this fluentd configuration in a Docker container, the &#x2F;var&#x2F;log</span><br><span class="line">    # directory should be mounted in the container.</span><br><span class="line">    #</span><br><span class="line">    # These logs are then submitted to Elasticsearch which assumes the</span><br><span class="line">    # installation of the fluent-plugin-elasticsearch &amp; the</span><br><span class="line">    # fluent-plugin-kubernetes_metadata_filter plugins.</span><br><span class="line">    # See https:&#x2F;&#x2F;github.com&#x2F;uken&#x2F;fluent-plugin-elasticsearch &amp;</span><br><span class="line">    # https:&#x2F;&#x2F;github.com&#x2F;fabric8io&#x2F;fluent-plugin-kubernetes_metadata_filter for</span><br><span class="line">    # more information about the plugins.</span><br><span class="line">    #</span><br><span class="line">    # Example</span><br><span class="line">    # &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">    # A line in the Docker log file might look like this JSON:</span><br><span class="line">    #</span><br><span class="line">    # &#123;&quot;log&quot;:&quot;2014&#x2F;09&#x2F;25 21:15:03 Got request with path wombat\n&quot;,</span><br><span class="line">    #  &quot;stream&quot;:&quot;stderr&quot;,</span><br><span class="line">    #   &quot;time&quot;:&quot;2014-09-25T21:15:03.499185026Z&quot;&#125;</span><br><span class="line">    #</span><br><span class="line">    # The time_format specification below makes sure we properly</span><br><span class="line">    # parse the time format produced by Docker. This will be</span><br><span class="line">    # submitted to Elasticsearch and should appear like:</span><br><span class="line">    # $ curl &#39;http:&#x2F;&#x2F;elasticsearch-logging:9200&#x2F;_search?pretty&#39;</span><br><span class="line">    # ...</span><br><span class="line">    # &#123;</span><br><span class="line">    #      &quot;_index&quot; : &quot;logstash-2014.09.25&quot;,</span><br><span class="line">    #      &quot;_type&quot; : &quot;fluentd&quot;,</span><br><span class="line">    #      &quot;_id&quot; : &quot;VBrbor2QTuGpsQyTCdfzqA&quot;,</span><br><span class="line">    #      &quot;_score&quot; : 1.0,</span><br><span class="line">    #      &quot;_source&quot;:&#123;&quot;log&quot;:&quot;2014&#x2F;09&#x2F;25 22:45:50 Got request with path wombat\n&quot;,</span><br><span class="line">    #                 &quot;stream&quot;:&quot;stderr&quot;,&quot;tag&quot;:&quot;docker.container.all&quot;,</span><br><span class="line">    #                 &quot;@timestamp&quot;:&quot;2014-09-25T22:45:50+00:00&quot;&#125;</span><br><span class="line">    #    &#125;,</span><br><span class="line">    # ...</span><br><span class="line">    #</span><br><span class="line">    # The Kubernetes fluentd plugin is used to write the Kubernetes metadata to the log</span><br><span class="line">    # record &amp; add labels to the log record if properly configured. This enables users</span><br><span class="line">    # to filter &amp; search logs on any metadata.</span><br><span class="line">    # For example a Docker container&#39;s logs might be in the directory:</span><br><span class="line">    #</span><br><span class="line">    #  &#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers&#x2F;997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b</span><br><span class="line">    #</span><br><span class="line">    # and in the file:</span><br><span class="line">    #</span><br><span class="line">    #  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log</span><br><span class="line">    #</span><br><span class="line">    # where 997599971ee6... is the Docker ID of the running container.</span><br><span class="line">    # The Kubernetes kubelet makes a symbolic link to this file on the host machine</span><br><span class="line">    # in the &#x2F;var&#x2F;log&#x2F;containers directory which includes the pod name and the Kubernetes</span><br><span class="line">    # container name:</span><br><span class="line">    #</span><br><span class="line">    #    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log</span><br><span class="line">    #    -&gt;</span><br><span class="line">    #    &#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers&#x2F;997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b&#x2F;997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log</span><br><span class="line">    #</span><br><span class="line">    # The &#x2F;var&#x2F;log directory on the host is mapped to the &#x2F;var&#x2F;log directory in the container</span><br><span class="line">    # running this instance of Fluentd and we end up collecting the file:</span><br><span class="line">    #</span><br><span class="line">    #   &#x2F;var&#x2F;log&#x2F;containers&#x2F;synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log</span><br><span class="line">    #</span><br><span class="line">    # This results in the tag:</span><br><span class="line">    #</span><br><span class="line">    #  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log</span><br><span class="line">    #</span><br><span class="line">    # The Kubernetes fluentd plugin is used to extract the namespace, pod name &amp; container name</span><br><span class="line">    # which are added to the log message as a kubernetes field object &amp; the Docker container ID</span><br><span class="line">    # is also added under the docker field object.</span><br><span class="line">    # The final tag is:</span><br><span class="line">    #</span><br><span class="line">    #   kubernetes.var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log</span><br><span class="line">    #</span><br><span class="line">    # And the final log record look like:</span><br><span class="line">    #</span><br><span class="line">    # &#123;</span><br><span class="line">    #   &quot;log&quot;:&quot;2014&#x2F;09&#x2F;25 21:15:03 Got request with path wombat\n&quot;,</span><br><span class="line">    #   &quot;stream&quot;:&quot;stderr&quot;,</span><br><span class="line">    #   &quot;time&quot;:&quot;2014-09-25T21:15:03.499185026Z&quot;,</span><br><span class="line">    #   &quot;kubernetes&quot;: &#123;</span><br><span class="line">    #     &quot;namespace&quot;: &quot;default&quot;,</span><br><span class="line">    #     &quot;pod_name&quot;: &quot;synthetic-logger-0.25lps-pod&quot;,</span><br><span class="line">    #     &quot;container_name&quot;: &quot;synth-lgr&quot;</span><br><span class="line">    #   &#125;,</span><br><span class="line">    #   &quot;docker&quot;: &#123;</span><br><span class="line">    #     &quot;container_id&quot;: &quot;997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b&quot;</span><br><span class="line">    #   &#125;</span><br><span class="line">    # &#125;</span><br><span class="line">    #</span><br><span class="line">    # This makes it easier for users to search for logs by pod name or by</span><br><span class="line">    # the name of the Kubernetes container regardless of how many times the</span><br><span class="line">    # Kubernetes pod has been restarted (resulting in a several Docker container IDs).</span><br><span class="line"></span><br><span class="line">    # Json Log Example:</span><br><span class="line">    # &#123;&quot;log&quot;:&quot;[info:2016-02-16T16:04:05.930-08:00] Some log text here\n&quot;,&quot;stream&quot;:&quot;stdout&quot;,&quot;time&quot;:&quot;2016-02-17T00:04:05.931087621Z&quot;&#125;</span><br><span class="line">    # CRI Log Example:</span><br><span class="line">    # 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id fluentd-containers.log</span><br><span class="line">      @type tail</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;containers&#x2F;*.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-containers.log.pos</span><br><span class="line">      tag raw.kubernetes.*</span><br><span class="line">      read_from_head true</span><br><span class="line">      &lt;parse&gt;</span><br><span class="line">        @type multi_format</span><br><span class="line">        &lt;pattern&gt;</span><br><span class="line">          format json</span><br><span class="line">          time_key time</span><br><span class="line">          time_format %Y-%m-%dT%H:%M:%S.%NZ</span><br><span class="line">        &lt;&#x2F;pattern&gt;</span><br><span class="line">        &lt;pattern&gt;</span><br><span class="line">          format &#x2F;^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr) [^ ]* (?&lt;log&gt;.*)$&#x2F;</span><br><span class="line">          time_format %Y-%m-%dT%H:%M:%S.%N%:z</span><br><span class="line">        &lt;&#x2F;pattern&gt;</span><br><span class="line">      &lt;&#x2F;parse&gt;</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Detect exceptions in the log output and forward them as one log entry.</span><br><span class="line">    &lt;match raw.kubernetes.**&gt;</span><br><span class="line">      @id raw.kubernetes</span><br><span class="line">      @type detect_exceptions</span><br><span class="line">      remove_tag_prefix raw</span><br><span class="line">      message log</span><br><span class="line">      stream stream</span><br><span class="line">      multiline_flush_interval 5</span><br><span class="line">      max_bytes 500000</span><br><span class="line">      max_lines 1000</span><br><span class="line">    &lt;&#x2F;match&gt;</span><br><span class="line"></span><br><span class="line">    # Concatenate multi-line logs</span><br><span class="line">    &lt;filter **&gt;</span><br><span class="line">      @id filter_concat</span><br><span class="line">      @type concat</span><br><span class="line">      key message</span><br><span class="line">      multiline_end_regexp &#x2F;\n$&#x2F;</span><br><span class="line">      separator &quot;&quot;</span><br><span class="line">    &lt;&#x2F;filter&gt;</span><br><span class="line"></span><br><span class="line">    # Enriches records with Kubernetes metadata</span><br><span class="line">    &lt;filter kubernetes.**&gt;</span><br><span class="line">      @id filter_kubernetes_metadata</span><br><span class="line">      @type kubernetes_metadata</span><br><span class="line">    &lt;&#x2F;filter&gt;</span><br><span class="line"></span><br><span class="line">    # Fixes json fields in Elasticsearch</span><br><span class="line">    &lt;filter kubernetes.**&gt;</span><br><span class="line">      @id filter_parser</span><br><span class="line">      @type parser</span><br><span class="line">      key_name log</span><br><span class="line">      reserve_data true</span><br><span class="line">      remove_key_name_field true</span><br><span class="line">      &lt;parse&gt;</span><br><span class="line">        @type multi_format</span><br><span class="line">        &lt;pattern&gt;</span><br><span class="line">          format json</span><br><span class="line">        &lt;&#x2F;pattern&gt;</span><br><span class="line">        &lt;pattern&gt;</span><br><span class="line">          format none</span><br><span class="line">        &lt;&#x2F;pattern&gt;</span><br><span class="line">      &lt;&#x2F;parse&gt;</span><br><span class="line">    &lt;&#x2F;filter&gt;</span><br><span class="line"></span><br><span class="line">  system.input.conf: |-</span><br><span class="line">    # Example:</span><br><span class="line">    # 2015-12-21 23:17:22,066 [salt.state       ][INFO    ] Completed state [net.ipv4.ip_forward] at time 23:17:22.066081</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id minion</span><br><span class="line">      @type tail</span><br><span class="line">      format &#x2F;^(?&lt;time&gt;[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?&lt;severity&gt;[^ \]]*) *\] (?&lt;message&gt;.*)$&#x2F;</span><br><span class="line">      time_format %Y-%m-%d %H:%M:%S</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;salt&#x2F;minion</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;salt.pos</span><br><span class="line">      tag salt</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Example:</span><br><span class="line">    # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script &#x2F;var&#x2F;run&#x2F;google.startup.script</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id startupscript.log</span><br><span class="line">      @type tail</span><br><span class="line">      format syslog</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;startupscript.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-startupscript.log.pos</span><br><span class="line">      tag startupscript</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Examples:</span><br><span class="line">    # time&#x3D;&quot;2016-02-04T06:51:03.053580605Z&quot; level&#x3D;info msg&#x3D;&quot;GET &#x2F;containers&#x2F;json&quot;</span><br><span class="line">    # time&#x3D;&quot;2016-02-04T07:53:57.505612354Z&quot; level&#x3D;error msg&#x3D;&quot;HTTP Error&quot; err&#x3D;&quot;No such image: -f&quot; statusCode&#x3D;404</span><br><span class="line">    # TODO(random-liu): Remove this after cri container runtime rolls out.</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id docker.log</span><br><span class="line">      @type tail</span><br><span class="line">      format &#x2F;^time&#x3D;&quot;(?&lt;time&gt;[^&quot;]*)&quot; level&#x3D;(?&lt;severity&gt;[^ ]*) msg&#x3D;&quot;(?&lt;message&gt;[^&quot;]*)&quot;( err&#x3D;&quot;(?&lt;error&gt;[^&quot;]*)&quot;)?( statusCode&#x3D;($&lt;status_code&gt;\d+))?&#x2F;</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;docker.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-docker.log.pos</span><br><span class="line">      tag docker</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Example:</span><br><span class="line">    # 2016&#x2F;02&#x2F;04 06:52:38 filePurge: successfully removed file &#x2F;var&#x2F;etcd&#x2F;data&#x2F;member&#x2F;wal&#x2F;00000000000006d0-00000000010a23d1.wal</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id etcd.log</span><br><span class="line">      @type tail</span><br><span class="line">      # Not parsing this, because it doesn&#39;t have anything particularly useful to</span><br><span class="line">      # parse out of it (like severities).</span><br><span class="line">      format none</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;etcd.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-etcd.log.pos</span><br><span class="line">      tag etcd</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Multi-line parsing is required for all the kube logs because very large log</span><br><span class="line">    # statements, such as those that include entire object bodies, get split into</span><br><span class="line">    # multiple lines by glog.</span><br><span class="line"></span><br><span class="line">    # Example:</span><br><span class="line">    # I0204 07:32:30.020537    3368 server.go:1048] POST &#x2F;stats&#x2F;container&#x2F;: (13.972191ms) 200 [[Go-http-client&#x2F;1.1] 10.244.1.3:40537]</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id kubelet.log</span><br><span class="line">      @type tail</span><br><span class="line">      format multiline</span><br><span class="line">      multiline_flush_interval 5s</span><br><span class="line">      format_firstline &#x2F;^\w\d&#123;4&#125;&#x2F;</span><br><span class="line">      format1 &#x2F;^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)&#x2F;</span><br><span class="line">      time_format %m%d %H:%M:%S.%N</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;kubelet.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-kubelet.log.pos</span><br><span class="line">      tag kubelet</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Example:</span><br><span class="line">    # I1118 21:26:53.975789       6 proxier.go:1096] Port &quot;nodePort for kube-system&#x2F;default-http-backend:http&quot; (:31429&#x2F;tcp) was open before and is still needed</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id kube-proxy.log</span><br><span class="line">      @type tail</span><br><span class="line">      format multiline</span><br><span class="line">      multiline_flush_interval 5s</span><br><span class="line">      format_firstline &#x2F;^\w\d&#123;4&#125;&#x2F;</span><br><span class="line">      format1 &#x2F;^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)&#x2F;</span><br><span class="line">      time_format %m%d %H:%M:%S.%N</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;kube-proxy.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-kube-proxy.log.pos</span><br><span class="line">      tag kube-proxy</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Example:</span><br><span class="line">    # I0204 07:00:19.604280       5 handlers.go:131] GET &#x2F;api&#x2F;v1&#x2F;nodes: (1.624207ms) 200 [[kube-controller-manager&#x2F;v1.1.3 (linux&#x2F;amd64) kubernetes&#x2F;6a81b50] 127.0.0.1:38266]</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id kube-apiserver.log</span><br><span class="line">      @type tail</span><br><span class="line">      format multiline</span><br><span class="line">      multiline_flush_interval 5s</span><br><span class="line">      format_firstline &#x2F;^\w\d&#123;4&#125;&#x2F;</span><br><span class="line">      format1 &#x2F;^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)&#x2F;</span><br><span class="line">      time_format %m%d %H:%M:%S.%N</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;kube-apiserver.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-kube-apiserver.log.pos</span><br><span class="line">      tag kube-apiserver</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Example:</span><br><span class="line">    # I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn&#39;t need update for service kube-system&#x2F;kube-ui</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id kube-controller-manager.log</span><br><span class="line">      @type tail</span><br><span class="line">      format multiline</span><br><span class="line">      multiline_flush_interval 5s</span><br><span class="line">      format_firstline &#x2F;^\w\d&#123;4&#125;&#x2F;</span><br><span class="line">      format1 &#x2F;^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)&#x2F;</span><br><span class="line">      time_format %m%d %H:%M:%S.%N</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;kube-controller-manager.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-kube-controller-manager.log.pos</span><br><span class="line">      tag kube-controller-manager</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Example:</span><br><span class="line">    # W0204 06:49:18.239674       7 reflector.go:245] pkg&#x2F;scheduler&#x2F;factory&#x2F;factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313&#x2F;2577886]) [2579312]</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id kube-scheduler.log</span><br><span class="line">      @type tail</span><br><span class="line">      format multiline</span><br><span class="line">      multiline_flush_interval 5s</span><br><span class="line">      format_firstline &#x2F;^\w\d&#123;4&#125;&#x2F;</span><br><span class="line">      format1 &#x2F;^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)&#x2F;</span><br><span class="line">      time_format %m%d %H:%M:%S.%N</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;kube-scheduler.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-kube-scheduler.log.pos</span><br><span class="line">      tag kube-scheduler</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Example:</span><br><span class="line">    # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path &#x2F;etc&#x2F;gce.conf</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id glbc.log</span><br><span class="line">      @type tail</span><br><span class="line">      format multiline</span><br><span class="line">      multiline_flush_interval 5s</span><br><span class="line">      format_firstline &#x2F;^\w\d&#123;4&#125;&#x2F;</span><br><span class="line">      format1 &#x2F;^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)&#x2F;</span><br><span class="line">      time_format %m%d %H:%M:%S.%N</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;glbc.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-glbc.log.pos</span><br><span class="line">      tag glbc</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Example:</span><br><span class="line">    # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path &#x2F;etc&#x2F;gce.conf</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id cluster-autoscaler.log</span><br><span class="line">      @type tail</span><br><span class="line">      format multiline</span><br><span class="line">      multiline_flush_interval 5s</span><br><span class="line">      format_firstline &#x2F;^\w\d&#123;4&#125;&#x2F;</span><br><span class="line">      format1 &#x2F;^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)&#x2F;</span><br><span class="line">      time_format %m%d %H:%M:%S.%N</span><br><span class="line">      path &#x2F;var&#x2F;log&#x2F;cluster-autoscaler.log</span><br><span class="line">      pos_file &#x2F;var&#x2F;log&#x2F;es-cluster-autoscaler.log.pos</span><br><span class="line">      tag cluster-autoscaler</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # Logs from systemd-journal for interesting services.</span><br><span class="line">    # TODO(random-liu): Remove this after cri container runtime rolls out.</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id journald-docker</span><br><span class="line">      @type systemd</span><br><span class="line">      matches [&#123; &quot;_SYSTEMD_UNIT&quot;: &quot;docker.service&quot; &#125;]</span><br><span class="line">      &lt;storage&gt;</span><br><span class="line">        @type local</span><br><span class="line">        persistent true</span><br><span class="line">        path &#x2F;var&#x2F;log&#x2F;journald-docker.pos</span><br><span class="line">      &lt;&#x2F;storage&gt;</span><br><span class="line">      read_from_head true</span><br><span class="line">      tag docker</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id journald-container-runtime</span><br><span class="line">      @type systemd</span><br><span class="line">      matches [&#123; &quot;_SYSTEMD_UNIT&quot;: &quot;&#123;&#123; fluentd_container_runtime_service &#125;&#125;.service&quot; &#125;]</span><br><span class="line">      &lt;storage&gt;</span><br><span class="line">        @type local</span><br><span class="line">        persistent true</span><br><span class="line">        path &#x2F;var&#x2F;log&#x2F;journald-container-runtime.pos</span><br><span class="line">      &lt;&#x2F;storage&gt;</span><br><span class="line">      read_from_head true</span><br><span class="line">      tag container-runtime</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id journald-kubelet</span><br><span class="line">      @type systemd</span><br><span class="line">      matches [&#123; &quot;_SYSTEMD_UNIT&quot;: &quot;kubelet.service&quot; &#125;]</span><br><span class="line">      &lt;storage&gt;</span><br><span class="line">        @type local</span><br><span class="line">        persistent true</span><br><span class="line">        path &#x2F;var&#x2F;log&#x2F;journald-kubelet.pos</span><br><span class="line">      &lt;&#x2F;storage&gt;</span><br><span class="line">      read_from_head true</span><br><span class="line">      tag kubelet</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id journald-node-problem-detector</span><br><span class="line">      @type systemd</span><br><span class="line">      matches [&#123; &quot;_SYSTEMD_UNIT&quot;: &quot;node-problem-detector.service&quot; &#125;]</span><br><span class="line">      &lt;storage&gt;</span><br><span class="line">        @type local</span><br><span class="line">        persistent true</span><br><span class="line">        path &#x2F;var&#x2F;log&#x2F;journald-node-problem-detector.pos</span><br><span class="line">      &lt;&#x2F;storage&gt;</span><br><span class="line">      read_from_head true</span><br><span class="line">      tag node-problem-detector</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id kernel</span><br><span class="line">      @type systemd</span><br><span class="line">      matches [&#123; &quot;_TRANSPORT&quot;: &quot;kernel&quot; &#125;]</span><br><span class="line">      &lt;storage&gt;</span><br><span class="line">        @type local</span><br><span class="line">        persistent true</span><br><span class="line">        path &#x2F;var&#x2F;log&#x2F;kernel.pos</span><br><span class="line">      &lt;&#x2F;storage&gt;</span><br><span class="line">      &lt;entry&gt;</span><br><span class="line">        fields_strip_underscores true</span><br><span class="line">        fields_lowercase true</span><br><span class="line">      &lt;&#x2F;entry&gt;</span><br><span class="line">      read_from_head true</span><br><span class="line">      tag kernel</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">  forward.input.conf: |-</span><br><span class="line">    # Takes the messages sent over TCP</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id forward</span><br><span class="line">      @type forward</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">  monitoring.conf: |-</span><br><span class="line">    # Prometheus Exporter Plugin</span><br><span class="line">    # input plugin that exports metrics</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id prometheus</span><br><span class="line">      @type prometheus</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id monitor_agent</span><br><span class="line">      @type monitor_agent</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # input plugin that collects metrics from MonitorAgent</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id prometheus_monitor</span><br><span class="line">      @type prometheus_monitor</span><br><span class="line">      &lt;labels&gt;</span><br><span class="line">        host $&#123;hostname&#125;</span><br><span class="line">      &lt;&#x2F;labels&gt;</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # input plugin that collects metrics for output plugin</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id prometheus_output_monitor</span><br><span class="line">      @type prometheus_output_monitor</span><br><span class="line">      &lt;labels&gt;</span><br><span class="line">        host $&#123;hostname&#125;</span><br><span class="line">      &lt;&#x2F;labels&gt;</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">    # input plugin that collects metrics for in_tail plugin</span><br><span class="line">    &lt;source&gt;</span><br><span class="line">      @id prometheus_tail_monitor</span><br><span class="line">      @type prometheus_tail_monitor</span><br><span class="line">      &lt;labels&gt;</span><br><span class="line">        host $&#123;hostname&#125;</span><br><span class="line">      &lt;&#x2F;labels&gt;</span><br><span class="line">    &lt;&#x2F;source&gt;</span><br><span class="line"></span><br><span class="line">  output.conf: |-</span><br><span class="line">    &lt;match **&gt;</span><br><span class="line">      @id elasticsearch</span><br><span class="line">      @type elasticsearch</span><br><span class="line">      @log_level info</span><br><span class="line">      type_name _doc</span><br><span class="line">      include_tag_key true</span><br><span class="line">      host elasticsearch-logging</span><br><span class="line">      port 9200</span><br><span class="line">      logstash_format true</span><br><span class="line">      &lt;buffer&gt;</span><br><span class="line">        @type file</span><br><span class="line">        path &#x2F;var&#x2F;log&#x2F;fluentd-buffers&#x2F;kubernetes.system.buffer</span><br><span class="line">        flush_mode interval</span><br><span class="line">        retry_type exponential_backoff</span><br><span class="line">        flush_thread_count 2</span><br><span class="line">        flush_interval 5s</span><br><span class="line">        retry_forever</span><br><span class="line">        retry_max_interval 30</span><br><span class="line">        chunk_limit_size 2M</span><br><span class="line">        total_limit_size 500M</span><br><span class="line">        overflow_action block</span><br><span class="line">      &lt;&#x2F;buffer&gt;</span><br><span class="line">    &lt;&#x2F;match&gt;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl appply -f fluentd-es-config.yaml</span><br></pre></td></tr></table></figure>


<h2 id="创建Fluentd"><a href="#创建Fluentd" class="headerlink" title="创建Fluentd"></a>创建Fluentd</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; fluentd-es.yaml &lt;&lt; EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-es</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-es</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-es</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-es</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - &quot;namespaces&quot;</span><br><span class="line">  - &quot;pods&quot;</span><br><span class="line">  verbs:</span><br><span class="line">  - &quot;get&quot;</span><br><span class="line">  - &quot;watch&quot;</span><br><span class="line">  - &quot;list&quot;</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-es</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-es</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: fluentd-es</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: fluentd-es</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line">---</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-es-v3.0.0</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-es</span><br><span class="line">    version: v3.0.0</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: fluentd-es</span><br><span class="line">      version: v3.0.0</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: fluentd-es</span><br><span class="line">        version: v3.0.0</span><br><span class="line">      # This annotation ensures that fluentd does not get evicted if the node</span><br><span class="line">      # supports critical pod annotation based priority scheme.</span><br><span class="line">      # Note that this does not guarantee admission on the nodes (#40573).</span><br><span class="line">      annotations:</span><br><span class="line">        seccomp.security.alpha.kubernetes.io&#x2F;pod: &#39;docker&#x2F;default&#39;</span><br><span class="line">    spec:</span><br><span class="line">      priorityClassName: system-node-critical</span><br><span class="line">      serviceAccountName: fluentd-es</span><br><span class="line">      containers:</span><br><span class="line">      - name: fluentd-es</span><br><span class="line">        image: quay.io&#x2F;fluentd_elasticsearch&#x2F;fluentd:v3.0.0</span><br><span class="line">        env:</span><br><span class="line">        - name: FLUENTD_ARGS</span><br><span class="line">          value: --no-supervisor -q</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            memory: 500Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 200Mi</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: varlog</span><br><span class="line">          mountPath: &#x2F;var&#x2F;log</span><br><span class="line">        - name: varlibdockercontainers</span><br><span class="line">          mountPath: &#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers</span><br><span class="line">          readOnly: true</span><br><span class="line">        - name: config-volume</span><br><span class="line">          mountPath: &#x2F;etc&#x2F;fluent&#x2F;config.d</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 24231</span><br><span class="line">          name: prometheus</span><br><span class="line">          protocol: TCP</span><br><span class="line">        livenessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: prometheus</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          timeoutSeconds: 10</span><br><span class="line">        readinessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: prometheus</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          timeoutSeconds: 10</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      volumes:</span><br><span class="line">      - name: varlog</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;var&#x2F;log</span><br><span class="line">      - name: varlibdockercontainers</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers</span><br><span class="line">      - name: config-volume</span><br><span class="line">        configMap:</span><br><span class="line">          name: fluentd-es-config-v0.2.0</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl apply -f fluentd-es.yaml</span><br></pre></td></tr></table></figure>


<h2 id="相关文档"><a href="#相关文档" class="headerlink" title="相关文档"></a>相关文档</h2><p><a href="https://github.com/coreos/prometheus-operator">https://github.com/coreos/prometheus-operator</a></p>
<p><a href="https://github.com/prometheus/prometheus">https://github.com/prometheus/prometheus</a></p>
<p><a href="https://github.com/kubernetes-incubator/external-storage">https://github.com/kubernetes-incubator/external-storage</a></p>
<p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">https://kubernetes.io/docs/concepts/storage/persistent-volumes/</a></p>
<p><a href="https://quay.io/repository/external_storage/rbd-provisioner?tab=tags">https://quay.io/repository/external_storage/rbd-provisioner?tab=tags</a></p>
<p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>ceph</tag>
        <tag>kubernets</tag>
      </tags>
  </entry>
  <entry>
    <title>FFmpeg静态构建</title>
    <url>/posts/e5fa3cb8.html</url>
    <content><![CDATA[<p>适用于Linux内核2.6.32及更高版本的FFmpeg的最新版本，由于通过yum安装版本过低，通过原代码编译过于繁杂，于是采用别人编译好的静态构建文件。</p>
<p>FFmpeg静态构建：<a href="https://www.johnvansickle.com/ffmpeg/">https://www.johnvansickle.com/ffmpeg/</a></p>
<p>FFmpeg官网：<a href="http://ffmpeg.org/">http://ffmpeg.org/</a></p>
<a id="more"></a>

<p>下载需要的版本即可，解压文件后将执行文件拷贝到/usr/bin/即可</p>
<p>或者添加环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;profile</span><br><span class="line">加入以下内容:</span><br><span class="line">export PATH&#x3D;&quot;&#x2F;usr&#x2F;local&#x2F;ffmpeg:$PATH&quot;</span><br><span class="line"></span><br><span class="line">然后保存并运行source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>日常部署记录</category>
      </categories>
      <tags>
        <tag>ffmpeg</tag>
      </tags>
  </entry>
  <entry>
    <title>日志分割、日志清理脚本</title>
    <url>/posts/68f72c3a.html</url>
    <content><![CDATA[<p>  用于工作中的日志分割、日志清理脚本</p>
<a id="more"></a>

<h1 id="日志清理"><a href="#日志清理" class="headerlink" title="日志清理"></a>日志清理</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">file_log&#x3D;&#x2F;data&#x2F;application&#x2F;mpmt&#x2F;mpmt-file&#x2F;</span><br><span class="line">comm_log&#x3D;&#x2F;data&#x2F;application&#x2F;mpmt&#x2F;mpmt-comm-dispatcher&#x2F;</span><br><span class="line">remove_file_log()&#123;</span><br><span class="line">        rm -f &#96;find $1 -name &quot;*.log&quot; -atime +7&#96;</span><br><span class="line">&#125;</span><br><span class="line">remove_comm_log()&#123;</span><br><span class="line">        rm -f &#96;find $1 -name &quot;*.log&quot; -atime +7&#96;</span><br><span class="line">&#125;</span><br><span class="line">remove_comm_log $comm_log</span><br><span class="line">remove_file_log $file_log</span><br><span class="line">find &#x2F;data&#x2F;application&#x2F;mpmt&#x2F;log&#x2F; -mtime +7 -name &quot;*.*&quot; -exec rm -rf &#123;&#125; \;</span><br><span class="line">find &#x2F;data&#x2F;application&#x2F;mpmt&#x2F;mpmt-comm-dispatcher&#x2F;backup&#x2F; -mtime +7 -name &quot;*.*&quot; -exec rm -rf &#123;&#125; \;</span><br></pre></td></tr></table></figure>

<h1 id="日志分割"><a href="#日志分割" class="headerlink" title="日志分割"></a>日志分割</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">fg_log()&#123;</span><br><span class="line">        T_DATE_FILE_NEW_1&#x3D;&#96;echo $1 | awk -F &#39;.&#39; &#39;&#123;print $1&#125;&#39;&#96;</span><br><span class="line">        T_DATE_FILE_NEW_2&#x3D;&#96;echo $1 | awk -F &#39;.&#39; &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line">        T_DATE_FILE_NEW&#x3D;$T_DATE_FILE_NEW_1&#39;-&#39;&#96;date +%F&#96;&#39;.&#39;$T_DATE_FILE_NEW_2</span><br><span class="line">        cp $1 $T_DATE_FILE_NEW</span><br><span class="line">        echo &#39;&#39; &gt; $1</span><br><span class="line">&#125;</span><br><span class="line">fg_log &#x2F;data&#x2F;application&#x2F;mpmt&#x2F;mpmt-comm-dispatcher&#x2F;nohup.log</span><br><span class="line">fg_log &#x2F;data&#x2F;application&#x2F;mpmt&#x2F;mpmt-file&#x2F;nohup.log</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>shell脚本</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>故障问题收集</title>
    <url>/posts/407ebcc2.html</url>
    <content><![CDATA[<h2 id="java-lang-OutOfMemoryError"><a href="#java-lang-OutOfMemoryError" class="headerlink" title="java.lang.OutOfMemoryError"></a>java.lang.OutOfMemoryError</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这个服务有一个线程池，在代码里面设置的最小是8，最大限制是2147483647 ，用完的线程要1分钟之后才能回收。这就存在两个问题：</span><br><span class="line">一、业务在持续不断的发送请求，这个服务就会一直创建线程，而因为给定的线程最大值过大，相当于可以无限制的创建线程了，会一直消耗资源；</span><br><span class="line">二、用完的线程1分钟之后才会回收，时间过长。</span><br><span class="line">在这两点的影响下，程序跑一段时间，就会出现创建大量的线程，过度的消耗内存资源.</span><br><span class="line"></span><br><span class="line">由于docker容器在最初的时候没有做容器的内存限制，所以默认情况下容器使用的资源是不受限制的。</span><br><span class="line">也就是可以使用主机内核调度器所允许的最大资源，因此当主机发现内存不够用的时候，也会抛出内存溢出的错误。而且会开始杀死一些进程用于释放内存空间。可怕的是任何进程都可能成为内核猎杀的对象，包括 docker daemon 和宿主机上的其它一些重要的程序。更危险的是如果某个支持系统运行的重要进程被kill掉了，整个系统也就宕掉了。</span><br><span class="line"></span><br><span class="line">1、开发优化代码，包括限制线程池的最大线程数量和线程回收的时间，重新发布代码打补丁，后面观察到目前，没有再出现类这个问题了；</span><br><span class="line">2、限制docker内存。重新优化了docker容器，限制了docker内存的使用量，减少docker容器过度占用宿主机资源的风险；</span><br><span class="line">3、加强对docker容器的监控与告警；</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>面试收集</category>
      </categories>
      <tags>
        <tag>故障</tag>
      </tags>
  </entry>
  <entry>
    <title>全站 HTTPS 改造方案</title>
    <url>/posts/ac68ea5a.html</url>
    <content><![CDATA[<h2 id="实例仅供参考"><a href="#实例仅供参考" class="headerlink" title="实例仅供参考"></a>实例仅供参考</h2><a id="more"></a>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># http 自动跳转 https</span></span><br><span class="line">server &#123;</span><br><span class="line">        listen       xxxx:80;</span><br><span class="line">        server_name  xxxx.com;</span><br><span class="line">        <span class="built_in">return</span> 301 https://<span class="variable">$server_name</span><span class="variable">$request_uri</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       xxxx:443 ssl;</span><br><span class="line">    server_name  xxxx.com;</span><br><span class="line"></span><br><span class="line">    access_log /usr/<span class="built_in">local</span>/nginx/logs/access.log;</span><br><span class="line">    error_log /usr/<span class="built_in">local</span>/nginx/logs/error.log;</span><br><span class="line"></span><br><span class="line">    client_max_body_size    512m;</span><br><span class="line"></span><br><span class="line">    ssl on;</span><br><span class="line">    ssl_certificate     /usr/<span class="built_in">local</span>/nginx/conf/domain.cert;</span><br><span class="line">    ssl_certificate_key /usr/<span class="built_in">local</span>/nginx/conf/domain.key;</span><br><span class="line">    ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;</span><br><span class="line">    ssl_ciphers         HIGH:!aNULL:!MD5;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============================ xxxx ============================</span></span><br><span class="line">    <span class="comment"># PAAS_SERVICE HOST/PORT</span></span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass http://SSL_OPEN;</span><br><span class="line">        proxy_pass_header Server;</span><br><span class="line">        proxy_set_header X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        proxy_set_header X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        proxy_set_header X-Scheme <span class="variable">$scheme</span>;</span><br><span class="line">        proxy_set_header Host <span class="variable">$http_host</span>;</span><br><span class="line">        proxy_redirect off;</span><br><span class="line">        proxy_read_timeout 600;</span><br><span class="line">    &#125;</span><br><span class="line">    .........</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>在k8s上通过helm与动态NFS部署kafka、zookeeper</title>
    <url>/posts/3648b3e2.html</url>
    <content><![CDATA[<p>helm仓库地址<a href="https://github.com/helm/charts">https://github.com/helm/charts</a></p>
<a id="more"></a>
<h1 id="添加helm仓库"><a href="#添加helm仓库" class="headerlink" title="添加helm仓库"></a>添加helm仓库</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">谷歌仓库</span><br><span class="line">helm repo add incubator http:&#x2F;&#x2F;storage.googleapis.com&#x2F;kubernetes-charts-incubator</span><br><span class="line"></span><br><span class="line">由于国内网络缘故，可添加阿里云，但是zookeeper版本为v2，谷歌为v3</span><br><span class="line">helm repo add incubator https:&#x2F;&#x2F;aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com&#x2F;charts-incubator&#x2F;</span><br><span class="line"></span><br><span class="line">这里采用谷歌仓库的kafka</span><br><span class="line">[root@public03 ~]# helm search kafka</span><br><span class="line">NAME                     	CHART VERSION	APP VERSION	DESCRIPTION                                                 </span><br><span class="line">incubator&#x2F;kafka          	0.9.6        	4.1.2      	Apache Kafka is publish-subscribe messaging rethought as ...</span><br></pre></td></tr></table></figure>

<h1 id="修改kafa相关配置"><a href="#修改kafa相关配置" class="headerlink" title="修改kafa相关配置"></a>修改kafa相关配置</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#下载</span><br><span class="line">hlem fetch incubator&#x2F;kafka</span><br><span class="line">#解压</span><br><span class="line">tar zxf kafka-0.9.6.tgz &amp;&amp; cd kafka</span><br><span class="line">#修改文件zookeeper</span><br><span class="line">charts&#x2F;zookeeper&#x2F;templates&#x2F;statefulset.yaml</span><br><span class="line">#zookeeper的镜像地址改为registry.cn-hangzhou.aliyuncs.com&#x2F;appstore&#x2F;k8szk:v3</span><br><span class="line">最后创建pvc模板修改为以下：</span><br><span class="line">volumeClaimTemplates:</span><br><span class="line">    - metadata:</span><br><span class="line">        name: data</span><br><span class="line">        annotations:</span><br><span class="line">          volume.beta.kubernetes.io&#x2F;storage-class: &quot;managed-nfs-storage&quot;</span><br><span class="line">      spec:</span><br><span class="line">        accessModes:</span><br><span class="line">          - ReadWriteMany</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 1Gi</span><br><span class="line"></span><br><span class="line">#修改文件kafka</span><br><span class="line">templates&#x2F;statefulset.yaml</span><br><span class="line">最后创建pvc模板修改为以下：</span><br><span class="line">volumeClaimTemplates:</span><br><span class="line">    - metadata:</span><br><span class="line">        name: data</span><br><span class="line">        annotations:</span><br><span class="line">          volume.beta.kubernetes.io&#x2F;storage-class: &quot;managed-nfs-storage&quot;</span><br><span class="line">      spec:</span><br><span class="line">        accessModes:</span><br><span class="line">          - ReadWriteMany</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 1Gi</span><br><span class="line"></span><br><span class="line">storageclass查询命令： kubectl get sc --all-namespaces</span><br></pre></td></tr></table></figure>

<h1 id="安装kafka"><a href="#安装kafka" class="headerlink" title="安装kafka"></a>安装kafka</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm install .&#x2F;kafka -n kafka --namespace xxx</span><br><span class="line">#查看pvc状态</span><br><span class="line">kubectl get pvc -n xxx</span><br></pre></td></tr></table></figure>

<h1 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h1><h2 id="创建测试pod"><a href="#创建测试pod" class="headerlink" title="创建测试pod"></a>创建测试pod</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: testclient</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: kafka</span><br><span class="line">    image: confluentinc&#x2F;cp-kafka:4.1.2-2</span><br><span class="line">    command:</span><br><span class="line">    - sh</span><br><span class="line">    - -c</span><br><span class="line">    - &quot;exec tail -f &#x2F;dev&#x2F;null&quot;</span><br><span class="line"></span><br><span class="line">kubectl apply -f testclient.yaml</span><br></pre></td></tr></table></figure>

<h2 id="列出topics"><a href="#列出topics" class="headerlink" title="列出topics"></a>列出topics</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl -n default exec testclient -- &#x2F;usr&#x2F;bin&#x2F;kafka-topics --zookeeper kafka-zookeeper:2181 --list</span><br><span class="line"></span><br><span class="line">[root@public03 ~]# kubectl -n default exec testclient -- &#x2F;usr&#x2F;bin&#x2F;kafka-topics --zookeeper kafka-zookeeper:2181 --list</span><br><span class="line">__confluent.support.metrics</span><br></pre></td></tr></table></figure>

<h2 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl -n default exec testclient -- &#x2F;usr&#x2F;bin&#x2F;kafka-topics --zookeeper kafka-zookeeper:2181 --topic test1 --create --partitions 1 --replication-factor 1</span><br><span class="line"></span><br><span class="line">[root@public03 ~]# kubectl -n default exec testclient -- &#x2F;usr&#x2F;bin&#x2F;kafka-topics --zookeeper kafka-zookeeper:2181 --topic test1 --create --partitions 1 --replication-factor 1</span><br><span class="line">Created topic &quot;test1&quot;.</span><br></pre></td></tr></table></figure>

<h2 id="在一个topic监听messages"><a href="#在一个topic监听messages" class="headerlink" title="在一个topic监听messages"></a>在一个topic监听messages</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl -n default exec -ti testclient -- &#x2F;usr&#x2F;bin&#x2F;kafka-console-consumer --bootstrap-server kafka:9092 --topic test1 --from-beginning</span><br><span class="line"></span><br><span class="line">#获取服务地址</span><br><span class="line">kubectl get svc</span><br></pre></td></tr></table></figure>

<h2 id="在另外一个终端启动交互式消息生产者会话"><a href="#在另外一个终端启动交互式消息生产者会话" class="headerlink" title="在另外一个终端启动交互式消息生产者会话"></a>在另外一个终端启动交互式消息生产者会话</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl -n default exec -ti testclient -- &#x2F;usr&#x2F;bin&#x2F;kafka-console-producer --broker-list kafka-headless:9092 --topic test1</span><br><span class="line"></span><br><span class="line">#获取服务地址</span><br><span class="line">kubectl get svc</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>nfs</tag>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title>搭建Docker私有仓库Harbor</title>
    <url>/posts/570aac64.html</url>
    <content><![CDATA[<p>官网：<a href="https://goharbor.io/">https://goharbor.io/</a><br>安装介绍：<a href="https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md">https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md</a></p>
<h1 id="系统要求"><a href="#系统要求" class="headerlink" title="系统要求"></a>系统要求</h1><p>docker 17.03.0-ce +<br>docker-compose 1.10.0+</p>
<a id="more"></a>
<h1 id="安装docker-compose"><a href="#安装docker-compose" class="headerlink" title="安装docker-compose"></a>安装docker-compose</h1><p>docker-compose安装：<a href="https://docs.docker.com/compose/install/">https://docs.docker.com/compose/install/</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo curl -L &quot;https:&#x2F;&#x2F;github.com&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.24.0&#x2F;docker-compose-$(uname -s)-$(uname -m)&quot; -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line"></span><br><span class="line">chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">docker-compose --version</span><br></pre></td></tr></table></figure>

<h1 id="安装harbor"><a href="#安装harbor" class="headerlink" title="安装harbor"></a>安装harbor</h1><p>下载地址：<a href="https://github.com/goharbor/harbor/releases">https://github.com/goharbor/harbor/releases</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#下载的是offline离线包</span><br><span class="line">wget https:&#x2F;&#x2F;storage.googleapis.com&#x2F;harbor-releases&#x2F;release-1.8.0&#x2F;harbor-offline-installer-v1.8.1.tgz</span><br><span class="line"></span><br><span class="line">#解压，配置harbor</span><br><span class="line">tar zxvf harbor-offline-installer-v1.8.1.tgz</span><br><span class="line">cd harbor&#x2F;</span><br><span class="line"></span><br><span class="line">#vim harbor.yml</span><br><span class="line">hostname &#x3D; 192.168.1.62</span><br><span class="line"></span><br><span class="line">#安装</span><br><span class="line">.&#x2F;install.sh</span><br><span class="line"></span><br><span class="line">#Harbor的启动和停止</span><br><span class="line">启动Harbor</span><br><span class="line"># docker-compose start</span><br><span class="line">停止Harbor</span><br><span class="line"># docker-comose stop</span><br><span class="line">重启Harbor</span><br><span class="line"># docker-compose restart</span><br><span class="line"></span><br><span class="line">#配置docker(https错误)</span><br><span class="line">#vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd --insecure-registry&#x3D;192.168.100.120</span><br><span class="line">#只加上--insecure-registry这个参数即可</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line">创建&#x2F;etc&#x2F;docker&#x2F;daemon.json文件，在文件中指定仓库地址</span><br><span class="line"># cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt; EOF</span><br><span class="line">&#123; </span><br><span class="line">&quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;bv55mwyn.mirror.aliyuncs.com&quot;],</span><br><span class="line">&quot;insecure-registries&quot;:[&quot;192.168.100.120&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">然后重启docker</span><br></pre></td></tr></table></figure>









]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Harbor</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux进程出现D状态</title>
    <url>/posts/89b3d146.html</url>
    <content><![CDATA[<h1 id="问题状态"><a href="#问题状态" class="headerlink" title="问题状态"></a>问题状态</h1><p>生产环境使用了开源软件srs服务来调取实时视频，但是调取时提示出现了暂无数据，请稍后再试的错误，一直调取不到视频信息。<br><img src="/images/20181018140643.png" width="100%" height="100%"></p>
<a id="more"></a>
<h1 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h1><p>查看了srs服务、文件服务、通信服务是否正常，云服务器负载情况，内存使用情况，网络情况，最后发现srs服务器进程状态为D，进程无法用 kill 杀掉，导致了生产环境不可用。<br><img src="/images/20181018141216.png" width="100%" height="100%"></p>
<p>ps 的手册里说 D 状态是 uninterruptible sleep，Linux 进程有两种睡眠状态，一种 interruptible sleep，处在这种睡眠状态的进程是可以通过给它发信号来唤醒的，比如发 HUP 信号给 nginx 的 master 进程可以让 nginx 重新加载配置文件而不需要重新启动 nginx 进程；另外一种睡眠状态是 uninterruptible sleep，处在这种状态的进程不接受外来的任何信号，这也是为什么之前我无法用 kill 杀掉这些处于 D 状态的进程，无论是 kill, kill -9 还是 kill -15，因为它们压根儿就不受这些信号的支配。</p>
<p>进程为什么会被置于 uninterruptible sleep 状态呢？处于 uninterruptible sleep 状态的进程通常是在等待 IO，比如磁盘 IO，网络 IO，其他外设 IO，如果进程正在等待的 IO 在较长的时间内都没有响应，那么就很会不幸地被 ps 看到了，同时也就意味着很有可能有 IO 出了问题，可能是外设本身出了故障，也可能是比如挂载的远程文件系统已经不可访问了，我这里遇到的问题就是由 down 掉的 NFS 服务器引起的。</p>
<p>正是因为得不到 IO 的相应，进程才进入了 uninterruptible sleep 状态，所以要想使进程从 uninterruptible sleep 状态恢复，就得使进程等待的 IO 恢复。</p>
<h2 id="查看message日志文件找到相关内核报错日志"><a href="#查看message日志文件找到相关内核报错日志" class="headerlink" title="查看message日志文件找到相关内核报错日志"></a>查看message日志文件找到相关内核报错日志</h2><img src="/images/20181018091126.png" width="100%" height="100%">

<p>默认情况下， Linux会最多使用40%的可用内存作为文件系统缓存。当超过这个阈值后，文件系统会把将缓存中的内存全部写入磁盘， 导致后续的IO请求都是同步的。将缓存写入磁盘时，有一个默认120秒的超时时间。 出现上面的问题的原因是IO子系统的处理速度不够快，不能在120秒将缓存中的数据全部写入磁盘。IO系统响应缓慢，导致越来越多的请求堆积，最终系统内存全部被占用，导致系统失去响应。</p>
<h2 id="从系统中看下-hung-task-相关的参数及其参数值"><a href="#从系统中看下-hung-task-相关的参数及其参数值" class="headerlink" title="从系统中看下 hung_task 相关的参数及其参数值"></a>从系统中看下 hung_task 相关的参数及其参数值</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliy-nt1 ~]# sysctl -a | grep &#39;vm.dirty&#39;</span><br><span class="line">vm.dirty_background_bytes &#x3D; 0</span><br><span class="line">vm.dirty_background_ratio &#x3D; 10</span><br><span class="line">vm.dirty_bytes &#x3D; 0</span><br><span class="line">vm.dirty_expire_centisecs &#x3D; 3000</span><br><span class="line">vm.dirty_ratio &#x3D; 20</span><br><span class="line">vm.dirty_writeback_centisecs &#x3D; 500</span><br><span class="line"></span><br><span class="line">[root@aliy-nt1 ~]# sysctl -a | grep hung</span><br><span class="line">kernel.hung_task_check_count &#x3D; 4194304</span><br><span class="line">kernel.hung_task_panic &#x3D; 0</span><br><span class="line">kernel.hung_task_timeout_secs &#x3D; 120</span><br><span class="line">kernel.hung_task_warnings &#x3D; 0</span><br></pre></td></tr></table></figure>

<h2 id="通过sar查看一段时间内的cpu使用情况，CPU使用率不高。"><a href="#通过sar查看一段时间内的cpu使用情况，CPU使用率不高。" class="headerlink" title="通过sar查看一段时间内的cpu使用情况，CPU使用率不高。"></a>通过sar查看一段时间内的cpu使用情况，CPU使用率不高。</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sar -f &#x2F;var&#x2F;log&#x2F;sa&#x2F;sa17</span><br></pre></td></tr></table></figure>
<img src="/images/20181018101248.png" width="100%" height="100%">


<h2 id="查看内存，内存使用也不高"><a href="#查看内存，内存使用也不高" class="headerlink" title="查看内存，内存使用也不高"></a>查看内存，内存使用也不高</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sar -r -f &#x2F;var&#x2F;log&#x2F;sa&#x2F;sa17</span><br></pre></td></tr></table></figure>
<img src="/images/20181018101935.png" width="100%" height="100%">

<h2 id="查看硬盘IO"><a href="#查看硬盘IO" class="headerlink" title="查看硬盘IO"></a>查看硬盘IO</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sar -d -f &#x2F;var&#x2F;log&#x2F;sa&#x2F;sa17</span><br></pre></td></tr></table></figure>
<img src="/images/20181018103620.png" width="100%" height="100%">

<h2 id="检查磁盘分区是否存在坏块，发现没有坏块情况"><a href="#检查磁盘分区是否存在坏块，发现没有坏块情况" class="headerlink" title="检查磁盘分区是否存在坏块，发现没有坏块情况"></a>检查磁盘分区是否存在坏块，发现没有坏块情况</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliy-nt1 ~]# &#x2F;sbin&#x2F;badblocks -v &#x2F;dev&#x2F;vda</span><br><span class="line">正在检查从 0 到 41943039的块</span><br><span class="line">Checking for bad blocks (read-only test): done                                                 </span><br><span class="line">Pass completed, 0 bad blocks found. (0&#x2F;0&#x2F;0 errors)</span><br><span class="line">[root@aliy-nt1 ~]# &#x2F;sbin&#x2F;badblocks -v &#x2F;dev&#x2F;vdb</span><br><span class="line">正在检查从 0 到 41943039的块</span><br><span class="line">Checking for bad blocks (read-only test): done                                                 </span><br><span class="line">Pass completed, 0 bad blocks found. (0&#x2F;0&#x2F;0 errors)</span><br><span class="line">[root@aliy-nt1 ~]# &#x2F;sbin&#x2F;badblocks -v &#x2F;dev&#x2F;vdc</span><br><span class="line">正在检查从 0 到 104857599的块</span><br><span class="line">Checking for bad blocks (read-only test): done                                                 </span><br><span class="line">Pass completed, 0 bad blocks found. (0&#x2F;0&#x2F;0 errors)</span><br></pre></td></tr></table></figure>

<h1 id="查找网上解决方法如下："><a href="#查找网上解决方法如下：" class="headerlink" title="查找网上解决方法如下："></a>查找网上解决方法如下：</h1><p>根据应用程序情况，对vm.dirty_ratio，vm.dirty_background_ratio两个参数进行调优设置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># sysctl -w vm.dirty_ratio&#x3D;10</span><br><span class="line"># sysctl -w vm.dirty_background_ratio&#x3D;5</span><br><span class="line"># sysctl -p</span><br><span class="line"></span><br><span class="line">如果系统永久生效，修改&#x2F;etc&#x2F;sysctl.conf文件。加入如下两行：</span><br><span class="line">#vi &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line"></span><br><span class="line">vm.dirty_background_ratio &#x3D; 5</span><br><span class="line">vm.dirty_ratio &#x3D; 10</span><br><span class="line"></span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure>

<p>但是观察了一段时间，进程依然状态会导致为D，查看了srs配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">listen              1935;</span><br><span class="line">max_connections     1000;</span><br><span class="line">srs_log_tank        file;</span><br><span class="line">srs_log_file        .&#x2F;objs&#x2F;srs.log;</span><br><span class="line">http_api &#123;</span><br><span class="line">    enabled         on;</span><br><span class="line">    listen          1985;</span><br><span class="line">&#125;</span><br><span class="line">http_server &#123;</span><br><span class="line">    enabled         on;</span><br><span class="line">    listen          8080;</span><br><span class="line">    dir             .&#x2F;objs&#x2F;nginx&#x2F;html;</span><br><span class="line">#    crossdomain     on;</span><br><span class="line">&#125;</span><br><span class="line">stats &#123;</span><br><span class="line">    network         0;</span><br><span class="line">    disk            sda sdb xvda xvdb;</span><br><span class="line">&#125;</span><br><span class="line">vhost __defaultVhost__ &#123;</span><br><span class="line">    http_remux &#123;</span><br><span class="line">        enabled     on;</span><br><span class="line">        mount       [vhost]&#x2F;[app]&#x2F;[stream].flv;</span><br><span class="line">        hstrs       on;</span><br><span class="line">    &#125;</span><br><span class="line">#	mix_correct		on;</span><br><span class="line">	http_hooks &#123;</span><br><span class="line">		enabled         on;</span><br><span class="line">        	on_publish      http:&#x2F;&#x2F;10.25.208.243:1241&#x2F;callbackOnPublish;</span><br><span class="line">        	on_unpublish    http:&#x2F;&#x2F;10.25.208.243:1241&#x2F;callbackOnUnpublish;</span><br><span class="line">        	on_play         http:&#x2F;&#x2F;10.25.208.243:1241&#x2F;callbackOnPlay;</span><br><span class="line">        	on_stop         http:&#x2F;&#x2F;10.25.208.243:1241&#x2F;callbackOnStop;</span><br><span class="line">        	on_dvr          http:&#x2F;&#x2F;10.25.208.243:1241&#x2F;callbackOnDvr;</span><br><span class="line">	&#125;</span><br><span class="line">        dvr &#123;</span><br><span class="line">                enabled         on;</span><br><span class="line">                dvr_path        &#x2F;mnt&#x2F;mov&#x2F;[stream]-[timestamp].flv;</span><br><span class="line">                dvr_plan        session;</span><br><span class="line">                dvr_duration    30;</span><br><span class="line">                dvr_wait_keyframe       on;</span><br><span class="line">                time_jitter             full;</span><br><span class="line">        &#125;</span><br><span class="line">        hls &#123;</span><br><span class="line">                enabled         on;</span><br><span class="line">                hls_path        &#x2F;mnt&#x2F;mov;</span><br><span class="line">                hls_m3u8_file   [stream].m3u8;</span><br><span class="line">                hls_ts_file     [stream]-[seq].ts;</span><br><span class="line">                hls_fragment    10;</span><br><span class="line">                hls_window      60000;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>srs录制视频、语音是通过阿里云NFS文件服务挂载到本地/mnt/mov文件，作为临时视频中转服务，后期会自动上传到OSS对象存储服务，是否因为频繁的对该NFS读写IO性能导致srs出现hung呢？<br>查看相关文档，阿里云由此文档<br><a href="https://help.aliyun.com/knowledge_detail/53839.html?spm=a2c4g.11186623.4.7.jXaKcm">https://help.aliyun.com/knowledge_detail/53839.html?spm=a2c4g.11186623.4.7.jXaKcm</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Linux nfs客户端对于同时发起的NFS请求数量进行了控制，若该参数配置较小会导致IO性能较差，请查看该参数：cat &#x2F;proc&#x2F;sys&#x2F;sunrpc&#x2F;tcp_slot_table_entries</span><br><span class="line"></span><br><span class="line">[root@aliy-nt1 srs]# cat &#x2F;proc&#x2F;sys&#x2F;sunrpc&#x2F;tcp_slot_table_entries</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">默认编译的内核该参数最大值为256，可适当提高该参数的值来取得较好的性能，请以root身份执行以下命令：</span><br><span class="line"></span><br><span class="line">echo &quot;options sunrpc tcp_slot_table_entries&#x3D;128&quot; &gt;&gt; &#x2F;etc&#x2F;modprobe.d&#x2F;sunrpc.conf</span><br><span class="line">echo &quot;options sunrpc tcp_max_slot_table_entries&#x3D;128&quot; &gt;&gt;  &#x2F;etc&#x2F;modprobe.d&#x2F;sunrpc.conf</span><br><span class="line">sysctl -w sunrpc.tcp_slot_table_entries&#x3D;128</span><br><span class="line">修改完成后，您需要重新挂载文件系统或重启机器。</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>nfs</tag>
      </tags>
  </entry>
  <entry>
    <title>ELK5.x安装过程中所遇到的问题</title>
    <url>/posts/1a217b43.html</url>
    <content><![CDATA[<p><strong>问题一</strong></p>
<p>[2017-11-30T17:37:20,165][WARN ][o.e.b.JNANatives         ] unable to install syscall filter:<br>java.lang.UnsupportedOperationException: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解决：使用linux内核3.5版本，不更换也可以。</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p><strong>问题二</strong><br>[2017-11-30T17:37:24,329][WARN ][o.e.b.BootstrapChecks    ] [m42Pcik] max number of threads [1024] for user [logtest] is too low, increase to at least [2048]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解决：切换到root用户，进入limits.d目录下修改配置文件。</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;90-nproc.conf </span><br><span class="line"></span><br><span class="line">修改如下内容：</span><br><span class="line">* soft nproc 1024</span><br><span class="line">#修改为</span><br><span class="line">* soft nproc 2048</span><br></pre></td></tr></table></figure>

<p><strong>问题三</strong><br>[2017-11-30T17:04:38,295][WARN ][o.e.b.BootstrapChecks    ] [m42Pcik] max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解决：切换到root用户，编辑limits.conf 添加类似如下内容</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;security&#x2F;limits.conf </span><br><span class="line"></span><br><span class="line">添加如下内容:</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072</span><br><span class="line">* soft nproc 2048</span><br><span class="line">* hard nproc 4096</span><br></pre></td></tr></table></figure>

<p><strong>问题四</strong><br>max number of threads [1024] for user [lish] likely too low, increase to at least [2048]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解决：切换到root用户修改配置sysctl.conf</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;sysctl.conf </span><br><span class="line"></span><br><span class="line">添加下面配置：</span><br><span class="line">vm.max_map_count&#x3D;655360</span><br><span class="line">并执行命令：</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure>

<p><strong>问题五</strong><br>org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: failed to obtain node locks, tried [[/usr/local/elasticsearch-5.2.0/data/my-application]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解决：</span><br><span class="line">cd &#x2F;usr&#x2F;local&#x2F;elasticsearch-5.2.0&#x2F;data</span><br><span class="line">rm -rf nodes&#x2F;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title>使用kubeadm安装kubernetes 1.12.x</title>
    <url>/posts/aaf38618.html</url>
    <content><![CDATA[<p>基本安装方法与其他版本安装无异，只是镜像名更改了，去掉了amd64</p>
<a id="more"></a>
<h1 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h1><h2 id="验证MAC地址和product-uuid对于每个节点都是唯一的"><a href="#验证MAC地址和product-uuid对于每个节点都是唯一的" class="headerlink" title="验证MAC地址和product_uuid对于每个节点都是唯一的"></a>验证MAC地址和product_uuid对于每个节点都是唯一的</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ip link或获取网络接口的MAC地址ifconfig -a</span><br><span class="line">sudo cat &#x2F;sys&#x2F;class&#x2F;dmi&#x2F;id&#x2F;product_uuid</span><br></pre></td></tr></table></figure>

<h2 id="检查端口是否开放"><a href="#检查端口是否开放" class="headerlink" title="检查端口是否开放"></a>检查端口是否开放</h2><p>ps 在一次客户提供的服务器安装k8s，无法操作k8s读取日志文件，才发生端口被禁</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">master</span><br><span class="line">TCP	6443	Kubernetes API server</span><br><span class="line">TCP 2379-2380	etcd server client API</span><br><span class="line">TCP 10250 	Kubelet API</span><br><span class="line">TCP 10251	kube-scheduler</span><br><span class="line">TCP 10252	kube-controller-manager</span><br><span class="line"></span><br><span class="line">node</span><br><span class="line">TCP 10250	Kubelet API</span><br><span class="line">TCP 30000-32767	NodePort Services</span><br></pre></td></tr></table></figure>

<h2 id="关闭Selinux-firewalld"><a href="#关闭Selinux-firewalld" class="headerlink" title="关闭Selinux/firewalld"></a>关闭Selinux/firewalld</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line"></span><br><span class="line">setenforce 0</span><br><span class="line">#以允许容器访问主机文件系统，例如pod网络所需</span><br><span class="line">sed -i &quot;s&#x2F;SELINUX&#x3D;enforcing&#x2F;SELINUX&#x3D;disabled&#x2F;g&quot; &#x2F;etc&#x2F;selinux&#x2F;config</span><br></pre></td></tr></table></figure>

<h2 id="关闭交换分区"><a href="#关闭交换分区" class="headerlink" title="关闭交换分区"></a>关闭交换分区</h2><h2 id="如采用云服务，可略过此步骤，云服务默认禁止使用交换分区，如阿里云、腾讯云、华为云"><a href="#如采用云服务，可略过此步骤，云服务默认禁止使用交换分区，如阿里云、腾讯云、华为云" class="headerlink" title="如采用云服务，可略过此步骤，云服务默认禁止使用交换分区，如阿里云、腾讯云、华为云"></a>如采用云服务，可略过此步骤，云服务默认禁止使用交换分区，如阿里云、腾讯云、华为云</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">swapoff -a</span><br><span class="line">cp -p &#x2F;etc&#x2F;fstab &#x2F;etc&#x2F;fstab.bak$(date &#39;+%Y%m%d%H%M%S&#39;)</span><br><span class="line">sed -i &quot;s&#x2F;\&#x2F;dev\&#x2F;mapper\&#x2F;rhel-swap&#x2F;\#\&#x2F;dev\&#x2F;mapper\&#x2F;rhel-swap&#x2F;g&quot; &#x2F;etc&#x2F;fstab</span><br><span class="line">sed -i &quot;s&#x2F;\&#x2F;dev\&#x2F;mapper\&#x2F;centos-swap&#x2F;\#\&#x2F;dev\&#x2F;mapper\&#x2F;centos-swap&#x2F;g&quot; &#x2F;etc&#x2F;fstab</span><br><span class="line">mount -a</span><br></pre></td></tr></table></figure>

<h2 id="网桥包经IPTables"><a href="#网桥包经IPTables" class="headerlink" title="网桥包经IPTables"></a>网桥包经IPTables</h2><p>由于iptables被绕过而导致流量路由不正确的问题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;  &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">net.ipv4.ip_forward &#x3D; 1</span><br><span class="line">vm.swappiness&#x3D;0</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br></pre></td></tr></table></figure>

<h2 id="同步时间"><a href="#同步时间" class="headerlink" title="同步时间"></a>同步时间</h2><p>如时间不一致，会导致认证过期，提示cluster-info: x509: certificate has expired or is not yet valid</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp1.aliyun.com</span><br></pre></td></tr></table></figure>

<h2 id="开启IPVS"><a href="#开启IPVS" class="headerlink" title="开启IPVS"></a>开启IPVS</h2><p>kubernets 1.12 默认更改为ipvs</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install ipvsadm</span><br><span class="line"></span><br><span class="line">cat &gt; &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &lt;&lt;EOF</span><br><span class="line"> </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">ipvs_modules&#x3D;&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4&quot;</span><br><span class="line">for kernel_module in \$&#123;ipvs_modules&#125;; do</span><br><span class="line"> &#x2F;sbin&#x2F;modinfo -F filename \$&#123;kernel_module&#125; &gt; &#x2F;dev&#x2F;null 2&gt;&amp;1</span><br><span class="line"> if [ $? -eq 0 ]; then</span><br><span class="line"> &#x2F;sbin&#x2F;modprobe \$&#123;kernel_module&#125;</span><br><span class="line"> fi</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chmod 755 &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &amp;&amp; bash &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br></pre></td></tr></table></figure>


<h1 id="安装dockre-ce"><a href="#安装dockre-ce" class="headerlink" title="安装dockre ce"></a>安装dockre ce</h1><p>目前官方文档提供已支持docker 1806<br>Kubernetes 1.12.3版本暂时不支持最新的Docker 18.09</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 卸载原docker</span><br><span class="line">yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine</span><br><span class="line"># 安装docker</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line">yum install -y docker-ce</span><br><span class="line"></span><br><span class="line"># 增加加速器</span><br><span class="line">tee &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;bv55mwyn.mirror.aliyuncs.com&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 启动docker</span><br><span class="line">systemctl enable docker &amp;&amp; systemctl start docker</span><br></pre></td></tr></table></figure>

<h1 id="安装kubeadm"><a href="#安装kubeadm" class="headerlink" title="安装kubeadm"></a>安装kubeadm</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加阿里云仓库</span><br><span class="line">cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 安装kubeadm</span><br><span class="line">yum install -y kubelet-1.12.3 kubeadm-1.12.3 kubectl-1.12.3</span><br><span class="line"></span><br><span class="line"># 启动kubectl</span><br><span class="line">systemctl enable kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure>

<h1 id="安装所需镜像"><a href="#安装所需镜像" class="headerlink" title="安装所需镜像"></a>安装所需镜像</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 编写一个脚本执行，通过阿里云镜像服务拉取谷歌镜像并tag</span><br><span class="line">#如安装kubernetes-version&#x3D;v1.12.3，需要更改KUBE_VERSION&#x3D;v1.12.3</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">set -e</span><br><span class="line"></span><br><span class="line">KUBE_VERSION&#x3D;v1.12.3</span><br><span class="line">KUBE_PAUSE_VERSION&#x3D;3.1</span><br><span class="line">ETCD_VERSION&#x3D;3.2.24</span><br><span class="line">CORE_DNS_VERSION&#x3D;1.2.2</span><br><span class="line"></span><br><span class="line">GCR_URL&#x3D;k8s.gcr.io</span><br><span class="line">ALIYUN_URL&#x3D;registry.cn-shenzhen.aliyuncs.com&#x2F;hyman0603</span><br><span class="line"></span><br><span class="line">images&#x3D;(kube-proxy:$&#123;KUBE_VERSION&#125;</span><br><span class="line">kube-scheduler:$&#123;KUBE_VERSION&#125;</span><br><span class="line">kube-controller-manager:$&#123;KUBE_VERSION&#125;</span><br><span class="line">kube-apiserver:$&#123;KUBE_VERSION&#125;</span><br><span class="line">pause:$&#123;KUBE_PAUSE_VERSION&#125;</span><br><span class="line">etcd:$&#123;ETCD_VERSION&#125;</span><br><span class="line">coredns:$&#123;CORE_DNS_VERSION&#125;)</span><br><span class="line"></span><br><span class="line">for imageName in $&#123;images[@]&#125; ; do</span><br><span class="line">  docker pull $ALIYUN_URL&#x2F;$imageName</span><br><span class="line">  docker tag  $ALIYUN_URL&#x2F;$imageName $GCR_URL&#x2F;$imageName</span><br><span class="line">  docker rmi $ALIYUN_URL&#x2F;$imageName</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h1 id="安装kubernetes-1-12-3"><a href="#安装kubernetes-1-12-3" class="headerlink" title="安装kubernetes 1.12.3"></a>安装kubernetes 1.12.3</h1><p>选择了kubernetes-version=v1.12.3，kubernetes镜像文件也需要对应一致</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm init --kubernetes-version&#x3D;v1.12.3 --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 ----apiserver-advertise-address&#x3D;172.16.0.17</span><br></pre></td></tr></table></figure>

<h1 id="重置kubernets"><a href="#重置kubernets" class="headerlink" title="重置kubernets"></a>重置kubernets</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm reset</span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ip link delete cni0</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ip link delete flannel.1</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;cni&#x2F;</span><br></pre></td></tr></table></figure>

<h1 id="安装Pod-Network"><a href="#安装Pod-Network" class="headerlink" title="安装Pod Network"></a>安装Pod Network</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml</span><br><span class="line">kubectl apply -f  kube-flannel.yml</span><br><span class="line"></span><br><span class="line">#如果Node有多个网卡的话，参考[flannel issues 39701](https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes&#x2F;issues&#x2F;39701)，目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface&#x3D;&lt;iface-name&gt;</span><br><span class="line"></span><br><span class="line">containers:</span><br><span class="line">      - name: kube-flannel</span><br><span class="line">        image: quay.io&#x2F;coreos&#x2F;flannel:v0.10.0-amd64</span><br><span class="line">        command:</span><br><span class="line">        - &#x2F;opt&#x2F;bin&#x2F;flanneld</span><br><span class="line">        args:</span><br><span class="line">        - --ip-masq</span><br><span class="line">        - --kube-subnet-mgr</span><br><span class="line">        - --iface&#x3D;eth1</span><br></pre></td></tr></table></figure>

<h1 id="master-node参与工作负载"><a href="#master-node参与工作负载" class="headerlink" title="master node参与工作负载"></a>master node参与工作负载</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl describe node node1 | grep Taint</span><br><span class="line">Taints:             node-role.kubernetes.io&#x2F;master:NoSchedule</span><br><span class="line"></span><br><span class="line">kubectl taint nodes node1 node-role.kubernetes.io&#x2F;master-</span><br><span class="line">node &quot;node1&quot; untainted</span><br></pre></td></tr></table></figure>

<h1 id="测试DNS"><a href="#测试DNS" class="headerlink" title="测试DNS"></a>测试DNS</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl run curl --image&#x3D;radial&#x2F;busyboxplus:curl -it</span><br><span class="line">kubectl run --generator&#x3D;deployment&#x2F;apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.</span><br><span class="line">If you don&#39;t see a command prompt, try pressing enter.</span><br><span class="line">[ root@curl-5cc7b478b6-r997p:&#x2F; ]$ </span><br><span class="line"></span><br><span class="line">nslookup kubernetes.default</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kubernetes.default</span><br><span class="line">Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span><br></pre></td></tr></table></figure>

<h1 id="集群中移除Node"><a href="#集群中移除Node" class="headerlink" title="集群中移除Node"></a>集群中移除Node</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get nodes</span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION</span><br><span class="line">node1     Ready     master    26m       v1.12.0</span><br><span class="line">node2     Ready     &lt;none&gt;    2m        v1.12.0</span><br><span class="line"></span><br><span class="line">在master节点上执行：</span><br><span class="line">kubectl drain node2 --delete-local-data --force --ignore-daemonsets</span><br><span class="line">kubectl delete node node2</span><br><span class="line"></span><br><span class="line">在node2上执行：</span><br><span class="line">kubeadm reset</span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ip link delete cni0</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ip link delete flannel.1</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;cni&#x2F;</span><br><span class="line"></span><br><span class="line">在node1上执行：</span><br><span class="line"></span><br><span class="line">kubectl delete node node2</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS 7 安装pgAdmin4</title>
    <url>/posts/911ba59c.html</url>
    <content><![CDATA[<p>PgAdmin4是一个易于使用的Web界面，用于管理PostgreSQL数据库</p>
<a id="more"></a>
<h1 id="PgAdmin4"><a href="#PgAdmin4" class="headerlink" title="PgAdmin4"></a>PgAdmin4</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装PgAdmin4库</span><br><span class="line"></span><br><span class="line">yum -y install https:&#x2F;&#x2F;download.postgresql.org&#x2F;pub&#x2F;repos&#x2F;yum&#x2F;11&#x2F;redhat&#x2F;rhel-7-x86_64&#x2F;pgdg-redhat-repo-42.0-2.noarch.rpm</span><br><span class="line"></span><br><span class="line"># 安装PgAdmin4</span><br><span class="line">yum -y install pgadmin4</span><br></pre></td></tr></table></figure>

<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><p>为了让pgAdmin4运行，需要进行一些小的配置更改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建配置文件</span><br><span class="line">mv &#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;pgadmin4.conf.sample &#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;pgadmin4.conf</span><br><span class="line">保持默认配置即可</span><br><span class="line"></span><br><span class="line"># 创建日志和lib目录</span><br><span class="line">mkdir -p &#x2F;var&#x2F;lib&#x2F;pgadmin4&#x2F;</span><br><span class="line">mkdir -p &#x2F;var&#x2F;log&#x2F;pgadmin4&#x2F;</span><br><span class="line">chown -R 777 &#x2F;var&#x2F;lib&#x2F;pgadmin4</span><br><span class="line">chown -R 777 &#x2F;var&#x2F;log&#x2F;pgadmin4</span><br><span class="line"></span><br><span class="line"># 扩展config_distro.py</span><br><span class="line">vi &#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;pgadmin4-web&#x2F;config_distro.py</span><br><span class="line"></span><br><span class="line">并添加以下行</span><br><span class="line"></span><br><span class="line">LOG_FILE &#x3D; &#39;&#x2F;var&#x2F;log&#x2F;pgadmin4&#x2F;pgadmin4.log&#39;</span><br><span class="line">SQLITE_PATH &#x3D; &#39;&#x2F;var&#x2F;lib&#x2F;pgadmin4&#x2F;pgadmin4.db&#39;</span><br><span class="line">SESSION_DB_PATH &#x3D; &#39;&#x2F;var&#x2F;lib&#x2F;pgadmin4&#x2F;sessions&#39;</span><br><span class="line">STORAGE_DIR &#x3D; &#39;&#x2F;var&#x2F;lib&#x2F;pgadmin4&#x2F;storage&#39;</span><br><span class="line"></span><br><span class="line">#创建用户帐户，将在Web界面中进行身份验证</span><br><span class="line">python &#x2F;usr&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;pgadmin4-web&#x2F;setup.py</span><br><span class="line"></span><br><span class="line">输入邮箱密码即可</span><br></pre></td></tr></table></figure>

<h1 id="访问PgAdmin4"><a href="#访问PgAdmin4" class="headerlink" title="访问PgAdmin4"></a>访问PgAdmin4</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl restart httpd</span><br><span class="line"></span><br><span class="line">http:&#x2F;&#x2F;ip-address&#x2F;pgadmin4 或者 http:&#x2F;&#x2F;localhost&#x2F;pgadmin4</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>PgAdmin4</tag>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>interview-question</title>
    <url>/posts/16a61fe.html</url>
    <content><![CDATA[<h1 id="傲雷"><a href="#傲雷" class="headerlink" title="傲雷"></a>傲雷</h1><p>1、自我介绍</p>
<p>2、redis应用的场景</p>
<p>3、设计一个秒杀平台</p>
<p>4、安全问题</p>
<p>5、兴趣爱好（系统攻防？亮点?）</p>
]]></content>
  </entry>
  <entry>
    <title>k8s 1.17.x 高可用部署：keepalived + haproxy</title>
    <url>/posts/365dbdc6.html</url>
    <content><![CDATA[<h1 id="部署说明"><a href="#部署说明" class="headerlink" title="部署说明"></a>部署说明</h1><h2 id="堆叠ETCD"><a href="#堆叠ETCD" class="headerlink" title="堆叠ETCD"></a>堆叠ETCD</h2><p>每个master节点上运行一个apiserver和etcd, etcd只与本节点apiserver通信。</p>
<a id="more"></a>
<img src="/images/stacked-etcd.png" width="100%" height="100%">

<h2 id="外部ETCD"><a href="#外部ETCD" class="headerlink" title="外部ETCD"></a>外部ETCD</h2><p>etcd集群运行在单独的主机上，每个etcd都与apiserver节点通信。</p>
<img src="/images/external-etcd.png" width="100%" height="100%">


<h2 id="本次部署方案"><a href="#本次部署方案" class="headerlink" title="本次部署方案"></a>本次部署方案</h2><img src="/images/k8s-ha.png" width="100%" height="100%">


<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>Centos 7.6</p>
<p>相关文档：<br><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/</a><br><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/#external-etcd-topology">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/#external-etcd-topology</a></p>
<h2 id="etcd-集群"><a href="#etcd-集群" class="headerlink" title="etcd 集群"></a>etcd 集群</h2><h3 id="下载证书生成工具"><a href="#下载证书生成工具" class="headerlink" title="下载证书生成工具"></a>下载证书生成工具</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#etcd三台机器安装创建证书所需软件</span><br><span class="line">curl -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cfssl https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl_linux-amd64</span><br><span class="line">curl -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cfssljson https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssljson_linux-amd64</span><br><span class="line">curl -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cfssl-certinfo https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl-certinfo_linux-amd64</span><br><span class="line">#cfssl授权</span><br><span class="line">chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cfssl*</span><br></pre></td></tr></table></figure>

<h3 id="创建CA"><a href="#创建CA" class="headerlink" title="创建CA"></a>创建CA</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#以下操作在 etcd1 机器执行</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd</span><br><span class="line">cd &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd</span><br><span class="line">#创建 CA 配置文件（ca-config.json）</span><br><span class="line">#我们可以创建一个初始的ca-config.json文件，如：cfssl print-defaults config &gt; ca-config.json，然后对其进行修改。</span><br><span class="line"></span><br><span class="line">cat &gt;ca-config.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;signing&quot;: &#123;</span><br><span class="line">    &quot;default&quot;: &#123;</span><br><span class="line">      &quot;expiry&quot;: &quot;876000h&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;profiles&quot;: &#123;</span><br><span class="line">      &quot;etcd&quot;: &#123;</span><br><span class="line">        &quot;usages&quot;: [</span><br><span class="line">            &quot;signing&quot;,</span><br><span class="line">            &quot;key encipherment&quot;,</span><br><span class="line">            &quot;server auth&quot;,</span><br><span class="line">            &quot;client auth&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;expiry&quot;: &quot;876000h&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">#对上面的字段进行说明</span><br><span class="line">&quot;ca-config.json&quot;：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；</span><br><span class="line">&quot;signing&quot;：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA&#x3D;TRUE；</span><br><span class="line">&quot;server auth&quot;：表示client可以用该 CA 对server提供的证书进行验证；</span><br><span class="line">&quot;client auth&quot;：表示server可以用该CA对client提供的证书进行验证；</span><br><span class="line"></span><br><span class="line">#创建 CA 证书签名请求（ca-csr.json）</span><br><span class="line">cat &gt;ca-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;etcd&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;shanghai&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;shanghai&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;etcd&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">#对上面的字段进行说明</span><br><span class="line">&quot;CN&quot;：Common Name，etcd 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；</span><br><span class="line">&quot;O&quot;：Organization，etcd 从证书中提取该字段作为请求用户所属的组 (Group)；</span><br><span class="line">这两个参数在后面的kubernetes启用RBAC模式中很重要，因为需要设置kubelet、admin等角色权限，那么在配置证书的时候就必须配置对了，具体后面在部署kubernetes的时候会进行讲解。</span><br><span class="line">&quot;在etcd这两个参数没太大的重要意义，跟着配置就好。&quot;</span><br><span class="line"></span><br><span class="line">#生成 CA 证书和私钥</span><br><span class="line">cfssl gencert -initca ca-csr.json | cfssljson -bare ca</span><br><span class="line">#证书文件说明</span><br><span class="line">* 生成 &quot;ca.csr  ca-key.pem  ca.pem&quot; 三个文件</span><br><span class="line">* ca.pem 根证书公钥文件</span><br><span class="line">* ca-key.pem 根证书私钥文件</span><br><span class="line">* ca.csr 证书签名请求，用于交叉签名或重新签名</span><br><span class="line">* ca-config.json 使用cfssl工具生成其他类型证书需要引用的配置文件</span><br><span class="line">* ca.pem用于签发后续其他的证书文件，因此ca.pem文件需要分发到集群中的每台服务器上去</span><br></pre></td></tr></table></figure>

<h3 id="创建etcd证书"><a href="#创建etcd证书" class="headerlink" title="创建etcd证书"></a>创建etcd证书</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#创建etcd的TLS认证证书</span><br><span class="line">#创建 etcd证书签名请求（etcd-csr.json）</span><br><span class="line">cd &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd</span><br><span class="line">cat &gt; etcd-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;etcd&quot;,</span><br><span class="line">  &quot;hosts&quot;: [</span><br><span class="line">    &quot;10.168.4.5&quot;, </span><br><span class="line">    &quot;10.168.4.6&quot;, </span><br><span class="line">    &quot;10.168.4.12&quot;,</span><br><span class="line">    &quot;master01&quot;,</span><br><span class="line">    &quot;master02&quot;,</span><br><span class="line">    &quot;master03&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;shanghai&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;shanghai&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;etcd&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">#生成 etcd证书和私钥</span><br><span class="line">cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;etcd etcd-csr.json | cfssljson -bare etcd</span><br></pre></td></tr></table></figure>

<h3 id="etcd免密认证和证书拷贝"><a href="#etcd免密认证和证书拷贝" class="headerlink" title="etcd免密认证和证书拷贝"></a>etcd免密认证和证书拷贝</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#etcd三台机器执行</span><br><span class="line">#三台机器免密认证</span><br><span class="line">ssh-copy-id root@&lt;etcd1-ip-address&gt;</span><br><span class="line">ssh-copy-id root@&lt;etcd2-ip-address&gt;</span><br><span class="line">ssh-copy-id root@&lt;etcd3-ip-address&gt;</span><br><span class="line">#etcd2&amp;etcd3执行</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd</span><br><span class="line">cd &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd</span><br><span class="line">scp root@10.168.4.5:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem .</span><br><span class="line">scp root@10.168.4.5:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca-key.pem .</span><br><span class="line">scp root@10.168.4.5:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd.pem .</span><br><span class="line">scp root@10.168.4.5:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd-key.pem .</span><br><span class="line">scp root@10.168.4.5:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca-config.json .</span><br></pre></td></tr></table></figure>

<h3 id="etcd集群部署"><a href="#etcd集群部署" class="headerlink" title="etcd集群部署"></a>etcd集群部署</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#etcd三台机器安装etcd可执行文件</span><br><span class="line">mkdir -p &#x2F;data&#x2F;sys&#x2F;var&#x2F;etcd</span><br><span class="line">chmod -R 777 &#x2F;data&#x2F;sys&#x2F;var&#x2F;etcd</span><br><span class="line">ln -s &#x2F;data&#x2F;sys&#x2F;var&#x2F;etcd &#x2F;var&#x2F;lib&#x2F;etcd</span><br><span class="line">export ETCD_VERSION&#x3D;v3.4.4</span><br><span class="line">curl -sSL https:&#x2F;&#x2F;github.com&#x2F;coreos&#x2F;etcd&#x2F;releases&#x2F;download&#x2F;$&#123;ETCD_VERSION&#125;&#x2F;etcd-$&#123;ETCD_VERSION&#125;-linux-amd64.tar.gz </span><br><span class="line">| tar -xzv --strip-components&#x3D;1 -C &#x2F;usr&#x2F;local&#x2F;bin&#x2F;</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line">export ETCD_VER&#x3D;v3.4.4</span><br><span class="line">curl -L https:&#x2F;&#x2F;storage.googleapis.com&#x2F;etcd&#x2F;$&#123;ETCD_VER&#125;&#x2F;etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#配置etcd三台机器执行</span><br><span class="line">#创建etcd环境配置文件</span><br><span class="line">touch &#x2F;etc&#x2F;etcd.env</span><br><span class="line">echo &quot;PEER_NAME&#x3D;master01&quot; &gt;&gt; &#x2F;etc&#x2F;etcd.env #另外两台就是master02&#x2F;03</span><br><span class="line">echo &quot;PRIVATE_IP&#x3D;10.168.4.5&quot; &gt;&gt; &#x2F;etc&#x2F;etcd.env #另外两台就是10.168.4.6&#x2F;10.168.4.12</span><br><span class="line">cat &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;etcd.service </span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;etcd</span><br><span class="line">Documentation&#x3D;https:&#x2F;&#x2F;github.com&#x2F;coreos&#x2F;etcd</span><br><span class="line">Conflicts&#x3D;etcd.service</span><br><span class="line">Conflicts&#x3D;etcd2.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile&#x3D;&#x2F;etc&#x2F;etcd.env</span><br><span class="line">Type&#x3D;notify</span><br><span class="line">Restart&#x3D;always</span><br><span class="line">RestartSec&#x3D;5s</span><br><span class="line">LimitNOFILE&#x3D;40000</span><br><span class="line">TimeoutStartSec&#x3D;0</span><br><span class="line"></span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;etcd --name $&#123;PEER_NAME&#125; \</span><br><span class="line">    --data-dir &#x2F;var&#x2F;lib&#x2F;etcd \</span><br><span class="line">    --listen-client-urls https:&#x2F;&#x2F;$&#123;PRIVATE_IP&#125;:2379 \</span><br><span class="line">    --advertise-client-urls https:&#x2F;&#x2F;$&#123;PRIVATE_IP&#125;:2379 \</span><br><span class="line">    --listen-peer-urls https:&#x2F;&#x2F;$&#123;PRIVATE_IP&#125;:2380 \</span><br><span class="line">    --initial-advertise-peer-urls https:&#x2F;&#x2F;$&#123;PRIVATE_IP&#125;:2380 \</span><br><span class="line">    --cert-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd.pem \</span><br><span class="line">    --key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd-key.pem \</span><br><span class="line">    --client-cert-auth \</span><br><span class="line">    --trusted-ca-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem \</span><br><span class="line">    --peer-cert-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd.pem \</span><br><span class="line">    --peer-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd-key.pem \</span><br><span class="line">    --peer-client-cert-auth \</span><br><span class="line">    --peer-trusted-ca-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem \</span><br><span class="line">    --initial-cluster &lt;etcd1&gt;&#x3D;https:&#x2F;&#x2F;&lt;etcd1-ip-address&gt;:2380,&lt;etcd2&gt;&#x3D;https:&#x2F;&#x2F;&lt;etcd2-ip-address&gt;:2380,&lt;etcd3&gt;&#x3D;https:&#x2F;&#x2F;&lt;etcd3-ip-address&gt;:2380 \</span><br><span class="line">    --initial-cluster-token my-etcd-token \</span><br><span class="line">    --initial-cluster-state new</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br><span class="line"></span><br><span class="line">说明：</span><br><span class="line">* 将&lt;etcd1&gt;&lt;etcd2&gt;&lt;etcd3&gt;改为对应节点的hostname</span><br><span class="line">* 将&lt;etcd1-ip-address&gt;&lt;etcd2-ip-address&gt;&lt;etcd3-ip-address&gt;改为对应节点的通讯ip</span><br><span class="line"></span><br><span class="line">#启动etcd集群</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start etcd</span><br><span class="line">systemctl enable etcd</span><br><span class="line">systemctl status etcd -l</span><br><span class="line"></span><br><span class="line">#etcd集群服务的信息</span><br><span class="line">mkdir &#x2F;etc&#x2F;kubernetes&#x2F;scripts</span><br><span class="line">cd &#x2F;etc&#x2F;kubernetes&#x2F;scripts</span><br><span class="line">cat etcd.sh </span><br><span class="line">HOST_1&#x3D;10.168.4.5</span><br><span class="line">HOST_2&#x3D;10.168.4.6</span><br><span class="line">HOST_3&#x3D;10.168.4.12</span><br><span class="line">ENDPOINTS&#x3D;$HOST_1:2379,$HOST_2:2379,$HOST_3:2379</span><br><span class="line">#etcd集群健康信息</span><br><span class="line">etcdctl --endpoints&#x3D;$ENDPOINTS --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd.pem  --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd-key.pem endpoint health</span><br><span class="line">#etcd集群状态信息</span><br><span class="line">etcdctl --endpoints&#x3D;$ENDPOINTS --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd.pem  --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd-key.pem --write-out&#x3D;table endpoint status</span><br><span class="line">#etcd集群成员信息</span><br><span class="line">etcdctl --endpoints&#x3D;$ENDPOINTS --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd.pem  --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd-key.pem member list -w table</span><br><span class="line"></span><br><span class="line">#执行上面脚本打印如下sh etcd.sh</span><br><span class="line">[root@master01 scripts]# sh etcd.sh </span><br><span class="line">10.168.4.5:2379 is healthy: successfully committed proposal: took &#x3D; 12.350574ms</span><br><span class="line">10.168.4.6:2379 is healthy: successfully committed proposal: took &#x3D; 11.83104ms</span><br><span class="line">10.168.4.12:2379 is healthy: successfully committed proposal: took &#x3D; 13.326317ms</span><br><span class="line">+------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|     ENDPOINT     |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|  10.168.4.5:2379 | 7db6f9384bffe8b2 |   3.4.4 |   20 kB |     false |      false |        76 |         52 |                 52 |        |</span><br><span class="line">|  10.168.4.6:2379 | bada5b5a99674a15 |   3.4.4 |   20 kB |      true |      false |        76 |         52 |                 52 |        |</span><br><span class="line">| 10.168.4.12:2379 | d54e4be0d69c6952 |   3.4.4 |   16 kB |     false |      false |        76 |         52 |                 52 |        |</span><br><span class="line">+------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">+------------------+---------+----------+--------------------------+--------------------------+------------+</span><br><span class="line">|        ID        | STATUS  |   NAME   |        PEER ADDRS        |       CLIENT ADDRS       | IS LEARNER |</span><br><span class="line">+------------------+---------+----------+--------------------------+--------------------------+------------+</span><br><span class="line">| 7db6f9384bffe8b2 | started | master03 |  https:&#x2F;&#x2F;10.168.4.5:2380 | https:&#x2F;&#x2F;10.168.4.12:2379 |      false |</span><br><span class="line">| bada5b5a99674a15 | started | master02 |  https:&#x2F;&#x2F;10.168.4.6:2380 |  https:&#x2F;&#x2F;10.168.4.6:2379 |      false |</span><br><span class="line">| d54e4be0d69c6952 | started | master03 | https:&#x2F;&#x2F;10.168.4.12:2380 | https:&#x2F;&#x2F;10.168.4.12:2379 |      false |</span><br><span class="line">+------------------+---------+----------+--------------------------+--------------------------+------------+</span><br></pre></td></tr></table></figure>

<h2 id="部署keepalived"><a href="#部署keepalived" class="headerlink" title="部署keepalived"></a>部署keepalived</h2><p>三台master机器均是如下操作</p>
<h3 id="安装keepalived"><a href="#安装keepalived" class="headerlink" title="安装keepalived"></a>安装keepalived</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y keepalived</span><br></pre></td></tr></table></figure>

<h3 id="配置keepalived"><a href="#配置keepalived" class="headerlink" title="配置keepalived"></a>配置keepalived</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;etc&#x2F;keepalived</span><br><span class="line">mv keepalived.conf keepalived.conf_bak</span><br><span class="line">cat &gt; &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf &lt;&lt; EOF</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id LVS_DEVEL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script check_haproxy &#123;</span><br><span class="line">    script &quot;killall -0 haproxy&quot;</span><br><span class="line">    interval 3</span><br><span class="line">    weight -2</span><br><span class="line">    fall 10</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER</span><br><span class="line">    interface ens33</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 250  #优先级保持唯一，这里master01为250，master02为200，master03为150</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 35f18af7190d51c9f7f78f37300a0cbd</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.246.200</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_haproxy</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">#上面配置文件说明</span><br><span class="line"></span><br><span class="line">*记得修改上面配置文件priority</span><br><span class="line">* killall -0 根据进程名称检测进程是否存活</span><br><span class="line">* master01节点为***MASTER***，其余节点为***BACKUP***</span><br><span class="line">* priority各个几点到优先级相差50，范围：0～250（非强制要求），数值越大优先级越高～</span><br></pre></td></tr></table></figure>

<h3 id="启动并检测服务"><a href="#启动并检测服务" class="headerlink" title="启动并检测服务"></a>启动并检测服务</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable keepalived.service </span><br><span class="line">systemctl start keepalived.service</span><br><span class="line">systemctl status keepalived.service </span><br><span class="line">#我们在master01主节点上，看下ip信息</span><br><span class="line">[root@master01 ~]# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1&#x2F;8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link&#x2F;ether 52:54:00:cc:c3:63 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.168.4.5&#x2F;24 brd 10.168.4.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.168.4.100&#x2F;32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default </span><br><span class="line">    link&#x2F;ether 02:42:9a:96:0c:fd brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1&#x2F;16 brd 172.17.255.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<h2 id="部署haproxy"><a href="#部署haproxy" class="headerlink" title="部署haproxy"></a>部署haproxy</h2><p>三台机器均是如下步骤</p>
<h3 id="安装HaProxy"><a href="#安装HaProxy" class="headerlink" title="安装HaProxy"></a>安装HaProxy</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y haproxy</span><br></pre></td></tr></table></figure>

<h3 id="配置haproxy"><a href="#配置haproxy" class="headerlink" title="配置haproxy"></a>配置haproxy</h3><figure class="highlight plain"><figcaption><span>/etc/haproxy</span></figcaption><table><tr><td class="code"><pre><span class="line">mv haproxy.cfg haproxy.cfg_bak</span><br><span class="line">cat &gt; &#x2F;etc&#x2F;haproxy&#x2F;haproxy.cfg &lt;&lt; EOF</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># Global settings</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">global</span><br><span class="line">    # to have these messages end up in &#x2F;var&#x2F;log&#x2F;haproxy.log you will</span><br><span class="line">    # need to:</span><br><span class="line">    #</span><br><span class="line">    # 1) configure syslog to accept network log events.  This is done</span><br><span class="line">    #    by adding the &#39;-r&#39; option to the SYSLOGD_OPTIONS in</span><br><span class="line">    #    &#x2F;etc&#x2F;sysconfig&#x2F;syslog</span><br><span class="line">    #</span><br><span class="line">    # 2) configure local2 events to go to the &#x2F;var&#x2F;log&#x2F;haproxy.log</span><br><span class="line">    #   file. A line like the following can be added to</span><br><span class="line">    #   &#x2F;etc&#x2F;sysconfig&#x2F;syslog</span><br><span class="line">    #</span><br><span class="line">    #    local2.*                       &#x2F;var&#x2F;log&#x2F;haproxy.log</span><br><span class="line">    #</span><br><span class="line">    log         127.0.0.1 local2</span><br><span class="line"></span><br><span class="line">    chroot      &#x2F;var&#x2F;lib&#x2F;haproxy</span><br><span class="line">    pidfile     &#x2F;var&#x2F;run&#x2F;haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon</span><br><span class="line"></span><br><span class="line">    # turn on stats unix socket</span><br><span class="line">    stats socket &#x2F;var&#x2F;lib&#x2F;haproxy&#x2F;stats</span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># common defaults that all the &#39;listen&#39; and &#39;backend&#39; sections will</span><br><span class="line"># use if not designated in their block</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">defaults</span><br><span class="line">    mode                    http</span><br><span class="line">    log                     global</span><br><span class="line">    option                  httplog</span><br><span class="line">    option                  dontlognull</span><br><span class="line">    option http-server-close</span><br><span class="line">    option forwardfor       except 127.0.0.0&#x2F;8</span><br><span class="line">    option                  redispatch</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout http-request    10s</span><br><span class="line">    timeout queue           1m</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line">    timeout http-keep-alive 10s</span><br><span class="line">    timeout check           10s</span><br><span class="line">    maxconn                 3000</span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># kubernetes apiserver frontend which proxys to the backends</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">frontend kubernetes-apiserver</span><br><span class="line">    mode                 tcp</span><br><span class="line">    bind                 *:16443</span><br><span class="line">    option               tcplog</span><br><span class="line">    default_backend      kubernetes-apiserver</span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># round robin balancing between the various backends</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">backend kubernetes-apiserver</span><br><span class="line">    mode        tcp</span><br><span class="line">    balance     roundrobin</span><br><span class="line">    server  master01 10.168.4.5:6443  check   #更改对应的主机名和IP</span><br><span class="line">    server  master02 10.168.4.6:6443  check   #更改对应的主机名和IP</span><br><span class="line">    server  master03 10.168.4.12:6443 check   #更改对应的主机名和IP</span><br><span class="line"></span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line"># collection haproxy statistics message</span><br><span class="line">#---------------------------------------------------------------------</span><br><span class="line">listen stats</span><br><span class="line">    bind                 *:1080</span><br><span class="line">    stats auth           admin:awesomePassword</span><br><span class="line">    stats refresh        5s</span><br><span class="line">    stats realm          HAProxy\ Statistics</span><br><span class="line">    stats uri            &#x2F;admin?stats</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">#说明：</span><br><span class="line">* 所有master节点上的配置完全相同</span><br></pre></td></tr></table></figure>

<h3 id="启动并检测服务-1"><a href="#启动并检测服务-1" class="headerlink" title="启动并检测服务"></a>启动并检测服务</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable haproxy.service </span><br><span class="line">systemctl start haproxy.service </span><br><span class="line">systemctl status haproxy.service </span><br><span class="line">ss -lnt | grep -E &quot;16443|1080&quot;</span><br><span class="line">LISTEN     0      128          *:1080                     *:*</span><br><span class="line">LISTEN     0      128          *:16443                    *:*</span><br></pre></td></tr></table></figure>

<h2 id="安装kubeadm、kubectl、kubelet"><a href="#安装kubeadm、kubectl、kubelet" class="headerlink" title="安装kubeadm、kubectl、kubelet"></a>安装kubeadm、kubectl、kubelet</h2><p>三台机器均是如下操作</p>
<h3 id="设置kubernetes的yum源"><a href="#设置kubernetes的yum源" class="headerlink" title="设置kubernetes的yum源"></a>设置kubernetes的yum源</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum clean all</span><br><span class="line">yum makecache fast</span><br></pre></td></tr></table></figure>

<h3 id="安装kubelet-kubeadm-kubectl"><a href="#安装kubelet-kubeadm-kubectl" class="headerlink" title="安装kubelet kubeadm kubectl"></a>安装kubelet kubeadm kubectl</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y kubelet kubeadm kubectl --disableexcludes&#x3D;kubernetes</span><br></pre></td></tr></table></figure>

<h3 id="kubelet-service设置开机启动"><a href="#kubelet-service设置开机启动" class="headerlink" title="kubelet.service设置开机启动"></a>kubelet.service设置开机启动</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable kubelet.service</span><br><span class="line"></span><br><span class="line">注意：这一步不能直接执行 systemctl start kubelet，会报错，成功初始化完后kubelet会自动起来</span><br></pre></td></tr></table></figure>

<h3 id="编辑hosts文件，添加如下内容"><a href="#编辑hosts文件，添加如下内容" class="headerlink" title="编辑hosts文件，添加如下内容"></a>编辑hosts文件，添加如下内容</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &#x2F;etc&#x2F;hosts</span><br><span class="line">10.168.4.100   cluster.kube.com</span><br><span class="line">10.168.4.5     master01</span><br><span class="line">10.168.4.6     master02</span><br><span class="line">10.168.4.12    master03</span><br></pre></td></tr></table></figure>

<h2 id="初始化第一个master节点"><a href="#初始化第一个master节点" class="headerlink" title="初始化第一个master节点"></a>初始化第一个master节点</h2><p>以下操作在master01节点进行</p>
<h3 id="编辑kubeadm配置文件"><a href="#编辑kubeadm配置文件" class="headerlink" title="编辑kubeadm配置文件"></a>编辑kubeadm配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kubernetes&#x2F;my-conf</span><br><span class="line">cd &#x2F;etc&#x2F;kubernetes&#x2F;my-conf</span><br><span class="line"></span><br><span class="line">cat &gt;config.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1beta2</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: 1.17.3</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers</span><br><span class="line">etcd:</span><br><span class="line">    external:</span><br><span class="line">        endpoints:</span><br><span class="line">        - https:&#x2F;&#x2F;10.168.4.5:2379</span><br><span class="line">        - https:&#x2F;&#x2F;10.168.4.6:2379</span><br><span class="line">        - https:&#x2F;&#x2F;10.168.4.12:2379</span><br><span class="line">        caFile: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem</span><br><span class="line">        certFile: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd.pem</span><br><span class="line">        keyFile: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd-key.pem</span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.244.0.0&#x2F;16</span><br><span class="line">#apiServer:</span><br><span class="line">#  certSANs:</span><br><span class="line">#    - &quot;cluster.kube.com&quot;</span><br><span class="line">controlPlaneEndpoint: &quot;cluster.kube.com:16443&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h3 id="启动集群，获得返回命令用来加入集群"><a href="#启动集群，获得返回命令用来加入集群" class="headerlink" title="启动集群，获得返回命令用来加入集群"></a>启动集群，获得返回命令用来加入集群</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm init --config&#x3D;config.yaml --upload-certs</span><br></pre></td></tr></table></figure>

<p>注意下面初始化成功之后的信息，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME&#x2F;.kube</span><br><span class="line">  sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;</span><br><span class="line"></span><br><span class="line">You can now join any number of the control-plane node running the following command on each as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join cluster.lolaage.com:6443 --token 9t4vc1.9q2mrf0cwmnpqmxx \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:8c0cb820a823e897ace35b73a5b50e05bb79d8b0e1998aab0e7d138ca8005f45 \</span><br><span class="line">    --control-plane --certificate-key 19d77056eba525205d134e92d3e92cf872aa03a4e17593dd25933f9ec8824375</span><br><span class="line"></span><br><span class="line">Please note that the certificate-key gives access to cluster sensitive data, keep it secret!</span><br><span class="line">As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use</span><br><span class="line">&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join cluster.lolaage.com:6443 --token 9t4vc1.9q2mrf0cwmnpqmxx \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:8c0cb820a823e897ace35b73a5b50e05bb79d8b0e1998aab0e7d138ca8005f45</span><br></pre></td></tr></table></figure>

<h3 id="认证linux用户操作权限"><a href="#认证linux用户操作权限" class="headerlink" title="认证linux用户操作权限"></a>认证linux用户操作权限</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure>

<h3 id="查看节点"><a href="#查看节点" class="headerlink" title="查看节点"></a>查看节点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 my-conf]# kubectl get node</span><br><span class="line">NAME       STATUS     ROLES    AGE    VERSION</span><br><span class="line">master01   NotReady   master   2m5s   v1.17.3</span><br></pre></td></tr></table></figure>

<h3 id="查看集群状态"><a href="#查看集群状态" class="headerlink" title="查看集群状态"></a>查看集群状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 my-conf]# kubectl get cs</span><br><span class="line">NAME                 STATUS    MESSAGE             ERROR</span><br><span class="line">scheduler            Healthy   ok                  </span><br><span class="line">controller-manager   Healthy   ok                  </span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line">etcd-1               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   </span><br><span class="line">etcd-2               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="动态查看-kube-system-命名空间下的pod"><a href="#动态查看-kube-system-命名空间下的pod" class="headerlink" title="动态查看 kube-system 命名空间下的pod"></a>动态查看 kube-system 命名空间下的pod</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 my-conf]# kubectl get pod -n kube-system -o wide -w</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE     IP           NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-7f9c544f75-g6tsj           0&#x2F;1     Pending   0          3m4s    &lt;none&gt;       &lt;none&gt;     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-7f9c544f75-n2c2f           0&#x2F;1     Pending   0          3m4s    &lt;none&gt;       &lt;none&gt;     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-master01            1&#x2F;1     Running   0          2m59s   10.168.4.5   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-master01   1&#x2F;1     Running   0          2m59s   10.168.4.5   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-kcrzj                   1&#x2F;1     Running   0          3m4s    10.168.4.5   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-master01            1&#x2F;1     Running   0          2m58s   10.168.4.5   master01   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h3 id="执行命令查看kubeadmin的配置"><a href="#执行命令查看kubeadmin的配置" class="headerlink" title="执行命令查看kubeadmin的配置"></a>执行命令查看kubeadmin的配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 my-conf]# kubeadm config view </span><br><span class="line">apiServer:</span><br><span class="line">  extraArgs:</span><br><span class="line">    authorization-mode: Node,RBAC</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1beta2</span><br><span class="line">certificatesDir: &#x2F;etc&#x2F;kubernetes&#x2F;pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controlPlaneEndpoint: cluster.lolaage.com:6443</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns:</span><br><span class="line">  type: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  external:</span><br><span class="line">    caFile: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem</span><br><span class="line">    certFile: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd.pem</span><br><span class="line">    endpoints:</span><br><span class="line">    - https:&#x2F;&#x2F;10.168.4.5:2379</span><br><span class="line">    - https:&#x2F;&#x2F;10.168.4.6:2379</span><br><span class="line">    - https:&#x2F;&#x2F;10.168.4.12:2379</span><br><span class="line">    keyFile: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd-key.pem</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.17.3</span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  podSubnet: 10.244.0.0&#x2F;16</span><br><span class="line">  serviceSubnet: 10.96.0.0&#x2F;12</span><br><span class="line">scheduler: &#123;&#125;</span><br></pre></td></tr></table></figure>

<h2 id="master02-amp-master03执行"><a href="#master02-amp-master03执行" class="headerlink" title="master02&amp;master03执行"></a>master02&amp;master03执行</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># master01执行</span><br><span class="line">scp &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt 10.168.4.6:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;</span><br><span class="line">scp &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key 10.168.4.6:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;</span><br><span class="line">scp &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;sa.key 10.168.4.6:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;</span><br><span class="line">scp &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;sa.pub 10.168.4.6:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;</span><br><span class="line"></span><br><span class="line">scp &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt 10.168.4.12:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;</span><br><span class="line">scp &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key 10.168.4.12:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;</span><br><span class="line">scp &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;sa.key 10.168.4.12:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;</span><br><span class="line">scp &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;sa.pub 10.168.4.12:&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;</span><br><span class="line"></span><br><span class="line"># 在master02&amp;master03执行</span><br><span class="line">kubeadm join cluster.lolaage.com:6443 --token 9t4vc1.9q2mrf0cwmnpqmxx \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:8c0cb820a823e897ace35b73a5b50e05bb79d8b0e1998aab0e7d138ca8005f45 \</span><br><span class="line">    --control-plane --certificate-key 19d77056eba525205d134e92d3e92cf872aa03a4e17593dd25933f9ec8824375</span><br></pre></td></tr></table></figure>

<h2 id="部署网络插件"><a href="#部署网络插件" class="headerlink" title="部署网络插件"></a>部署网络插件</h2><p>在 master01 节点部署插件</p>
<h3 id="使用calico网络插件"><a href="#使用calico网络插件" class="headerlink" title="使用calico网络插件"></a>使用calico网络插件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;my.conf&#x2F;network-utils</span><br><span class="line"></span><br><span class="line">curl -o &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;my.conf&#x2F;network-utils&#x2F;rbac-kdd.yaml  https:&#x2F;&#x2F;docs.projectcalico.org&#x2F;v3.3&#x2F;getting-started&#x2F;kubernetes&#x2F;installation&#x2F;hosted&#x2F;rbac-kdd.yaml</span><br><span class="line"></span><br><span class="line">curl -o &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;my.conf&#x2F;network-utils&#x2F;calico-3.9.2.yaml  https:&#x2F;&#x2F;kuboard.cn&#x2F;install-script&#x2F;calico&#x2F;calico-3.9.2.yaml</span><br><span class="line"></span><br><span class="line">kubectl apply -f &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;my.conf&#x2F;network-utils&#x2F;rbac-kdd.yaml</span><br><span class="line"></span><br><span class="line">sed -i &quot;s#192\.168\.0\.0&#x2F;16#$&#123;POD_SUBNET&#125;#&quot; &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;my.conf&#x2F;network-utils&#x2F;calico-3.9.2.yaml</span><br><span class="line"></span><br><span class="line">kubectl apply -f &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;my.conf&#x2F;network-utils&#x2F;calico-3.9.2.yaml</span><br></pre></td></tr></table></figure>

<h3 id="使用flannel网络插件"><a href="#使用flannel网络插件" class="headerlink" title="使用flannel网络插件"></a>使用flannel网络插件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -o &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;my.conf&#x2F;network-utils&#x2F;kube-flannel.yml https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml</span><br><span class="line"></span><br><span class="line">kubectl apply -f &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;my.conf&#x2F;network-utils&#x2F;kube-flannel.yml</span><br></pre></td></tr></table></figure>

<h3 id="查看集群节点状态"><a href="#查看集群节点状态" class="headerlink" title="查看集群节点状态"></a>查看集群节点状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 my-conf]# kubectl get nodes -o wide</span><br><span class="line">NAME       STATUS     ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">master01   Ready      master   60m   v1.17.3   10.168.4.5    &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker:&#x2F;&#x2F;18.9.7</span><br><span class="line">master02   NotReady   master   49m   v1.17.3   10.168.4.6    &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.14.4.el7.x86_64   docker:&#x2F;&#x2F;18.9.7</span><br><span class="line">master03   Ready      master   49m   v1.17.3   10.168.4.12   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.14.4.el7.x86_64   docker:&#x2F;&#x2F;18.6.3</span><br></pre></td></tr></table></figure>
<h3 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 my-conf]# kubectl get pod -n kube-system -o wide</span><br><span class="line">NAME                                      READY   STATUS              RESTARTS   AGE   IP              NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">calico-kube-controllers-dc6cb64cb-x5xdx   1&#x2F;1     Running             0          20m   10.244.241.67   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">calico-node-7mn66                         1&#x2F;1     Running             0          20m   10.168.4.5      master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">calico-node-gmvpv                         0&#x2F;1     Init:ErrImagePull   0          12m   10.168.4.6      master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">calico-node-vj264                         1&#x2F;1     Running             0          20m   10.168.4.12     master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-7f9c544f75-g6tsj                  1&#x2F;1     Running             0          63m   10.244.241.65   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-7f9c544f75-n2c2f                  1&#x2F;1     Running             0          63m   10.244.241.66   master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-master01                   1&#x2F;1     Running             0          63m   10.168.4.5      master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-master02                   1&#x2F;1     Running             1          52m   10.168.4.6      master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-master03                   1&#x2F;1     Running             0          52m   10.168.4.12     master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-master01          1&#x2F;1     Running             0          63m   10.168.4.5      master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-master02          1&#x2F;1     Running             1          52m   10.168.4.6      master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-master03          1&#x2F;1     Running             0          52m   10.168.4.12     master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-4k4fb                          1&#x2F;1     Running             0          52m   10.168.4.12     master03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-kcrzj                          1&#x2F;1     Running             0          63m   10.168.4.5      master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-r4bsq                          1&#x2F;1     Running             1          52m   10.168.4.6      master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-master01                   1&#x2F;1     Running             0          63m   10.168.4.5      master01   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-master02                   1&#x2F;1     Running             1          52m   10.168.4.6      master02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-master03                   1&#x2F;1     Running             0          52m   10.168.4.12     master03   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h2 id="calicoctl常用命令"><a href="#calicoctl常用命令" class="headerlink" title="calicoctl常用命令"></a>calicoctl常用命令</h2><h3 id="下载和安装calicoctl工具，注意calico版本"><a href="#下载和安装calicoctl工具，注意calico版本" class="headerlink" title="下载和安装calicoctl工具，注意calico版本"></a>下载和安装calicoctl工具，注意calico版本</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;local&#x2F;bin</span><br><span class="line">curl -O -L https:&#x2F;&#x2F;github.com&#x2F;projectcalico&#x2F;calicoctl&#x2F;releases&#x2F;download&#x2F;v3.9.2&#x2F;calicoctl</span><br><span class="line">chmod +x calicoctl</span><br></pre></td></tr></table></figure>

<h3 id="查看网络节点"><a href="#查看网络节点" class="headerlink" title="查看网络节点"></a>查看网络节点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">calicoctl get node</span><br></pre></td></tr></table></figure>

<h3 id="节点网络状态"><a href="#节点网络状态" class="headerlink" title="节点网络状态"></a>节点网络状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">calicoctl node status</span><br></pre></td></tr></table></figure>

<h3 id="使用calicoctl工具查看有问题的节点"><a href="#使用calicoctl工具查看有问题的节点" class="headerlink" title="使用calicoctl工具查看有问题的节点"></a>使用calicoctl工具查看有问题的节点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">calicoctl node status</span><br></pre></td></tr></table></figure>

<p>#使用calicoctl工具来对calico进行更改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看问题节点的yaml文件</span><br><span class="line">calicoctl get node master03 -o yaml</span><br><span class="line"></span><br><span class="line">calicoctl get node master03 -o yaml &gt; calicomaster03.yaml</span><br><span class="line"></span><br><span class="line">calicoctl apply -f calicomaster03.yaml</span><br></pre></td></tr></table></figure>

<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="etcd-member-d54e4be0d69c6952-has-already-been-bootstrapped"><a href="#etcd-member-d54e4be0d69c6952-has-already-been-bootstrapped" class="headerlink" title="etcd: member d54e4be0d69c6952 has already been bootstrapped"></a>etcd: member d54e4be0d69c6952 has already been bootstrapped</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">其中一个成员是通过discovery service引导的。必须删除以前的数据目录来清理成员信息。否则成员将忽略新配置，使用旧配置。</span><br><span class="line"></span><br><span class="line">第一种方式我们可以通过修改启动参数解决这类错误。</span><br><span class="line">--initial-cluster-state&#x3D;existing \  **# 将new这个参数修改成existing，启动正常！**</span><br><span class="line"></span><br><span class="line">第二种方式删除所有etcd节点的 data-dir 文件（不删也行），重启各个节点的etcd服务</span><br><span class="line"></span><br><span class="line">第三种方式是复制其他节点的data-dir中的内容，以此为基础上以 --force-new-cluster 的形式强行拉起一个，然后以添加新成员的方式恢复这个集群。</span><br></pre></td></tr></table></figure>

<h3 id="忘记-join-token"><a href="#忘记-join-token" class="headerlink" title="忘记 join token"></a>忘记 join token</h3><p>获取方法：在master01上执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br><span class="line"></span><br><span class="line">说明：默认情况下，通过kubeadm create token创建的 token ，过期时间是24小时。可以运行 kubeadm token create --ttl 0生成一个永不过期的 token，参考文档[https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;reference&#x2F;setup-tools&#x2F;kubeadm&#x2F;kubeadm-token&#x2F;](https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;reference&#x2F;setup-tools&#x2F;kubeadm&#x2F;kubeadm-token&#x2F;)。</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernets</tag>
      </tags>
  </entry>
  <entry>
    <title>部署Jenkins</title>
    <url>/posts/854a2ed6.html</url>
    <content><![CDATA[<h1 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h1><h2 id="下载jdk"><a href="#下载jdk" class="headerlink" title="下载jdk"></a>下载jdk</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.oracle.com&#x2F;technetwork&#x2F;java&#x2F;javase&#x2F;downloads&#x2F;jdk8-downloads-2133151.html</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="环境变量-注意版本号"><a href="#环境变量-注意版本号" class="headerlink" title="环境变量(注意版本号)"></a>环境变量(注意版本号)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8.0_221</span><br><span class="line">export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;rt.jar:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar </span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br><span class="line"></span><br><span class="line">生效命令：source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>

<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure>

<h2 id="yum-安装-java"><a href="#yum-安装-java" class="headerlink" title="yum 安装 java"></a>yum 安装 java</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install java-1.8.0-openjdk* -y</span><br></pre></td></tr></table></figure>

<h1 id="Jenkins"><a href="#Jenkins" class="headerlink" title="Jenkins"></a>Jenkins</h1><h2 id="安装jenkins仓库"><a href="#安装jenkins仓库" class="headerlink" title="安装jenkins仓库"></a>安装jenkins仓库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;jenkins.repo https:&#x2F;&#x2F;pkg.jenkins.io&#x2F;redhat-stable&#x2F;jenkins.repo</span><br><span class="line">sudo rpm --import https:&#x2F;&#x2F;pkg.jenkins.io&#x2F;redhat-stable&#x2F;jenkins.io.key</span><br></pre></td></tr></table></figure>

<h2 id="安装jenkins"><a href="#安装jenkins" class="headerlink" title="安装jenkins"></a>安装jenkins</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install jenkins</span><br></pre></td></tr></table></figure>

<h2 id="启动和停止"><a href="#启动和停止" class="headerlink" title="启动和停止"></a>启动和停止</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start jenkins</span><br><span class="line">systemctl stop jenkins</span><br></pre></td></tr></table></figure>

<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>如启动找不到java，可修改文件vi /etc/init.d/jenkins 大约82行，添加java路劲即可</p>
<h2 id="修改目录"><a href="#修改目录" class="headerlink" title="修改目录"></a>修改目录</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;sysconfig&#x2F;jenkins</span><br><span class="line"></span><br><span class="line">10行</span><br><span class="line">JENKINS_HOME&#x3D;&quot;&#x2F;data&#x2F;jenkins&quot;</span><br></pre></td></tr></table></figure>

<h2 id="修改启动用户"><a href="#修改启动用户" class="headerlink" title="修改启动用户"></a>修改启动用户</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;sysconfig&#x2F;jenkins</span><br><span class="line"></span><br><span class="line">29行</span><br><span class="line">JENKINS_USER&#x3D;&quot;root&quot;</span><br></pre></td></tr></table></figure>

<h2 id="修改端口号"><a href="#修改端口号" class="headerlink" title="修改端口号"></a>修改端口号</h2><p>jenkins默认的端口号是8080，如果和服务器上8080端口占用了，可以修改默认的端口号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;sysconfig&#x2F;jenkins</span><br><span class="line"></span><br><span class="line">56行</span><br><span class="line">JENKINS_PORT&#x3D;&quot;8080&quot;</span><br></pre></td></tr></table></figure>

<h2 id="访问jenkins"><a href="#访问jenkins" class="headerlink" title="访问jenkins"></a>访问jenkins</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ip:8080</span><br><span class="line"></span><br><span class="line">初次登录，密码输入以下命令查看</span><br><span class="line">cat &#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;secrets&#x2F;initialAdminPassword</span><br></pre></td></tr></table></figure>

<h1 id="jenkins-配置"><a href="#jenkins-配置" class="headerlink" title="jenkins 配置"></a>jenkins 配置</h1><h2 id="全局配置"><a href="#全局配置" class="headerlink" title="全局配置"></a>全局配置</h2><p>Manage Jenkins  –&gt;  Global Tool Configuration  –&gt;  JDK  –&gt;  Maven</p>
<h2 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h2><p>新建Item  –&gt;  hwzs  –&gt;  General  –&gt;   This project is parameterized  –&gt;  图1  –&gt;  源码管理  –&gt;  图2</p>
<p>–&gt;  构建 –&gt;  执行shell  –&gt;  图3</p>
<img src="/images/图1-20190731.png" width="100%" height="100%">
<img src="/images/图2-20190731.png" width="100%" height="100%">
<img src="/images/图3-20190731.png" width="100%" height="100%">


]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s Deployment 字段解释</title>
    <url>/posts/6401f101.html</url>
    <content><![CDATA[<h1 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h1><p>apiVersion: 表示版本<br>kind: 表示资源<br>metadata: 表示元信息<br>spec: 资源规范字段</p>
<a id="more"></a>

<h2 id="Deployment-yaml-名词解释"><a href="#Deployment-yaml-名词解释" class="headerlink" title="Deployment yaml 名词解释"></a>Deployment yaml 名词解释</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1  # 指定api版本，此值必须在kubectl api-versions中  </span><br><span class="line">kind: Deployment  # 指定创建资源的角色&#x2F;类型   </span><br><span class="line">metadata:  # 资源的元数据&#x2F;属性 </span><br><span class="line">  name: demo  # 资源的名字，在同一个namespace中必须唯一</span><br><span class="line">  namespace: default # 部署在哪个namespace中</span><br><span class="line">  labels:  # 设定资源的标签</span><br><span class="line">    app: demo</span><br><span class="line">    version: stable</span><br><span class="line">spec: # 资源规范字段</span><br><span class="line">  replicas: 1 # 声明副本数目</span><br><span class="line">  revisionHistoryLimit: 3 # 保留历史版本</span><br><span class="line">  selector: # 选择器</span><br><span class="line">    matchLabels: # 匹配标签</span><br><span class="line">      app: demo</span><br><span class="line">      version: stable</span><br><span class="line">  strategy: # 策略</span><br><span class="line">    rollingUpdate: # 滚动更新</span><br><span class="line">      maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数</span><br><span class="line">      maxUnavailable: 30% # 示在更新过程中能够进入不可用状态的 Pod 的最大值，可以为百分比，也可以为整数</span><br><span class="line">    type: RollingUpdate # 滚动更新策略</span><br><span class="line">  template: # 模版</span><br><span class="line">    metadata: # 资源的元数据&#x2F;属性 </span><br><span class="line">      annotations: # 自定义注解列表</span><br><span class="line">        sidecar.istio.io&#x2F;inject: &quot;false&quot; # 自定义注解名字</span><br><span class="line">      labels: # 设定资源的标签</span><br><span class="line">        app: demo</span><br><span class="line">        version: stable</span><br><span class="line">    spec: # 资源规范字段</span><br><span class="line">      containers:</span><br><span class="line">      - name: demo # 容器的名字   </span><br><span class="line">        image: demo:v1 # 容器使用的镜像地址   </span><br><span class="line">        imagePullPolicy: IfNotPresent # 每次Pod启动拉取镜像策略，三个选择 Always、Never、IfNotPresent</span><br><span class="line">                                      # Always，每次都检查；Never，每次都不检查（不管本地是否有）；IfNotPresent，如果本地有就不检查，如果没有就拉取 </span><br><span class="line">        resources: # 资源管理</span><br><span class="line">          limits: # 最大使用</span><br><span class="line">            cpu: 300m # CPU，1核心 &#x3D; 1000m</span><br><span class="line">            memory: 500Mi # 内存，1G &#x3D; 1000Mi</span><br><span class="line">          requests:  # 容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 100Mi</span><br><span class="line">        livenessProbe: # pod 内部健康检查的设置</span><br><span class="line">          httpGet: # 通过httpget检查健康，返回200-399之间，则认为容器正常</span><br><span class="line">            path: &#x2F;healthCheck # URI地址</span><br><span class="line">            port: 8080 # 端口</span><br><span class="line">            scheme: HTTP # 协议</span><br><span class="line">            # host: 127.0.0.1 # 主机地址</span><br><span class="line">          initialDelaySeconds: 30 # 表明第一次检测在容器启动后多长时间后开始</span><br><span class="line">          timeoutSeconds: 5 # 检测的超时时间</span><br><span class="line">          periodSeconds: 30 # 检查间隔时间</span><br><span class="line">          successThreshold: 1 # 成功门槛</span><br><span class="line">          failureThreshold: 5 # 失败门槛，连接失败5次，pod杀掉，重启一个新的pod</span><br><span class="line">        readinessProbe: # Pod 准备服务健康检查设置</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;healthCheck</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 30</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          failureThreshold: 5</span><br><span class="line">      	#也可以用这种方法   </span><br><span class="line">      	#exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常   </span><br><span class="line">      	#  command:   </span><br><span class="line">      	#    - cat   </span><br><span class="line">      	#    - &#x2F;tmp&#x2F;health   </span><br><span class="line">      	#也可以用这种方法   </span><br><span class="line">      	#tcpSocket: # 通过tcpSocket检查健康  </span><br><span class="line">      	#  port: number </span><br><span class="line">        ports:</span><br><span class="line">          - name: http # 名称</span><br><span class="line">            containerPort: 8080 # 容器开发对外的端口 </span><br><span class="line">            protocol: TCP # 协议</span><br><span class="line">      imagePullSecrets: # 镜像仓库拉取密钥</span><br><span class="line">        - name: harbor-certification</span><br><span class="line">      affinity: # 亲和性调试</span><br><span class="line">        nodeAffinity: # 节点亲和力</span><br><span class="line">          requiredDuringSchedulingIgnoredDuringExecution: # pod 必须部署到满足条件的节点上</span><br><span class="line">            nodeSelectorTerms: # 节点满足任何一个条件就可以</span><br><span class="line">            - matchExpressions: # 有多个选项，则只有同时满足这些逻辑选项的节点才能运行 pod</span><br><span class="line">              - key: beta.kubernetes.io&#x2F;arch</span><br><span class="line">                operator: In</span><br><span class="line">                values:</span><br><span class="line">                - amd64</span><br></pre></td></tr></table></figure>

<h2 id="Service-yaml-名词解释"><a href="#Service-yaml-名词解释" class="headerlink" title="Service yaml 名词解释"></a>Service yaml 名词解释</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1 # 指定api版本，此值必须在kubectl api-versions中 </span><br><span class="line">kind: Service # 指定创建资源的角色&#x2F;类型 </span><br><span class="line">metadata: # 资源的元数据&#x2F;属性</span><br><span class="line">  name: demo # 资源的名字，在同一个namespace中必须唯一</span><br><span class="line">  namespace: default # 部署在哪个namespace中</span><br><span class="line">  labels: # 设定资源的标签</span><br><span class="line">    app: demo</span><br><span class="line">spec: # 资源规范字段</span><br><span class="line">  type: ClusterIP # ClusterIP 类型</span><br><span class="line">  ports:</span><br><span class="line">    - port: 8080 # service 端口</span><br><span class="line">      targetPort: http # 容器暴露的端口</span><br><span class="line">      protocol: TCP # 协议</span><br><span class="line">      name: http # 端口名称</span><br><span class="line">  selector: # 选择器</span><br><span class="line">    app: demo</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernets</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s获取客户端访问真实 IP</title>
    <url>/posts/9f2bb797.html</url>
    <content><![CDATA[<p>通常，当集群内的客户端连接到服务的时候，是支持服务的 Pod 可以获取到客户端的 IP 地址的，但是，当通过节点端口接收到连接时，由于对数据包执行了源网络地址转换（SNAT），因此数据包的源 IP 地址会发生变化，后端的 Pod 无法看到实际的客户端 IP，对于某些应用来说是个问题，比如，nginx 的请求日志就无法获取准确的客户端访问 IP 了。</p>
<a id="more"></a>
<h1 id="官方文档："><a href="#官方文档：" class="headerlink" title="官方文档："></a>官方文档：</h1><p><a href="https://kubernetes.io/docs/tutorials/services/source-ip/">https://kubernetes.io/docs/tutorials/services/source-ip/</a></p>
<h1 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h1><ol>
<li>可以使用 NodePort + {“spec”:{“externalTrafficPolicy”:”Local”}} 的配置来实现。</li>
</ol>
<p>注意：需要将pod调度到指定node节点上才可以</p>
<ol start="2">
<li><p>利用 INGERSS，可以在INGRESS 请求转发阶段将客户端IP 带到请求头中。</p>
</li>
<li><p>pod直接使用 HOST 网络模式。</p>
</li>
</ol>
<p>以下实例使用方法1：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name:  nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">      - operator: &quot;Exists&quot;</span><br><span class="line">      nodeSelector:</span><br><span class="line">        kubernetes.io&#x2F;hostname: master</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line"> externalTrafficPolicy: Local</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernets</tag>
      </tags>
  </entry>
  <entry>
    <title>Calico 网络通信原理</title>
    <url>/posts/62b928e0.html</url>
    <content><![CDATA[<p>Calico 是一个纯三层的数据中心网络方案，而且无缝集成像 OpenStack 这种 Iaas 云架构，能够提供可控的 VM、容器、裸机之间的 IP 通信。为什么说它是纯三层呢？因为所有的数据包都是通过路由的形式找到对应的主机和容器的，然后通过 BGP 协议来将所有路由同步到所有的机器或数据中心，从而完成整个网络的互联。</p>
<a id="more"></a>
<p>简单来说，Calico 在主机上创建了一堆的 veth pair，其中一端在主机上，另一端在容器的网络命名空间里，然后在容器和主机中分别设置几条路由，来完成网络的互联。</p>
<h1 id="Calico-网络模型"><a href="#Calico-网络模型" class="headerlink" title="Calico 网络模型"></a>Calico 网络模型</h1><p>任意选择 k8s 集群中的一个节点作为实验节点，进入容器 A，查看容器 A 的 IP 地址：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000</span><br><span class="line">    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1&#x2F;8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if771: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1440 qdisc noqueue state UP</span><br><span class="line">    link&#x2F;ether 66:fb:34:db:c9:b4 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.8.2&#x2F;32 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>容器 A 的默认路由</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ip route</span><br><span class="line">default via 169.254.1.1 dev eth0</span><br><span class="line">169.254.1.1 dev eth0 scope link</span><br></pre></td></tr></table></figure>

<p>从路由表可以知道 169.254.1.1 是容器的默认网关，但却找不到任何一张网卡对应这个 IP 地址</p>
<p>当一个数据包的目的地址不是本机时，就会查询路由表，从路由表中查到网关后，它首先会通过 ARP 获得网关的 MAC 地址，然后在发出的网络数据包中将目标 MAC 改为网关的 MAC，而网关的 IP 地址不会出现在任何网络包头中。也就是说，没有人在乎这个 IP 地址究竟是什么，只要能找到对应的 MAC 地址，能响应 ARP 就行了。</p>
<p>可以通过 ip neigh 命令查看一下本地的 ARP 缓存：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ip neigh</span><br><span class="line">169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee REACHABLE</span><br></pre></td></tr></table></figure>

<p>实际上 Calico 利用了网卡的代理 ARP 功能。代理 ARP 是 ARP 协议的一个变种，当 ARP 请求目标跨网段时，网关设备收到此 ARP 请求，会用自己的 MAC 地址返回给请求者，这便是代理 ARP（Proxy ARP）。举个例子：<br><img src="/images/2019-07-30-061928.jpg" width="100%" height="100%"></p>
<p>上面这张图中，电脑发送 ARP 请求服务器 8.8.8.8 的 MAC 地址，路由器（网关）收到这个请求时会进行判断，由于目标 8.8.8.8 不属于本网段（即跨网段），此时便返回自己的接口 MAC 地址给 PC，后续电脑访问服务器时，目标 MAC 直接封装为 MAC254。</p>
<p>查看宿主机的网卡信息和路由信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ip addr</span><br><span class="line">...</span><br><span class="line">771: calicba2f87f6bb@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UP group default</span><br><span class="line">    link&#x2F;ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 14</span><br><span class="line">    inet6 fe80::ecee:eeff:feee:eeee&#x2F;64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">$ ip route </span><br><span class="line">...</span><br><span class="line">172.17.8.2 dev calicba2f87f6bb scope link</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>查看是否开启代理 ARP：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;calicba2f87f6bb&#x2F;proxy_arp</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p>如果还不放心，可以通过 tcpdump 抓包验证一下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ tcpdump -i calicba2f87f6bb -e -nn</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on calicba2f87f6bb, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">14:27:13.565539 ee:ee:ee:ee:ee:ee &gt; 0a:58:ac:1c:ce:12, ethertype IPv4 (0x0800), length 4191: 10.96.0.1.443 &gt; 172.17.8.2.36180: Flags [P.], seq 403862039:403866164, ack 2023703985, win 990, options [nop,nop,TS val 331780572 ecr 603755526], length 4125</span><br><span class="line">14:27:13.565613 0a:58:ac:1c:ce:12 &gt; ee:ee:ee:ee:ee:ee, ethertype IPv4 (0x0800), length 66: 172.17.8.2.36180 &gt; 10.96.0.1.443: Flags [.], ack 4125, win 2465, options [nop,nop,TS val 603758497 ecr 331780572], length 0</span><br></pre></td></tr></table></figure>

<p>总结：</p>
<ol>
<li><p>Calico 通过一个巧妙的方法将 workload 的所有流量引导到一个特殊的网关 169.254.1.1，从而引流到主机的 calixxx 网络设备上，最终将二三层流量全部转换成三层流量来转发。</p>
</li>
<li><p>在主机上通过开启代理 ARP 功能来实现 ARP 应答，使得 ARP 广播被抑制在主机上，抑制了广播风暴，也不会有 ARP 表膨胀的问题。</p>
</li>
</ol>
<h1 id="模拟组网"><a href="#模拟组网" class="headerlink" title="模拟组网"></a>模拟组网</h1><p>架构如图所示：</p>
<img src="/images/2019-07-30-calico-test.jpg" width="100%" height="100%">

<p>先在 Host0 上执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ip link add veth0 type veth peer name eth0</span><br><span class="line">$ ip netns add ns0</span><br><span class="line">$ ip link set eth0 netns ns0</span><br><span class="line">$ ip netns exec ns0 ip a add 10.20.1.2&#x2F;24 dev eth0</span><br><span class="line">$ ip netns exec ns0 ip link set eth0 up</span><br><span class="line">$ ip netns exec ns0 ip route add 169.254.1.1 dev eth0 scope link</span><br><span class="line">$ ip netns exec ns0 ip route add default via 169.254.1.1 dev eth0</span><br><span class="line">$ ip link set veth0 up</span><br><span class="line">$ ip route add 10.20.1.2 dev veth0 scope link</span><br><span class="line">$ ip route add 10.20.1.3 via 192.168.1.16 dev ens192</span><br><span class="line">$ echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;veth0&#x2F;proxy_arp</span><br></pre></td></tr></table></figure>

<p>在 Host1 上执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ip link add veth0 type veth peer name eth0</span><br><span class="line">$ ip netns add ns1</span><br><span class="line">$ ip link set eth0 netns ns1</span><br><span class="line">$ ip netns exec ns1 ip a add 10.20.1.3&#x2F;24 dev eth0</span><br><span class="line">$ ip netns exec ns1 ip link set eth0 up</span><br><span class="line">$ ip netns exec ns1 ip route add 169.254.1.1 dev eth0 scope link</span><br><span class="line">$ ip netns exec ns1 ip route add default via 169.254.1.1 dev eth0</span><br><span class="line">$ ip link set veth0 up</span><br><span class="line">$ ip route add 10.20.1.3 dev veth0 scope link</span><br><span class="line">$ ip route add 10.20.1.2 via 192.168.1.32 dev ens192</span><br><span class="line">$ echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;veth0&#x2F;proxy_arp</span><br></pre></td></tr></table></figure>

<p>网络连通性测试：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Host0</span><br><span class="line">$ ip netns exec ns1 ping 10.20.1.3</span><br><span class="line">PING 10.20.1.3 (10.20.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.20.1.3: icmp_seq&#x3D;1 ttl&#x3D;62 time&#x3D;0.303 ms</span><br><span class="line">64 bytes from 10.20.1.3: icmp_seq&#x3D;2 ttl&#x3D;62 time&#x3D;0.334 ms</span><br></pre></td></tr></table></figure>

<p>具体的转发过程如下：</p>
<ol>
<li><p>ns0 网络空间的所有数据包都转发到一个虚拟的 IP 地址 169.254.1.1，发送 ARP 请求。</p>
</li>
<li><p>Host0 的 veth 端收到 ARP 请求时通过开启网卡的代理 ARP 功能直接把自己的 MAC 地址返回给 ns0。</p>
</li>
<li><p>ns0 发送目的地址为 ns1 的 IP 数据包。</p>
</li>
<li><p>因为使用了 169.254.1.1 这样的地址，Host 判断为三层路由转发，查询本地路由 10.20.1.3 via 192.168.1.16 dev ens192 发送给对端 Host1，如果配置了 BGP，这里就会看到 proto 协议为 BIRD。</p>
</li>
<li><p>当 Host1 收到 10.20.1.3 的数据包时，匹配本地的路由表 10.20.1.3 dev veth0 scope link，将数据包转发到对应的 veth0 端，从而到达 ns1。</p>
</li>
<li><p>回程类似</p>
</li>
</ol>
<p>通过这个实验，我们可以很清晰地掌握 Calico 网络的数据转发流程，首先需要给所有的 ns 配置一条特殊的路由，并利用 veth 的代理 ARP 功能让 ns 出来的所有转发都变成三层路由转发，然后再利用主机的路由进行转发。这种方式不仅实现了同主机的二三层转发，也能实现跨主机的转发。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernets</tag>
        <tag>calico</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes集群中添加node节点导致flannel因网卡名不一致启动失败</title>
    <url>/posts/514dbaf1.html</url>
    <content><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>加入一节点到k8s中，发现新节点的守护容器kube-flannel-ds启动失败。</p>
<a id="more"></a>
<p>查看pod状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods -n kube-system</span><br><span class="line">NAME                             READY   STATUS             RESTARTS   AGE</span><br><span class="line">coredns-58cc8c89f4-5g6q7         1&#x2F;1     Running            9          137d</span><br><span class="line">coredns-58cc8c89f4-9f6xf         1&#x2F;1     Running            8          137d</span><br><span class="line">etcd-master                      1&#x2F;1     Running            8          137d</span><br><span class="line">kube-apiserver-master            1&#x2F;1     Running            10         137d</span><br><span class="line">kube-controller-manager-master   1&#x2F;1     Running            13         137d</span><br><span class="line">kube-flannel-ds-amd64-7gxkk      1&#x2F;1     Running            4          137d</span><br><span class="line">kube-flannel-ds-amd64-7q2xg      1&#x2F;1     Running            6          137d</span><br><span class="line">kube-flannel-ds-amd64-jqkxc      1&#x2F;1     Running            6          51d</span><br><span class="line">kube-flannel-ds-amd64-mlcp7      1&#x2F;1     Running            8          137d</span><br><span class="line">kube-flannel-ds-amd64-mmcj5      0&#x2F;1     CrashLoopBackOff   7          16m</span><br><span class="line">kube-proxy-4276m                 1&#x2F;1     Running            6          137d</span><br><span class="line">kube-proxy-8td8l                 1&#x2F;1     Running            0          21m</span><br><span class="line">kube-proxy-chv82                 1&#x2F;1     Running            4          51d</span><br><span class="line">kube-proxy-jv9vg                 1&#x2F;1     Running            11         137d</span><br><span class="line">kube-proxy-v82vj                 1&#x2F;1     Running            5          137d</span><br><span class="line">kube-scheduler-master            1&#x2F;1     Running            11         137d</span><br></pre></td></tr></table></figure>

<p>查看pod详细状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl describe pods kube-flannel-ds-amd64-mmcj5 -n kube-system</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                  From                 Message</span><br><span class="line">  ----     ------     ----                 ----                 -------</span><br><span class="line">  Normal   Scheduled  &lt;unknown&gt;            default-scheduler    Successfully assigned kube-system&#x2F;kube-flannel-ds-amd64-mmcj5 to k8s-master</span><br><span class="line">  Normal   Pulling    17m                  kubelet, k8s-master  Pulling image &quot;quay.io&#x2F;coreos&#x2F;flannel:v0.11.0-amd64&quot;</span><br><span class="line">  Normal   Pulled     15m                  kubelet, k8s-master  Successfully pulled image &quot;quay.io&#x2F;coreos&#x2F;flannel:v0.11.0-amd64&quot;</span><br><span class="line">  Normal   Created    14m                  kubelet, k8s-master  Created container install-cni</span><br><span class="line">  Normal   Started    14m                  kubelet, k8s-master  Started container install-cni</span><br><span class="line">  Normal   Pulled     13m (x4 over 14m)    kubelet, k8s-master  Container image &quot;quay.io&#x2F;coreos&#x2F;flannel:v0.11.0-amd64&quot; already present on machine</span><br><span class="line">  Normal   Created    13m (x4 over 14m)    kubelet, k8s-master  Created container kube-flannel</span><br><span class="line">  Normal   Started    13m (x4 over 14m)    kubelet, k8s-master  Started container kube-flannel</span><br><span class="line">  Warning  BackOff    2m8s (x61 over 14m)  kubelet, k8s-master  Back-off restarting failed container</span><br></pre></td></tr></table></figure>

<p>查看pod日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl logs -f kube-flannel-ds-amd64-mmcj5 -n kube-system</span><br><span class="line">I0330 01:50:24.544864       1 main.go:210] Could not find valid interface matching ens192: error looking up interface ens192: route ip+net: no such network interface</span><br><span class="line">E0330 01:50:24.545148       1 main.go:234] Failed to find interface to use that matches the interfaces and&#x2F;or regexes provided</span><br></pre></td></tr></table></figure>

<p>查看pod文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl edit pods kube-flannel-ds-amd64-mmcj5 -n kube-system</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">  containers:</span><br><span class="line">  - args:</span><br><span class="line">    - --ip-masq</span><br><span class="line">    - --kube-subnet-mgr</span><br><span class="line">    - --iface&#x3D;ens192</span><br><span class="line">    command:</span><br><span class="line">    - &#x2F;opt&#x2F;bin&#x2F;flanneld</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h1 id="原因、解决"><a href="#原因、解决" class="headerlink" title="原因、解决"></a>原因、解决</h1><p>查看需要添加node节点的网卡信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master ~]# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1&#x2F;8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: enp1s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link&#x2F;ether 00:15:17:a1:e7:aa brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.100.65&#x2F;24 brd 192.168.100.255 scope global enp1s0f0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>需改ds yaml配置文件信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl edit ds kube-flannel-ds-amd64 -n kube-system</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">  containers:</span><br><span class="line">  - args:</span><br><span class="line">    - --ip-masq</span><br><span class="line">    - --kube-subnet-mgr</span><br><span class="line">    - --iface&#x3D;ens192</span><br><span class="line">    - --iface&#x3D;enp1s0f0</span><br><span class="line">    #- --iface-regex&#x3D;eth*|ens*</span><br><span class="line">    command:</span><br><span class="line">    - &#x2F;opt&#x2F;bin&#x2F;flanneld</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://coreos.com/flannel/docs/latest/flannel-config.html">https://coreos.com/flannel/docs/latest/flannel-config.html</a></p>
]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernets</tag>
        <tag>fannal</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s简易安装1.11.0</title>
    <url>/posts/57a40ada.html</url>
    <content><![CDATA[<p>** 备注 **</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">运行环境：CentOS 7.4</span><br><span class="line">k8s镜像：采用阿里云及阿里云私有仓库</span><br><span class="line">系统架构：master 172.16.2.30；node 172.16.2.31</span><br><span class="line"></span><br><span class="line">官网：https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;tasks&#x2F;tools&#x2F;install-kubeadm&#x2F;</span><br><span class="line">      https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;setup&#x2F;independent&#x2F;create-cluster-kubeadm&#x2F;</span><br><span class="line">日志：&#x2F;var&#x2F;log&#x2F;message</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<p>由于kubernetes迭代速度快，变更版本不一致问题，安装方法移至github</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;hyman0603&#x2F;k8s-deploy</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>利用cert-manager让Ingress启用免费的HTTPS证书</title>
    <url>/posts/44d2f690.html</url>
    <content><![CDATA[<p>cert-manager 是替代 kube-lego 的一个开源项目，用于在 Kubernetes 集群中自动提供 HTTPS 证书，支持 Let’s Encrypt, HashiCorp Vault 这些免费证书的签发。</p>
<a id="more"></a>

<ul>
<li><p>使用 Helm 安装，所以请确保 Helm 已安装</p>
</li>
<li><p>集群必须已经装有 Ingress Controller</p>
</li>
<li><p>需要颁发免费证书的域名配置DNS记录，IP 指向 Ingress Controller 对外暴露的地址</p>
</li>
<li><p>开源地址：<a href="https://github.com/jetstack/cert-manager">https://github.com/jetstack/cert-manager</a></p>
</li>
<li><p>文档地址：<a href="https://cert-manager.readthedocs.io">https://cert-manager.readthedocs.io</a></p>
</li>
</ul>
<h1 id="安装-cert-manager"><a href="#安装-cert-manager" class="headerlink" title="安装 cert-manager"></a>安装 cert-manager</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm install \</span><br><span class="line">    --name cert-manager \</span><br><span class="line">    --namespace kube-system \</span><br><span class="line">    stable&#x2F;cert-manager</span><br></pre></td></tr></table></figure>

<h1 id="生成免费证书"><a href="#生成免费证书" class="headerlink" title="生成免费证书"></a>生成免费证书</h1><p>先创建一个签发机构，cert-manager 给我们提供了 Issuer 和 ClusterIssuer 这两种用于创建签发机构的自定义资源对象，Issuer 只能用来签发自己所在 namespace 下的证书，ClusterIssuer 可以签发任意 namespace 下的证书，这里以 ClusterIssuer 为例创建一个签发机构</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi issuer.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: certmanager.k8s.io&#x2F;v1alpha1</span><br><span class="line">kind: ClusterIssuer</span><br><span class="line">metadata:</span><br><span class="line">  name: letsencrypt-prod</span><br><span class="line">spec:</span><br><span class="line">  acme:</span><br><span class="line">    server: https:&#x2F;&#x2F;acme-v02.api.letsencrypt.org&#x2F;directory</span><br><span class="line">    email: ywthings@qq.com</span><br><span class="line">    privateKeySecretRef:</span><br><span class="line">      name: letsencrypt-prod</span><br><span class="line">    http01: &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>metadata.name 是我们创建的签发机构的名称，后面我们创建证书的时候会引用它<br>spec.acme.email 是你自己的邮箱，证书快过期的时候会有邮件提醒，不过 cert-manager 会利用 acme 协议自动给我们重新颁发证书来续期<br>spec.acme.server 是 acme 协议的服务端，我们这里用 Let’s Encrypt，这个地址就写死成这样就行<br>spec.acme.privateKeySecretRef 指示此签发机构的私钥将要存储到哪个 Secret 对象中，名称不重要<br>spec.acme.http01 这里指示签发机构使用 HTTP-01 的方式进行 acme 协议 (还可以用 DNS 方式，acme 协议的目的是证明这台机器和域名都是属于你的，然后才准许给你颁发证书)</p>
<p>签发机构，接下来我们就可以生成免费证书了，cert-manager 给我们提供了 Certificate 这个用于生成证书的自定义资源对象，它必须局限在某一个 namespace 下，证书最终会在这个 namespace 下以 Secret 的资源对象存储，假如我想在 dashboard 这个 namespace 下生成免费证书（这个 namespace 已存在)，创建一个 Certificate 对象：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi cert.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: certmanager.k8s.io&#x2F;v1alpha1</span><br><span class="line">kind: Certificate</span><br><span class="line">metadata:</span><br><span class="line">  name: ywthings.com</span><br><span class="line">  namespace: dashboard</span><br><span class="line">spec:</span><br><span class="line">  secretName: ywthings.com-tls</span><br><span class="line">  issuerRef:</span><br><span class="line">    name: letsencrypt-prod</span><br><span class="line">    kind: ClusterIssuer</span><br><span class="line">  dnsNames:</span><br><span class="line">  - ywthings.com</span><br><span class="line">  acme:</span><br><span class="line">    config:</span><br><span class="line">    - http01:</span><br><span class="line">        ingressClass: nginx</span><br><span class="line">      domains:</span><br><span class="line">      - ywthings.com</span><br></pre></td></tr></table></figure>

<p>spec.secretName 指示证书最终存到哪个 Secret 中<br>spec.issuerRef.kind 值为 ClusterIssuer 说明签发机构不在本 namespace 下，而是在全局<br>spec.issuerRef.name 我们创建的签发机构的名称 (ClusterIssuer.metadata.name)<br>spec.dnsNames 指示该证书的可以用于哪些域名<br>spec.acme.config.http01.ingressClass 使用 HTTP-01 方式校验该域名和机器时，cert-manager 会尝试创建Ingress 对象来实现该校验，如果指定该值，会给创建的 Ingress 加上 kubernetes.io/ingress.class 这个 annotation，如果我们的 Ingress Controller 是 Nginx Ingress Controller，指定这个字段可以让创建的 Ingress 被 Nginx Ingress Controller 处理。<br>spec.acme.config.http01.domains 指示该证书的可以用于哪些域名</p>
<p>Ingress 使用来启用免费 HTTPS</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io&#x2F;ingress.class: &quot;nginx&quot;</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: ywthings.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: my-nginx</span><br><span class="line">          servicePort: 443</span><br><span class="line">        path: &#x2F;</span><br><span class="line">  tls:</span><br><span class="line">   - secretName: ywthings.com-tls</span><br><span class="line">     hosts:</span><br><span class="line">       - ywthings.com</span><br></pre></td></tr></table></figure>

<p>在 Ingress 定义的 spec.tls.secretName 引用生成的证书所在的 Secret 名称即可实现使用免费证书<br>将 Ingress 定义的 spec.rules.host 和 spec.tls.hosts 里的域名都替换为你自己的域名</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
        <tag>tls</tag>
        <tag>cert-manager</tag>
      </tags>
  </entry>
  <entry>
    <title>在kubernetes-1.12.x上搭建prometheus-operator监控预警系统</title>
    <url>/posts/5f93a102.html</url>
    <content><![CDATA[<p>基于CoreOS开源的prometheus-operator监控系统，该监控系统比较全面，主要是集成grafana的监控模版，维护比较方便。由于项目开发迭代速度较快，部署方法可能会更新，必要时请参考官方文档。</p>
<p>官方git：<a href="https://github.com/coreos/prometheus-operator">https://github.com/coreos/prometheus-operator</a></p>
<a id="more"></a>
<h1 id="部署Prometheus-Operator"><a href="#部署Prometheus-Operator" class="headerlink" title="部署Prometheus Operator"></a>部署Prometheus Operator</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;coreos&#x2F;prometheus-operator.git</span><br><span class="line">cd prometheus-operator</span><br><span class="line"></span><br><span class="line">#为了配合其他组件，可以将namespace默认为default修改为monitoring</span><br><span class="line">kubectl apply -f bundle.yaml</span><br></pre></td></tr></table></figure>

<h1 id="部署grafana-alternanager-kube-status-meric"><a href="#部署grafana-alternanager-kube-status-meric" class="headerlink" title="部署grafana alternanager kube-status-meric"></a>部署grafana alternanager kube-status-meric</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd prometheus-operator&#x2F;contrib&#x2F;kube-prometheus&#x2F;manifests</span><br><span class="line">kubectl apply -f .</span><br></pre></td></tr></table></figure>
<p>部署成功，如图所示<br><img src="/images/20181117104620.png" width="100%" height="100%"></p>
<h1 id="修改访问方式"><a href="#修改访问方式" class="headerlink" title="修改访问方式"></a>修改访问方式</h1><p>把svc的访问方式改为NodePort模式,使用kubectl edit svc svcname方式修改，或者修改manifests文件夹下的yaml相关文件，如图所示<br><img src="/images/20181117105045.png" width="100%" height="100%"></p>
<h1 id="访问grafana"><a href="#访问grafana" class="headerlink" title="访问grafana"></a>访问grafana</h1><p>部署成功，如图所示<br><img src="/images/20181117104928.png" width="100%" height="100%"></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s的几种部署策略</title>
    <url>/posts/a828f573.html</url>
    <content><![CDATA[<p>在Kubernetes中有几种不同的方式发布应用，所以为了让应用在升级期间依然平稳提供服务，选择一个正确的发布策略就非常重要了。</p>
<p>选择正确的部署策略是要依赖于我们的业务需求的，下面我们列出了一些可能会使用到的策略：</p>
<ul>
<li>重建(recreate)：停止旧版本部署新版本</li>
</ul>
<ul>
<li>滚动更新(rolling-update)：一个接一个地以滚动更新方式发布新版本</li>
</ul>
<ul>
<li>蓝绿(blue/green)：新版本与旧版本一起存在，然后切换流量</li>
</ul>
<ul>
<li>金丝雀(canary)：将新版本面向一部分用户发布，然后继续全量发布</li>
</ul>
<ul>
<li>A/B测(a/b testing)：以精确的方式（HTTP 头、cookie、权重等）向部分用户发布新版本。A/B测实际上是一种基于数据统计做出业务决策的技术。在 Kubernetes 中并不原生支持，需要额外的一些高级组件来完成改设置（比如Istio、Linkerd、Traefik、或者自定义 Nginx/Haproxy 等）。<a id="more"></a>
你可以在Kubernetes集群上来对上面的这些策略进行测试，下面的仓库中有需要使用到的资源清单：<a href="https://github.com/ContainerSolutions/k8s-deployment-strategies">https://github.com/ContainerSolutions/k8s-deployment-strategies</a></li>
</ul>
<p>接下来我们来介绍下每种策略，看看在什么场景下面适合哪种策略。</p>
<h1 id="重建-Recreate-最好在开发环境"><a href="#重建-Recreate-最好在开发环境" class="headerlink" title="重建(Recreate) - 最好在开发环境"></a>重建(Recreate) - 最好在开发环境</h1><p>策略定义为Recreate的Deployment，会终止所有正在运行的实例，然后用较新的版本来重新创建它们。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br></pre></td></tr></table></figure>

<img src="/images/RCXtdC.jpg" width="100%" height="100%">

<p>重新创建策略是一个虚拟部署，包括关闭版本A，然后在关闭版本A后部署版本B. 此技术意味着服务的停机时间取决于应用程序的关闭和启动持续时间。</p>
<p>我们这里创建两个相关的资源清单文件，app-v1.yaml：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: my-app</span><br><span class="line">  labels:</span><br><span class="line">    app: my-app</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: http</span><br><span class="line">  selector:</span><br><span class="line">    app: my-app</span><br><span class="line">---</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: my-app</span><br><span class="line">  labels:</span><br><span class="line">    app: my-app</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: my-app</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: my-app</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: my-app</span><br><span class="line">        version: v1.0.0</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io&#x2F;scrape: &quot;true&quot;</span><br><span class="line">        prometheus.io&#x2F;port: &quot;9101&quot;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-app</span><br><span class="line">        image: containersol&#x2F;k8s-deployment-strategies</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 8080</span><br><span class="line">        - name: probe</span><br><span class="line">          containerPort: 8086</span><br><span class="line">        env:</span><br><span class="line">        - name: VERSION</span><br><span class="line">          value: v1.0.0</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;live</span><br><span class="line">            port: probe</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;ready</span><br><span class="line">            port: probe</span><br><span class="line">          periodSeconds: 5</span><br></pre></td></tr></table></figure>

<p>app-v2.yaml 文件内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: my-app</span><br><span class="line">  labels:</span><br><span class="line">    app: my-app</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: my-app</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: my-app</span><br><span class="line">        version: v2.0.0</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io&#x2F;scrape: &quot;true&quot;</span><br><span class="line">        prometheus.io&#x2F;port: &quot;9101&quot;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-app</span><br><span class="line">        image: containersol&#x2F;k8s-deployment-strategies</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 8080</span><br><span class="line">        - name: probe</span><br><span class="line">          containerPort: 8086</span><br><span class="line">        env:</span><br><span class="line">        - name: VERSION</span><br><span class="line">          value: v2.0.0</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;live</span><br><span class="line">            port: probe</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;ready</span><br><span class="line">            port: probe</span><br><span class="line">          periodSeconds: 5</span><br></pre></td></tr></table></figure>

<p>上面两个资源清单文件中的 Deployment 定义几乎是一直的，唯一不同的是定义的环境变量VERSION值不同，接下来按照下面的步骤来验证Recreate策略：</p>
<ol>
<li>版本1提供服务</li>
<li>删除版本1</li>
<li>部署版本2</li>
<li>等待所有副本准备就绪</li>
</ol>
<p>首先部署第一个应用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f app-v1.yaml</span><br><span class="line">service &quot;my-app&quot; created</span><br><span class="line">deployment.apps &quot;my-app&quot; created</span><br></pre></td></tr></table></figure>

<p>测试版本1是否部署成功：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get pods -l app&#x3D;my-app</span><br><span class="line">NAME                      READY     STATUS    RESTARTS   AGE</span><br><span class="line">my-app-7b4874cd75-m5kct   1&#x2F;1       Running   0          19m</span><br><span class="line">my-app-7b4874cd75-pc444   1&#x2F;1       Running   0          19m</span><br><span class="line">my-app-7b4874cd75-tlctl   1&#x2F;1       Running   0          19m</span><br><span class="line">$ kubectl get svc my-app</span><br><span class="line">NAME      TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">my-app    NodePort   10.108.238.76   &lt;none&gt;        80:32532&#x2F;TCP   5m</span><br><span class="line">$ curl http:&#x2F;&#x2F;127.0.0.1:32532</span><br><span class="line">Host: my-app-7b4874cd75-pc444, Version: v1.0.0</span><br></pre></td></tr></table></figure>

<p>可以看到版本1的应用正常运行了。为了查看部署的运行情况，打开一个新终端并运行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ watch kubectl get po -l app&#x3D;my-app</span><br></pre></td></tr></table></figure>

<p>然后部署版本2的应用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f app-v2.yaml</span><br></pre></td></tr></table></figure>

<p>这个时候可以观察上面新开的终端中的 Pod 列表的变化，可以看到之前的3个 Pod 都会先处于Terminating状态，并且3个 Pod 都被删除后才开始创建新的 Pod。</p>
<p>然后测试第二个版本应用的部署进度：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ while sleep 0.1; do curl http:&#x2F;&#x2F;127.0.0.1:32532; done</span><br><span class="line">curl: (7) Failed connect to 127.0.0.1:32532; Connection refused</span><br><span class="line">curl: (7) Failed connect to 127.0.0.1:32532; Connection refused</span><br><span class="line">......</span><br><span class="line">Host: my-app-f885c8d45-sp44p, Version: v2.0.0</span><br><span class="line">Host: my-app-f885c8d45-t8g7g, Version: v2.0.0</span><br><span class="line">Host: my-app-f885c8d45-sp44p, Version: v2.0.0</span><br></pre></td></tr></table></figure>

<p>可以看到最开始的阶段服务都是处于不可访问的状态，然后到第二个版本的应用部署成功后才正常访问，可以看到现在访问的数据是版本2了。</p>
<p>最后，可以执行下面的命令来清空上面的资源对象：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl delete all -l app&#x3D;my-app</span><br></pre></td></tr></table></figure>

<p>结论:</p>
<ul>
<li>应用状态全部更新</li>
<li>停机时间取决于应用程序的关闭和启动消耗的时间</li>
</ul>
<h1 id="滚动更新-rolling-update"><a href="#滚动更新-rolling-update" class="headerlink" title="滚动更新(rolling-update)"></a>滚动更新(rolling-update)</h1><p>滚动更新通过逐个替换实例来逐步部署新版本的应用，直到所有实例都被替换完成为止。它通常遵循以下过程：在负载均衡器后面使用版本 A 的实例池，然后部署版本 B 的一个实例，当服务准备好接收流量时(Readiness Probe 正常)，将该实例添加到实例池中，然后从实例池中删除一个版本 A 的实例并关闭，如下图所示：</p>
<img src="/images/30iUcb.jpg" width="100%" height="100%">

<p>下图是滚动更新过程应用接收流量的示意图：</p>
<img src="/images/OIhAqJ.jpg" width="100%" height="100%">

<p>下面是 Kubernetes 中通过 Deployment 来进行滚动更新的关键参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 2        # 一次可以添加多少个Pod</span><br><span class="line">      maxUnavailable: 1  # 滚动更新期间最大多少个Pod不可用</span><br></pre></td></tr></table></figure>

<p>现在仍然使用上面的 app-v1.yaml 这个资源清单文件，新建一个定义滚动更新的资源清单文件 app-v2-rolling-update.yaml，文件内容如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: my-app</span><br><span class="line">  labels:</span><br><span class="line">    app: my-app</span><br><span class="line">spec:</span><br><span class="line">  replicas: 10</span><br><span class="line">  # maxUnavailable设置为0可以完全确保在滚动更新期间服务不受影响，还可以使用百分比的值来进行设置。</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 1</span><br><span class="line">      maxUnavailable: 0</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: my-app</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: my-app</span><br><span class="line">        version: v2.0.0</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io&#x2F;scrape: &quot;true&quot;</span><br><span class="line">        prometheus.io&#x2F;port: &quot;9101&quot;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-app</span><br><span class="line">        image: containersol&#x2F;k8s-deployment-strategies</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 8080</span><br><span class="line">        - name: probe</span><br><span class="line">          containerPort: 8086</span><br><span class="line">        env:</span><br><span class="line">        - name: VERSION</span><br><span class="line">          value: v2.0.0</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;live</span><br><span class="line">            port: probe</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;ready</span><br><span class="line">            port: probe</span><br><span class="line">          # 初始延迟设置高点可以更好地观察滚动更新过程</span><br><span class="line">          initialDelaySeconds: 15</span><br><span class="line">          periodSeconds: 5</span><br></pre></td></tr></table></figure>

<p>上面的资源清单中我们在环境变量中定义了版本2，然后通过设置strategy.type=RollingUpdate来定义该 Deployment 使用滚动更新的策略来更新应用，接下来我们按下面的步骤来验证滚动更新策略：</p>
<ol>
<li>版本1提供服务</li>
<li>部署版本2</li>
<li>等待直到所有副本都被版本2替换完成</li>
</ol>
<p>同样，首先部署版本1应用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f app-v1.yaml</span><br><span class="line">service &quot;my-app&quot; created</span><br><span class="line">deployment.apps &quot;my-app&quot; created</span><br></pre></td></tr></table></figure>

<p>测试版本1是否部署成功：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get pods -l app&#x3D;my-app</span><br><span class="line">NAME                      READY     STATUS    RESTARTS   AGE</span><br><span class="line">my-app-7b4874cd75-h8c4d   1&#x2F;1       Running   0          47s</span><br><span class="line">my-app-7b4874cd75-p4l8f   1&#x2F;1       Running   0          47s</span><br><span class="line">my-app-7b4874cd75-qnt7p   1&#x2F;1       Running   0          47s</span><br><span class="line">$ kubectl get svc my-app</span><br><span class="line">NAME      TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">my-app    NodePort   10.109.99.184   &lt;none&gt;        80:30486&#x2F;TCP   1m</span><br><span class="line">$ curl http:&#x2F;&#x2F;127.0.0.1:30486</span><br><span class="line">Host: my-app-7b4874cd75-qnt7p, Version: v1.0.0</span><br></pre></td></tr></table></figure>

<p>同样，在一个新终端中执行下面命令观察 Pod 变化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ watch kubectl get pod -l app&#x3D;my-app</span><br></pre></td></tr></table></figure>

<p>然后部署滚动更新版本2应用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f app-v2-rolling-update.yaml</span><br><span class="line">deployment.apps &quot;my-app&quot; configured</span><br></pre></td></tr></table></figure>

<p>这个时候在上面的 watch 终端中可以看到多了很多 Pod，还在创建当中，并没有一开始就删除之前的 Pod，同样，这个时候执行下面命令，测试应用状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ while sleep 0.1; do curl http:&#x2F;&#x2F;127.0.0.1:30486; done</span><br><span class="line">Host: my-app-7b4874cd75-vrlj7, Version: v1.0.0</span><br><span class="line">......</span><br><span class="line">Host: my-app-7b4874cd75-vrlj7, Version: v1.0.0</span><br><span class="line">Host: my-app-6b5479d97f-2fk24, Version: v2.0.0</span><br><span class="line">Host: my-app-7b4874cd75-p4l8f, Version: v1.0.0</span><br><span class="line">......</span><br><span class="line">Host: my-app-6b5479d97f-s5ctz, Version: v2.0.0</span><br><span class="line">Host: my-app-7b4874cd75-5ldqx, Version: v1.0.0</span><br><span class="line">......</span><br><span class="line">Host: my-app-6b5479d97f-5z6ww, Version: v2.0.0</span><br></pre></td></tr></table></figure>

<p>我们可以看到上面的应用并没有出现不可用的情况，最开始访问到的都是版本1的应用，然后偶尔会出现版本2的应用，直到最后全都变成了版本2的应用，而这个时候看上面 watch 终端中 Pod 已经全部变成10个版本2的应用了，我们可以看到这就是一个逐步替换的过程。</p>
<p>如果在滚动更新过程中发现新版本应用有问题，我们可以通过下面的命令来进行一键回滚：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout undo deploy my-app</span><br><span class="line">deployment.apps &quot;my-app&quot;</span><br></pre></td></tr></table></figure>

<p>如果你想保持两个版本的应用都存在，那么我们也可以执行 pause 命令来暂停更新：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout pause deploy my-app</span><br><span class="line">deployment.apps &quot;my-app&quot; paused</span><br></pre></td></tr></table></figure>

<p>这个时候我们再去循环访问我们的应用就可以看到偶尔会出现版本1的应用信息了。</p>
<p>如果新版本应用程序没问题了，也可以继续恢复更新：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout resume deploy my-app</span><br><span class="line">deployment.apps &quot;my-app&quot; resumed</span><br></pre></td></tr></table></figure>

<p>最后，可以执行下面的命令来清空上面的资源对象：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl delete all -l app&#x3D;my-app</span><br></pre></td></tr></table></figure>

<p>结论：</p>
<ul>
<li>版本在实例之间缓慢替换</li>
<li>rollout/rollback 可能需要一定时间</li>
<li>无法控制流量</li>
</ul>
<h1 id="蓝-绿-blue-green-最好用来验证-API-版本问题"><a href="#蓝-绿-blue-green-最好用来验证-API-版本问题" class="headerlink" title="蓝/绿(blue/green) - 最好用来验证 API 版本问题"></a>蓝/绿(blue/green) - 最好用来验证 API 版本问题</h1><p>蓝/绿发布是版本2 与版本1 一起发布，然后流量切换到版本2，也称为红/黑部署。蓝/绿发布与滚动更新不同，版本2(绿) 与版本1(蓝)一起部署，在测试新版本满足要求后，然后更新更新 Kubernetes 中扮演负载均衡器角色的 Service 对象，通过替换 label selector 中的版本标签来将流量发送到新版本，如下图所示：<br><img src="/images/mEQW8i.jpg" width="100%" height="100%"></p>
<p>下面是蓝绿发布策略下应用方法的示例图：<br><img src="/images/FxTxRP.jpg" width="100%" height="100%"></p>
<p>在 Kubernetes 中，我们可以用两种方法来实现蓝绿发布，通过单个 Service 对象或者 Ingress 控制器来实现蓝绿发布，实际操作都是类似的，都是通过 label 标签去控制。</p>
<p>实现蓝绿发布的关键点就在于 Service 对象中 label selector 标签的匹配方法，比如我们重新定义版本1 的资源清单文件 app-v1-single-svc.yaml，文件内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: my-app</span><br><span class="line">  labels:</span><br><span class="line">    app: my-app</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: http</span><br><span class="line">  # 注意这里我们匹配 app 和 version 标签，当要切换流量的时候，我们更新 version 标签的值，比如：v2.0.0</span><br><span class="line">  selector:</span><br><span class="line">    app: my-app</span><br><span class="line">    version: v1.0.0</span><br><span class="line">---</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: my-app-v1</span><br><span class="line">  labels:</span><br><span class="line">    app: my-app</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: my-app</span><br><span class="line">      version: v1.0.0</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: my-app</span><br><span class="line">        version: v1.0.0</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io&#x2F;scrape: &quot;true&quot;</span><br><span class="line">        prometheus.io&#x2F;port: &quot;9101&quot;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-app</span><br><span class="line">        image: containersol&#x2F;k8s-deployment-strategies</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 8080</span><br><span class="line">        - name: probe</span><br><span class="line">          containerPort: 8086</span><br><span class="line">        env:</span><br><span class="line">        - name: VERSION</span><br><span class="line">          value: v1.0.0</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;live</span><br><span class="line">            port: probe</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;ready</span><br><span class="line">            port: probe</span><br><span class="line">          periodSeconds: 5</span><br></pre></td></tr></table></figure>

<p>上面定义的资源对象中，最重要的就是 Service 中 label selector 的定义：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">selector:</span><br><span class="line">  app: my-app</span><br><span class="line">  version: v1.0.0</span><br></pre></td></tr></table></figure>

<p>版本2 的应用定义和以前一样，新建文件 app-v2-single-svc.yaml，文件内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: my-app-v2</span><br><span class="line">  labels:</span><br><span class="line">    app: my-app</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: my-app</span><br><span class="line">      version: v2.0.0</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: my-app</span><br><span class="line">        version: v2.0.0</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io&#x2F;scrape: &quot;true&quot;</span><br><span class="line">        prometheus.io&#x2F;port: &quot;9101&quot;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-app</span><br><span class="line">        image: containersol&#x2F;k8s-deployment-strategies</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 8080</span><br><span class="line">        - name: probe</span><br><span class="line">          containerPort: 8086</span><br><span class="line">        env:</span><br><span class="line">        - name: VERSION</span><br><span class="line">          value: v2.0.0</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;live</span><br><span class="line">            port: probe</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;ready</span><br><span class="line">            port: probe</span><br><span class="line">          periodSeconds: 5</span><br></pre></td></tr></table></figure>

<p>然后按照下面的步骤来验证使用单个 Service 对象实现蓝/绿部署的策略：</p>
<ol>
<li>版本1 应用提供服务</li>
<li>部署版本2 应用</li>
<li>等到版本2 应用全部部署完成</li>
<li>切换入口流量从版本1 到版本2</li>
<li>关闭版本1 应用</li>
</ol>
<p>首先，部署版本1 应用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f app-v1-single-svc.yaml</span><br><span class="line">service &quot;my-app&quot; created</span><br><span class="line">deployment.apps &quot;my-app-v1&quot; created</span><br></pre></td></tr></table></figure>

<p>测试版本1 应用是否部署成功：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get pods -l app&#x3D;my-app</span><br><span class="line">NAME                         READY     STATUS    RESTARTS   AGE</span><br><span class="line">my-app-v1-7b4874cd75-7xh6s   1&#x2F;1       Running   0          41s</span><br><span class="line">my-app-v1-7b4874cd75-dmq8f   1&#x2F;1       Running   0          41s</span><br><span class="line">my-app-v1-7b4874cd75-t64z7   1&#x2F;1       Running   0          41s</span><br><span class="line">$ kubectl get svc -l app&#x3D;my-app</span><br><span class="line">NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">my-app    NodePort   10.106.184.144   &lt;none&gt;        80:31539&#x2F;TCP   50s</span><br><span class="line">$ curl http:&#x2F;&#x2F;127.0.0.1:31539</span><br><span class="line">Host: my-app-v1-7b4874cd75-7xh6s, Version: v1.0.0</span><br></pre></td></tr></table></figure>

<p>同样，新开一个终端，执行如下命令观察 Pod 变化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ watch kubectl get pod -l app&#x3D;my-app</span><br></pre></td></tr></table></figure>

<p>然后部署版本2 应用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f app-v2-single-svc.yaml</span><br><span class="line">deployment.apps &quot;my-app-v2&quot; created</span><br></pre></td></tr></table></figure>

<p>然后在上面 watch 终端中可以看到会多3个my-app-v2开头的 Pod，待这些 Pod 部署成功后，我们再去访问当前的应用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ while sleep 0.1; do curl http:&#x2F;&#x2F;127.0.0.1:31539; done</span><br><span class="line">Host: my-app-v1-7b4874cd75-dmq8f, Version: v1.0.0</span><br><span class="line">Host: my-app-v1-7b4874cd75-dmq8f, Version: v1.0.0</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>我们会发现访问到的都是版本1 的应用，和我们刚刚部署的版本2 没有任何关系，这是因为我们 Service 对象中通过 label selector 匹配的是version=v1.0.0这个标签，我们可以通过修改 Service 对象的匹配标签，将流量路由到标签version=v2.0.0的 Pod 去：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl patch service my-app -p &#39;&#123;&quot;spec&quot;:&#123;&quot;selector&quot;:&#123;&quot;version&quot;:&quot;v2.0.0&quot;&#125;&#125;&#125;&#39;</span><br><span class="line">service &quot;my-app&quot; patched</span><br></pre></td></tr></table></figure>

<p>然后再去访问应用，可以发现现在都是版本2 的信息了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ while sleep 0.1; do curl http:&#x2F;&#x2F;127.0.0.1:31539; done</span><br><span class="line">Host: my-app-v2-f885c8d45-r5m6z, Version: v2.0.0</span><br><span class="line">Host: my-app-v2-f885c8d45-r5m6z, Version: v2.0.0</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>如果你需要回滚到版本1，同样只需要更改 Service 的匹配标签即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl patch service my-app -p &#39;&#123;&quot;spec&quot;:&#123;&quot;selector&quot;:&#123;&quot;version&quot;:&quot;v1.0.0&quot;&#125;&#125;&#125;&#39;</span><br></pre></td></tr></table></figure>

<p>如果新版本已经完全符合我们的需求了，就可以删除版本1 的应用了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl delete deploy my-app-v1</span><br></pre></td></tr></table></figure>

<p>最后，同样，执行如下命令清理上述资源对象：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl delete all -l app&#x3D;my-app</span><br></pre></td></tr></table></figure>

<p>结论：</p>
<ul>
<li><p>实时部署/回滚</p>
</li>
<li><p>避免版本问题，因为一次更改是整个应用的改变</p>
</li>
<li><p>需要两倍的资源</p>
</li>
<li><p>在发布到生产之前，应该对整个应用进行适当的测试</p>
</li>
</ul>
<h1 id="金丝雀-Canary-让部分用户参与测试"><a href="#金丝雀-Canary-让部分用户参与测试" class="headerlink" title="金丝雀(Canary) - 让部分用户参与测试"></a>金丝雀(Canary) - 让部分用户参与测试</h1><p>金丝雀部署是让部分用户访问到新版本应用，在 Kubernetes 中，可以使用两个具有相同 Pod 标签的 Deployment 来实现金丝雀部署。新版本的副本和旧版本的一起发布。在一段时间后如果没有检测到错误，则可以扩展新版本的副本数量并删除旧版本的应用。</p>
<p>如果需要按照具体的百分比来进行金丝雀发布，需要尽可能的启动多的 Pod 副本，这样计算流量百分比的时候才方便，比如，如果你想将 1% 的流量发送到版本 B，那么我们就需要有一个运行版本 B 的 Pod 和 99 个运行版本 A 的 Pod，当然如果你对具体的控制策略不在意的话也就无所谓了，如果你需要更精确的控制策略，建议使用服务网格（如 Istio），它们可以更好地控制流量。</p>
<img src="/images/RvcM4f.jpg" width="100%" height="100%">

<p>在下面的例子中，我们使用 Kubernetes 原生特性来实现一个穷人版的金丝雀发布，如果你想要对流量进行更加细粒度的控制，请使用豪华版本的 Istio。下面是金丝雀发布的应用请求示意图：</p>
<img src="/images/OclNv8.jpg" width="100%" height="100%">

<p>接下来我们按照下面的步骤来验证金丝雀策略：</p>
<ol>
<li>10个副本的版本1 应用提供服务</li>
<li>版本2 应用部署1个副本（意味着小于10%的流量）</li>
<li>等待足够的时间来确认版本2 应用足够稳定没有任何错误信息</li>
<li>将版本2 应用扩容到10个副本</li>
<li>等待所有实例完成</li>
<li>关闭版本1 应用</li>
</ol>
<p>首先，创建版本1 的应用资源清单，app-v1-canary.yaml，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: my-app</span><br><span class="line">  labels:</span><br><span class="line">    app: my-app</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: http</span><br><span class="line">  selector:</span><br><span class="line">    app: my-app</span><br><span class="line">---</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: my-app-v1</span><br><span class="line">  labels:</span><br><span class="line">    app: my-app</span><br><span class="line">spec:</span><br><span class="line">  replicas: 10</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: my-app</span><br><span class="line">      version: v1.0.0</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: my-app</span><br><span class="line">        version: v1.0.0</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io&#x2F;scrape: &quot;true&quot;</span><br><span class="line">        prometheus.io&#x2F;port: &quot;9101&quot;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-app</span><br><span class="line">        image: containersol&#x2F;k8s-deployment-strategies</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 8080</span><br><span class="line">        - name: probe</span><br><span class="line">          containerPort: 8086</span><br><span class="line">        env:</span><br><span class="line">        - name: VERSION</span><br><span class="line">          value: v1.0.0</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;live</span><br><span class="line">            port: probe</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;ready</span><br><span class="line">            port: probe</span><br><span class="line">          periodSeconds: 5</span><br></pre></td></tr></table></figure>

<p>其中核心的部分也是 Service 对象中的 label selector 标签，不在具有版本相关的标签了，然后定义版本2 的资源清单文件，app-v2-canary.yaml，文件内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: my-app-v2</span><br><span class="line">  labels:</span><br><span class="line">    app: my-app</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: my-app</span><br><span class="line">      version: v2.0.0</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: my-app</span><br><span class="line">        version: v2.0.0</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io&#x2F;scrape: &quot;true&quot;</span><br><span class="line">        prometheus.io&#x2F;port: &quot;9101&quot;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-app</span><br><span class="line">        image: containersol&#x2F;k8s-deployment-strategies</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 8080</span><br><span class="line">        - name: probe</span><br><span class="line">          containerPort: 8086</span><br><span class="line">        env:</span><br><span class="line">        - name: VERSION</span><br><span class="line">          value: v2.0.0</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;live</span><br><span class="line">            port: probe</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;ready</span><br><span class="line">            port: probe</span><br><span class="line">          periodSeconds: 5</span><br></pre></td></tr></table></figure>

<p>版本1 和版本2 的 Pod 都具有一个共同的标签app=my-app，所以对应的 Service 会匹配两个版本的 Pod。</p>
<p>首先，部署版本1 应用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f app-v1-canary.yaml</span><br><span class="line">service &quot;my-app&quot; created</span><br><span class="line">deployment.apps &quot;my-app-v1&quot; created</span><br></pre></td></tr></table></figure>

<p>然后测试版本1 应用是否正确部署了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get svc -l app&#x3D;my-app</span><br><span class="line">NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">my-app        NodePort    10.105.133.213   &lt;none&gt;        80:30760&#x2F;TCP   47s</span><br><span class="line">$ curl http:&#x2F;&#x2F;127.0.0.1:30760</span><br><span class="line">Host: my-app-v1-7b4874cd75-tsh2s, Version: v1.0.0</span><br></pre></td></tr></table></figure>

<p>同样，新开一个终端，查看 Pod 的变化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ watch kubectl get pod</span><br></pre></td></tr></table></figure>
<p>然后部署版本2 应用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f app-v2-canary.yaml</span><br><span class="line">deployment.apps &quot;my-app-v2&quot; created</span><br></pre></td></tr></table></figure>

<p>然后在 watch 终端页面可以看到多了一个 Pod，现在一共 11 个 Pod，其中只有1 个 Pod 运行新版本应用，然后同样可以循环访问该应用，查看是否会有版本2 的应用信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ while sleep 0.1; do curl http:&#x2F;&#x2F;127.0.0.1:30760; done</span><br><span class="line">Host: my-app-v1-7b4874cd75-bhxbp, Version: v1.0.0</span><br><span class="line">Host: my-app-v1-7b4874cd75-wmcqc, Version: v1.0.0</span><br><span class="line">Host: my-app-v1-7b4874cd75-tsh2s, Version: v1.0.0</span><br><span class="line">Host: my-app-v1-7b4874cd75-ml58j, Version: v1.0.0</span><br><span class="line">Host: my-app-v1-7b4874cd75-spsdv, Version: v1.0.0</span><br><span class="line">Host: my-app-v2-f885c8d45-mc2fx, Version: v2.0.0</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>正常情况下可以看到大部分都是返回的版本1 的应用信息，偶尔会出现版本2 的应用信息，这就证明我们的金丝雀发布成功了，待确认了版本2 的这个应用没有任何问题后，可以将版本2 应用扩容到10 个副本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl scale --replicas&#x3D;10 deploy my-app-v2</span><br><span class="line">deployment.extensions &quot;my-app-v2&quot; scaled</span><br></pre></td></tr></table></figure>

<p>其实这个时候访问应用的话新版本和旧版本的流量分配是1:1了，确认了版本2 正常后，就可以删除版本1 的应用了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl delete deploy my-app-v1</span><br><span class="line">deployment.extensions &quot;my-app-v1&quot; deleted</span><br></pre></td></tr></table></figure>

<p>最终留下的是 10 个新版本的 Pod 了，到这里我们的整个金丝雀发布就完成了。</p>
<p>同样，最后，执行下面的命令删除上面的资源对象：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl delete all -l app&#x3D;my-app</span><br></pre></td></tr></table></figure>

<p>结论：</p>
<ul>
<li>部分用户获取新版本</li>
<li>方便错误和性能监控</li>
<li>快速回滚</li>
<li>发布较慢</li>
<li>流量精准控制很浪费（99％A / 1％B = 99 Pod A，1 Pod B）</li>
</ul>
<h1 id="A-B测试-A-B-testing-最适合部分用户的功能测试"><a href="#A-B测试-A-B-testing-最适合部分用户的功能测试" class="headerlink" title="A/B测试(A/B testing) - 最适合部分用户的功能测试"></a>A/B测试(A/B testing) - 最适合部分用户的功能测试</h1><p>A/B 测试实际上是一种基于统计信息而非部署策略来制定业务决策的技术，与业务结合非常紧密。但是它们也是相关的，也可以使用金丝雀发布来实现。</p>
<p>除了基于权重在版本之间进行流量控制之外，A/B 测试还可以基于一些其他参数（比如 Cookie、User Agent、地区等等）来精确定位给定的用户群，该技术广泛用于测试一些功能特性的效果，然后按照效果来进行确定。</p>
<p>要使用这些细粒度的控制，仍然还是建议使用 Istio，可以根据权重或 HTTP 头等来动态请求路由控制流量转发。</p>
<img src="/images/b4qVNZ.jpg" width="100%" height="100%">

<p>下面是使用 Istio 进行规则设置的示例，因为 Istio 还不太稳定，以下示例规则将来可能会更改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">route:</span><br><span class="line">- tags:</span><br><span class="line">  version: v1.0.0</span><br><span class="line">  weight: 90</span><br><span class="line">- tags:</span><br><span class="line">  version: v2.0.0</span><br><span class="line">  weight: 10</span><br></pre></td></tr></table></figure>

<p>关于在 Istio 中具体如何做 A/B 测试，我们这里就不再详细介绍了，我们在istio-book文档中有相关的介绍。</p>
<img src="/images/2lK8wZ.jpg" width="100%" height="100%">

<p>结论：</p>
<ul>
<li>几个版本并行运行</li>
<li>完全控制流量分配</li>
<li>特定的一个访问错误难以排查，需要分布式跟踪</li>
<li>Kubernetes 没有直接的支持，需要其他额外的工具</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>发布应用有许多种方法，当发布到开发/测试环境的时候，重建或者滚动更新通常是一个不错的选择。在生产环境，滚动更新或者蓝绿发布比较合适，但是新版本的提前测试是非常有必要的。如果你对新版本的应用不是很有信心的话，那应该使用金丝雀发布，将用户的影响降到最低。最后，如果你的公司需要在特定的用户群体中进行新功能的测试，例如，移动端用户请求路由到版本 A，桌面端用户请求路由到版本 B，那么你就看使用A/B 测试，通过使用 Kubernetes 服务网关的配置，可以根据某些请求参数来确定用户应路由的服务。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernets</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes In Action 书中一些命令摘抄</title>
    <url>/posts/255fa8e1.html</url>
    <content><![CDATA[<h1 id="第三章-pod"><a href="#第三章-pod" class="headerlink" title="第三章 pod"></a>第三章 pod</h1><h2 id="使用-kubectl-explain-来发现可能的-API-对象字段"><a href="#使用-kubectl-explain-来发现可能的-API-对象字段" class="headerlink" title="使用 kubectl explain 来发现可能的 API 对象字段"></a>使用 kubectl explain 来发现可能的 API 对象字段</h2><p>在准备 manifest 时，可以转到 http: /kubemetes.io docs/api 上的 Kubemetes 参考文档查看每个 API 对象支持哪些属性，也可以使用命令 kubectl explain</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如当从头创建一个 pod manifest 时，可以从请求 kubectl 来解释 pod开始：</span><br><span class="line">$ kubectl explain pods </span><br><span class="line"></span><br><span class="line">Kubectl 打印出对象的解释并列出对象可以包含的属性，接下来就可以深入了解各个属性的更多信息 例如，可以这样查看 spec 属性：</span><br><span class="line">$ kubectl explain pod.spec</span><br></pre></td></tr></table></figure>

<h2 id="获取多容器-pod-的日志时指定容器名称"><a href="#获取多容器-pod-的日志时指定容器名称" class="headerlink" title="获取多容器 pod 的日志时指定容器名称"></a>获取多容器 pod 的日志时指定容器名称</h2><p>如果我们的 pod 包含多个容器，在运行 kubectl logs 命令时则必须通过包含-c ＜容器名称＞选项来显式指定容器名称。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl logs kubia-manual -c kubia</span><br></pre></td></tr></table></figure>

<h2 id="将本地网络端口转发到-pod-中的端口"><a href="#将本地网络端口转发到-pod-中的端口" class="headerlink" title="将本地网络端口转发到 pod 中的端口"></a>将本地网络端口转发到 pod 中的端口</h2><p>如果想要在不通过 service 的情况下与某个特定的 pod 进行通信 出于调试或其他原因）, Kubemetes 将允许我们配置端口转发到该 pod。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">例如以下命令会将机器的本地端口 8888 转发到我们的 kubia-manual pod 的端口 8080 : </span><br><span class="line">$ kubectl port-forward kubia-manual 8888:8080 </span><br><span class="line">Forwarding from 127.0.0.1:8888 -&gt; 8080 </span><br><span class="line">Forwarding from [: :l] :8888 -&gt; 8080</span><br></pre></td></tr></table></figure>

<h2 id="pod-标签"><a href="#pod-标签" class="headerlink" title="pod 标签"></a>pod 标签</h2><p>你只对某些标签感兴趣，可以使用-L选项指定它们并将它们分别显示在自己的列中，而不是列出所有标签 接下来我们再次列出所有pod。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get po --show-labels </span><br><span class="line"></span><br><span class="line">$ kubectl get po -L creation_method,env </span><br><span class="line">NAME             READY   STATUS  RESTARTS AGE CREATION_METHOD   ENV </span><br><span class="line">kubia-manual     1&#x2F;1   Running    0      16m    &lt;none&gt;         &lt;none&gt; </span><br><span class="line">kubia-manual-v2  1&#x2F;1   Running    0      2m     manual          prod</span><br></pre></td></tr></table></figure>
<h3 id="创建标签"><a href="#创建标签" class="headerlink" title="创建标签"></a>创建标签</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl label po kubia-manual creation_method&#x3D;manual</span><br></pre></td></tr></table></figure>

<h3 id="修改标签"><a href="#修改标签" class="headerlink" title="修改标签"></a>修改标签</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl label po kubia-manual-v2 env&#x3D;debug --overwrite</span><br></pre></td></tr></table></figure>
<p><strong>注意 在更改现有标签 ，需要使用 –overwrite 选项</strong></p>
<h2 id="删除命名空间中的（几乎）所有资源"><a href="#删除命名空间中的（几乎）所有资源" class="headerlink" title="删除命名空间中的（几乎）所有资源"></a>删除命名空间中的（几乎）所有资源</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl delete all --all</span><br></pre></td></tr></table></figure>
<p>命令中的第一个all指定正在删除所有资源类型，而–all选项指定将删除所有资源实例<br><strong>注意 kubectl delete all –all 命令也会删除名为 kubernetes Service ，但它应该会在几分钟后自动重新创建</strong></p>
<h1 id="第四章-控制器"><a href="#第四章-控制器" class="headerlink" title="第四章 控制器"></a>第四章 控制器</h1><h2 id="获取前容器日志"><a href="#获取前容器日志" class="headerlink" title="获取前容器日志"></a>获取前容器日志</h2><p>kubectl logs 命令将显示当前容器的日志,当你想知道为什么前一个容器终止时，你想看到的是前一个容器的日志，而不是当前容可以通过添加–previous 选项来完成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl logs liveness --previous</span><br></pre></td></tr></table></figure>

<h2 id="kubectl-describe说明"><a href="#kubectl-describe说明" class="headerlink" title="kubectl describe说明"></a>kubectl describe说明</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Last State:     Terminated</span><br><span class="line">      Reason:       Error</span><br><span class="line">      Exit Code:    137</span><br><span class="line">      Started:      Wed, 22 May 2019 16:14:45 +0800</span><br><span class="line">      Finished:     Wed, 22 May 2019 16:16:33 +0800</span><br></pre></td></tr></table></figure>
<p>退出代码为137这有特殊的含义，表示该进程由外部信号终止 数字 137 是两个数字的总和：128+x ，其中x是终止进程的信号编号。在这个例子中， x等于9，这是SIGKILL的信号编号，意味着这个进程被强行终止。</p>
<p><strong>注意 退出代码 137 表示进程被外部信号终止，退出代码为 128+9 (SIGKILL）同样，退出代码143对应于128+15 (SIGTERM）。</strong></p>
<p><strong>注意 当容器被强行终止时会创建一个全新的容器,而不是重启原来的容器</strong></p>
<h2 id="存活探针说明"><a href="#存活探针说明" class="headerlink" title="存活探针说明"></a>存活探针说明</h2><p>yaml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">livenessProbe:</span><br><span class="line">      httpGet:</span><br><span class="line">        path: &#x2F;</span><br><span class="line">        port: 8080</span><br></pre></td></tr></table></figure>
<p>kubectl describe 输出如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Liveness:       http-get http:&#x2F;&#x2F;:8080&#x2F; delay&#x3D;0s timeout&#x3D;1s period&#x3D;10s #success&#x3D;1 #failure&#x3D;3</span><br></pre></td></tr></table></figure>
<p>如 delay（延迟）、timeout（超时）、period（周期）等 。delay=Os 部分显示在容器启动后立即开始探测，timeout 仅设置为1秒，因此容器必须在1秒内进行响应，不然这次探测记作失败。每 10 秒探测一次容器（ period=lOs ），并在探测连续三次失败( # failure=3）后重启容器。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">livenessProbe:</span><br><span class="line">      httpGet:</span><br><span class="line">        path: &#x2F;</span><br><span class="line">        port: 8080</span><br><span class="line">initialDelaySeconds: 15  # Kubernetes 会在第一次探测前等待15秒</span><br></pre></td></tr></table></figure>

<h1 id="第五章-服务"><a href="#第五章-服务" class="headerlink" title="第五章 服务"></a>第五章 服务</h1><h2 id="在运行的容器中远程执行命令"><a href="#在运行的容器中远程执行命令" class="headerlink" title="在运行的容器中远程执行命令"></a>在运行的容器中远程执行命令</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl exec kubia-7nogl -- curl -s http: &#x2F;&#x2F;10 .111. 249 .153</span><br></pre></td></tr></table></figure>
<p>为什么是双横杠？<br>双横杠（–）代表着 kubectl 命令项的结束,在两个横杠之后的内容是指在 pod 内部需要执行的命令。如果需要执行的命令并没有以横杠开始的参数，横杠也不是必需的,如下情况，如果这里不使用横杠号,–选项会被解析成kubectl exec 选项，会导致结果异常和歧义错误。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl exec kubia-7nogl curl -s http:&#x2F;&#x2F;10.lll.249.153 </span><br><span class="line">The connection to the server 10.11 .249.153 was refused - did you </span><br><span class="line">specify the right host or port? </span><br><span class="line"></span><br><span class="line">服务除拒绝连接外什么都不做,这是因为 kubectl 并不能连接到位于10.111.249.153的API服务器（ -s 选项用来告诉 kubectl 需要连接一个</span><br><span class="line">不同的 API 服务器而不是默认的）</span><br></pre></td></tr></table></figure>

<h2 id="配置服务上的会话亲和性"><a href="#配置服务上的会话亲和性" class="headerlink" title="配置服务上的会话亲和性"></a>配置服务上的会话亲和性</h2><p>如果希望特定客户端产生的所有请求每次都指向同一个pod，可以设置服务的sessionAffinity 属性为 ClientIP（而不是None, None是默值），如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: vl </span><br><span class="line">kind: Service</span><br><span class="line">spec:</span><br><span class="line">  sessionAffinity: ClientIP </span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p>Kubernetes 仅仅支持两种形式的会话亲和性服务：None 和 ClientIP，不支持基于 cookie 的会话亲和性的选项。了解 kubernetes服务不是在 HTTP层面上工作，服务处理TCP和UDP包。而cookie是HTTP协议中的一部分。</p>
<h2 id="同一个服务暴露多个端口"><a href="#同一个服务暴露多个端口" class="headerlink" title="同一个服务暴露多个端口"></a>同一个服务暴露多个端口</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: vl </span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 8080 # pod的8080映射成80端口</span><br><span class="line">  - name: https</span><br><span class="line">    port: 443</span><br><span class="line">    targetPort: 8443</span><br><span class="line">  selector:</span><br><span class="line">    app: kubia     # 标签选择器适用于整个服务</span><br></pre></td></tr></table></figure>

<h2 id="无法-ping-通服务-IP-的原因"><a href="#无法-ping-通服务-IP-的原因" class="headerlink" title="无法 ping 通服务 IP 的原因"></a>无法 ping 通服务 IP 的原因</h2><p>pod里面可以执行curl访问服务，但是却ping不通。这是因为服务的集群是一个虚拟IP，并且只有在与服务端口结合时才有意义。</p>
<h2 id="使用-JSONPath-获取所有节点的-IP"><a href="#使用-JSONPath-获取所有节点的-IP" class="headerlink" title="使用 JSONPath 获取所有节点的 IP"></a>使用 JSONPath 获取所有节点的 IP</h2><p>可以在节点的 JSON 或YAML描述符中找到IP。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get nodes -o jsonpath＝&#39;&#123;.items[*].status.addresses[?(@.type&#x3D;&#x3D;&quot;ExternaIP&quot;)].address&#125;&#39;</span><br></pre></td></tr></table></figure>
<p>通过指定 kubectl 的 JSONPath ，使得其只输出需要的信息。你可能已经<br>熟悉 XPath，并且知道如何使用XML，JSONPath 基本上是 JSON 的XPath。上例中的 JSONPath 指示 kubectl 执行以下操作：</p>
<ul>
<li>浏览 item 属性中的所有元素</li>
<li>对于每个元素 ，输入 status 属性</li>
<li>过滤 address 属性的元素，仅包含那些具有将 type 属性设置为External IP 的元素</li>
<li>最后 ，打印过滤元素的 address 属性</li>
</ul>
<h2 id="防止不必要的网络跳数"><a href="#防止不必要的网络跳数" class="headerlink" title="防止不必要的网络跳数"></a>防止不必要的网络跳数</h2><p>当外部客户端通过节点端口连接到服务时（这也包括先通过负载均衡器时的情况），随机选择 pod 并不一定在接收连接的同一节点上运行。可能需要额外的网络跳转才能到达 pod ，但这种行为并不符合期望。</p>
<p>可以通过将服务配置为仅将外部通信重定 向到接收连接的节点上运行的 pod来阻止额外跳数。这是通过在服务的 spec 部分中设置 externalTrafficPolicy宇段来完成的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spec: </span><br><span class="line">  externalTrafficPolicy: Local</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p><strong>注意 这种配置客户端 IP 是不记录的</strong></p>
<h1 id="第八章-API交互"><a href="#第八章-API交互" class="headerlink" title="第八章 API交互"></a>第八章 API交互</h1><h2 id="关闭基于角色的访问控制（-RBAC"><a href="#关闭基于角色的访问控制（-RBAC" class="headerlink" title="关闭基于角色的访问控制（ RBAC)"></a>关闭基于角色的访问控制（ RBAC)</h2><p>目前最简单的方式就是运行下面的命令查询 API 服务器，从而绕过 RBAC 方式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl create clusterrolebinding permissive-binding \ </span><br><span class="line">--clusterrole&#x3D;cluster admin \ </span><br><span class="line">--group&#x3D;system:serviceaccounts</span><br></pre></td></tr></table></figure>
<p>这个命令赋予了所有服务账户（也可以说所有的 pod ）的集群管理员权限，允许它们执行任何需要的操作，很明显这是一个危险的操作，永远都不应该在生产的集群中执行，对于测试来说是没有问题的。</p>
]]></content>
      <categories>
        <category>日常部署记录</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes包管理器Helm快速安装</title>
    <url>/posts/22bfa3ae.html</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p><a href="https://github.com/helm/helm">Helm</a> 是由 <a href="https://deis.com/">Deis</a> 发起的一个开源工具，Helm是查找，共享和使用为Kubernetes构建的软件的最佳方式。</p>
<a id="more"></a>

<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>执行脚本安装 helm 客户端</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;helm&#x2F;master&#x2F;scripts&#x2F;get | bash</span><br><span class="line">如下载不来，可以直接下载二进制文件</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;helm&#x2F;helm&#x2F;releases</span><br><span class="line"></span><br><span class="line">解包并将二进制文件helm拷贝到&#x2F;usr&#x2F;local&#x2F;bin目录下</span><br><span class="line"></span><br><span class="line">wget https:&#x2F;&#x2F;storage.googleapis.com&#x2F;kubernetes-helm&#x2F;helm-v2.11.0-linux-amd64.tar.gz</span><br><span class="line">tar -zxvf helm-v2.11.0-linux-amd64.tar.gz</span><br><span class="line">cd linux-amd64&#x2F;</span><br><span class="line">cp helm &#x2F;usr&#x2F;local&#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure>
<p>安装 tiller 服务端到 kubernetes 集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm init</span><br></pre></td></tr></table></figure>

<p>查看 tiller 是否启动成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pods --namespace&#x3D;kube-system | grep tiller</span><br></pre></td></tr></table></figure>
<p>如果状态是 ImagePullBackOff ，说明是镜像问题，一般是未拉取到镜像（国内机器拉取不到 gcr.io 下的镜像) ，使用阿里源解决</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm init --upgrade -i registry.cn-shenzhen.aliyuncs.com&#x2F;hyman0603&#x2F;tiller:v2.11.0 --stable-repo-url https:&#x2F;&#x2F;kubernetes.oss-cn-hangzhou.aliyuncs.com&#x2F;charts</span><br></pre></td></tr></table></figure>
<p>自Kubernetes 1.6版本开始，API Server启用了RBAC授权。而目前的Tiller部署没有定义授权的ServiceAccount，这会导致访问API Server时被拒绝。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create serviceaccount --namespace kube-system tiller</span><br><span class="line">kubectl create clusterrolebinding tiller-cluster-rule --clusterrole&#x3D;cluster-admin --serviceaccount&#x3D;kube-system:tiller</span><br><span class="line">kubectl patch deploy --namespace kube-system tiller-deploy -p &#39;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&#39;</span><br></pre></td></tr></table></figure>
<p>更多文档资料可以查看官网<br><a href="https://docs.helm.sh/">https://docs.helm.sh/</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>利用 kubeadm 部署 Kubernetes v1.11.x HA 集群</title>
    <url>/posts/8f52266.html</url>
    <content><![CDATA[<p>本篇将说明如何透过Kubeadm来部署Kubernetes v1.11版本的高可用性丛集，而本安装主要是参考官方文件中的用kubeadm创建高可用的集群内容来进行，这边将透过HAProxy与Keepalived的结合来实现控制面的Load Balancer与VIP。</p>
<a id="more"></a>

<table>
<thead>
<tr>
<th>ip</th>
<th>hosts</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>172.16.2.30</td>
<td>server01</td>
<td>master</td>
</tr>
<tr>
<td>172.16.2.31</td>
<td>server02</td>
<td>master</td>
</tr>
<tr>
<td>172.16.2.32</td>
<td>server03</td>
<td>master</td>
</tr>
<tr>
<td>172.16.2.33</td>
<td>server04</td>
<td>Node</td>
</tr>
</tbody></table>
<h1 id="环境准备-所有节点操作"><a href="#环境准备-所有节点操作" class="headerlink" title="环境准备(所有节点操作)"></a>环境准备(所有节点操作)</h1><h2 id="配置系统相关参数"><a href="#配置系统相关参数" class="headerlink" title="配置系统相关参数"></a>配置系统相关参数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 临时禁用selinux</span><br><span class="line"># 永久关闭 修改&#x2F;etc&#x2F;sysconfig&#x2F;selinux文件设置</span><br><span class="line">sed -i &#39;s&#x2F;SELINUX&#x3D;permissive&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;sysconfig&#x2F;selinux</span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line"># 临时关闭swap</span><br><span class="line"># 永久关闭 注释&#x2F;etc&#x2F;fstab文件里swap相关的行</span><br><span class="line">swapoff -a</span><br><span class="line"></span><br><span class="line"># 开启forward</span><br><span class="line"># Docker从1.13版本开始调整了默认的防火墙规则</span><br><span class="line"># 禁用了iptables filter表中FOWARD链</span><br><span class="line"># 这样会引起Kubernetes集群中跨Node的Pod无法通信</span><br><span class="line"></span><br><span class="line">iptables -P FORWARD ACCEPT</span><br><span class="line"></span><br><span class="line"># 或者关闭防火墙</span><br><span class="line">systemctl stop firewalld &amp;&amp; systemctl disable firewalld</span><br><span class="line"></span><br><span class="line"># 配置转发相关参数，否则可能会出错</span><br><span class="line">cat &lt;&lt;EOF &gt;  &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">vm.swappiness&#x3D;0</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line"># 加载ipvs相关内核模块，如不采用ipvs方式可不操作</span><br><span class="line"># 如果重新开机，需要重新加载</span><br><span class="line">modprobe ip_vs</span><br><span class="line">modprobe ip_vs_rr</span><br><span class="line">modprobe ip_vs_wrr</span><br><span class="line">modprobe ip_vs_sh</span><br><span class="line">modprobe nf_conntrack_ipv4</span><br><span class="line">lsmod | grep ip_vs</span><br></pre></td></tr></table></figure>

<h2 id="安装-docker"><a href="#安装-docker" class="headerlink" title="安装 docker"></a>安装 docker</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 卸载安装指定版本docker-ce</span><br><span class="line">yum remove -y docker-ce docker-ce-selinux container-selinux</span><br><span class="line">yum install -y --setopt&#x3D;obsoletes&#x3D;0 \</span><br><span class="line">docker-ce-17.03.1.ce-1.el7.centos \</span><br><span class="line">docker-ce-selinux-17.03.1.ce-1.el7.centos</span><br><span class="line"></span><br><span class="line">#启动docker</span><br><span class="line">systemctl enable docker &amp;&amp; systemctl restart docker</span><br></pre></td></tr></table></figure>

<h2 id="安装-kubeadm-kubelet-和-kubectl"><a href="#安装-kubeadm-kubelet-和-kubectl" class="headerlink" title="安装 kubeadm, kubelet 和 kubectl"></a>安装 kubeadm, kubelet 和 kubectl</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置源</span><br><span class="line">cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 安装</span><br><span class="line">yum install -y kubelet-1.11.0 kubeadm-1.11.0 kubectl-1.11.0 ipvsadm</span><br></pre></td></tr></table></figure>

<h2 id="配置hosts解析"><a href="#配置hosts解析" class="headerlink" title="配置hosts解析"></a>配置hosts解析</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">172.16.2.30 server01</span><br><span class="line">172.16.2.31 server02</span><br><span class="line">172.16.2.32 server03</span><br><span class="line">172.16.2.33 server04</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h2 id="配置haproxy代理和keepalived"><a href="#配置haproxy代理和keepalived" class="headerlink" title="配置haproxy代理和keepalived"></a>配置haproxy代理和keepalived</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 拉取haproxy镜像</span><br><span class="line">docker pull haproxy:1.7.8-alpine</span><br><span class="line">mkdir &#x2F;etc&#x2F;haproxy</span><br><span class="line">cat &gt;&#x2F;etc&#x2F;haproxy&#x2F;haproxy.cfg&lt;&lt;EOF</span><br><span class="line">global</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  #daemon</span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  bind 0.0.0.0:1080</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     &#x2F;haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    will:will</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin if TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  bind 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  #maxconn 50000</span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server server01 172.16.2.30:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server server02 172.16.2.31:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server server03 172.16.2.32:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 启动haproxy</span><br><span class="line">docker run -d --name my-haproxy \</span><br><span class="line">-v &#x2F;etc&#x2F;haproxy:&#x2F;usr&#x2F;local&#x2F;etc&#x2F;haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br><span class="line"></span><br><span class="line"># 拉取keepalived镜像</span><br><span class="line">docker pull osixia&#x2F;keepalived:1.4.4</span><br><span class="line"></span><br><span class="line"># 启动</span><br><span class="line"># 载入内核相关模块</span><br><span class="line">lsmod | grep ip_vs</span><br><span class="line">modprobe ip_vs</span><br><span class="line"></span><br><span class="line"># 启动keepalived</span><br><span class="line"># ens160为本次实验172.16.2.0&#x2F;24网段的所在网卡</span><br><span class="line">docker run --net&#x3D;host --cap-add&#x3D;NET_ADMIN \</span><br><span class="line">-e KEEPALIVED_INTERFACE&#x3D;ens160 \</span><br><span class="line">-e KEEPALIVED_VIRTUAL_IPS&#x3D;&quot;#PYTHON2BASH:[&#39;172.16.2.29&#39;]&quot; \</span><br><span class="line">-e KEEPALIVED_UNICAST_PEERS&#x3D;&quot;#PYTHON2BASH:[&#39;172.16.2.30&#39;,&#39;172.16.2.31&#39;,&#39;172.16.2.32&#39;]&quot; \</span><br><span class="line">-e KEEPALIVED_PASSWORD&#x3D;hello \</span><br><span class="line">--name k8s-keepalived \</span><br><span class="line">--restart always \</span><br><span class="line">-d osixia&#x2F;keepalived:1.4.4</span><br><span class="line"></span><br><span class="line"># 查看日志</span><br><span class="line"># 会看到两个成为backup 一个成为master</span><br><span class="line">docker logs k8s-keepalived</span><br><span class="line"></span><br><span class="line"># 此时会配置 172.16.2.29 到其中一台机器</span><br><span class="line"># ping测试</span><br><span class="line">ping -c4 172.16.2.29</span><br><span class="line"></span><br><span class="line"># 如果失败后清理后，重新实验</span><br><span class="line">docker rm -f k8s-keepalived</span><br><span class="line">ip a del 172.16.2.29&#x2F;32 dev ens160</span><br></pre></td></tr></table></figure>

<h1 id="部署Kubernetes-HA"><a href="#部署Kubernetes-HA" class="headerlink" title="部署Kubernetes HA"></a>部署Kubernetes HA</h1><h2 id="Master1"><a href="#Master1" class="headerlink" title="Master1"></a>Master1</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成配置文件</span><br><span class="line">CP0_IP&#x3D;&quot;172.16.2.30&quot;</span><br><span class="line">CP0_HOSTNAME&#x3D;&quot;server01&quot;</span><br><span class="line">cat &gt;kubeadm-config.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.0</span><br><span class="line"></span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- &quot;172.16.2.29&quot;</span><br><span class="line"></span><br><span class="line">api:</span><br><span class="line">  controlPlaneEndpoint: &quot;172.16.2.29:8443&quot;</span><br><span class="line"></span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https:&#x2F;&#x2F;127.0.0.1:2379,https:&#x2F;&#x2F;$CP0_IP:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https:&#x2F;&#x2F;$CP0_IP:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https:&#x2F;&#x2F;$CP0_IP:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https:&#x2F;&#x2F;$CP0_IP:2380&quot;</span><br><span class="line">      initial-cluster: &quot;$CP0_HOSTNAME&#x3D;https:&#x2F;&#x2F;$CP0_IP:2380&quot;</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - $CP0_HOSTNAME</span><br><span class="line">      - $CP0_IP</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - $CP0_HOSTNAME</span><br><span class="line">      - $CP0_IP</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.244.0.0&#x2F;16</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 初始化</span><br><span class="line"># 注意保存返回的 join 命令</span><br><span class="line">kubeadm init --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure>
<p>执行以下指令来使用kubeconfig</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">cp -rp &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure>
<p>上面完成后，在master01将CA与Certs复制到其他master节点上以供使用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export DIR&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;</span><br><span class="line">for NODE in server02 server03; do</span><br><span class="line">    echo &quot;------ $&#123;NODE&#125; ------&quot;</span><br><span class="line">    ssh $&#123;NODE&#125; &quot;mkdir -p $&#123;DIR&#125;&#x2F;pki&#x2F;etcd&quot;</span><br><span class="line">    scp $&#123;DIR&#125;&#x2F;pki&#x2F;ca.crt $&#123;NODE&#125;:$&#123;DIR&#125;&#x2F;pki&#x2F;ca.crt</span><br><span class="line">    scp $&#123;DIR&#125;&#x2F;pki&#x2F;ca.key $&#123;NODE&#125;:$&#123;DIR&#125;&#x2F;pki&#x2F;ca.key</span><br><span class="line">    scp $&#123;DIR&#125;&#x2F;pki&#x2F;sa.key $&#123;NODE&#125;:$&#123;DIR&#125;&#x2F;pki&#x2F;sa.key</span><br><span class="line">    scp $&#123;DIR&#125;&#x2F;pki&#x2F;sa.pub $&#123;NODE&#125;:$&#123;DIR&#125;&#x2F;pki&#x2F;sa.pub</span><br><span class="line">    scp $&#123;DIR&#125;&#x2F;pki&#x2F;front-proxy-ca.crt $&#123;NODE&#125;:$&#123;DIR&#125;&#x2F;pki&#x2F;front-proxy-ca.crt</span><br><span class="line">    scp $&#123;DIR&#125;&#x2F;pki&#x2F;front-proxy-ca.key $&#123;NODE&#125;:$&#123;DIR&#125;&#x2F;pki&#x2F;front-proxy-ca.key</span><br><span class="line">    scp $&#123;DIR&#125;&#x2F;pki&#x2F;etcd&#x2F;ca.crt $&#123;NODE&#125;:$&#123;DIR&#125;&#x2F;pki&#x2F;etcd&#x2F;ca.crt</span><br><span class="line">    scp $&#123;DIR&#125;&#x2F;pki&#x2F;etcd&#x2F;ca.key $&#123;NODE&#125;:$&#123;DIR&#125;&#x2F;pki&#x2F;etcd&#x2F;ca.key</span><br><span class="line">    scp $&#123;DIR&#125;&#x2F;admin.conf $&#123;NODE&#125;:$&#123;DIR&#125;&#x2F;admin.conf</span><br><span class="line">  done</span><br></pre></td></tr></table></figure>

<h2 id="Master2"><a href="#Master2" class="headerlink" title="Master2"></a>Master2</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成配置文件</span><br><span class="line">CP0_IP&#x3D;&quot;172.16.2.30&quot;</span><br><span class="line">CP0_HOSTNAME&#x3D;&quot;server01&quot;</span><br><span class="line">CP1_IP&#x3D;&quot;172.16.2.31&quot;</span><br><span class="line">CP1_HOSTNAME&#x3D;&quot;server02&quot;</span><br><span class="line">cat &gt;kubeadm-config.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.0</span><br><span class="line"></span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- &quot;172.16.2.29&quot;</span><br><span class="line"></span><br><span class="line">api:</span><br><span class="line">  controlPlaneEndpoint: 172.16.2.29:8443</span><br><span class="line"></span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https:&#x2F;&#x2F;127.0.0.1:2379,https:&#x2F;&#x2F;$CP1_IP:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https:&#x2F;&#x2F;$CP1_IP:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https:&#x2F;&#x2F;$CP1_IP:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https:&#x2F;&#x2F;$CP1_IP:2380&quot;</span><br><span class="line">      initial-cluster: &quot;$CP0_HOSTNAME&#x3D;https:&#x2F;&#x2F;$CP0_IP:2380,$CP1_HOSTNAME&#x3D;https:&#x2F;&#x2F;$CP1_IP:2380&quot;</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - $CP1_HOSTNAME</span><br><span class="line">      - $CP1_IP</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - $CP1_HOSTNAME</span><br><span class="line">      - $CP1_IP</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.244.0.0&#x2F;16</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>通过kubeadm alpha来启动master2的kubelet：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm alpha phase certs all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<p>将节点添加到etcd集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export CP0_IP&#x3D;172.16.2.30</span><br><span class="line">export CP0_HOSTNAME&#x3D;server01</span><br><span class="line">export CP1_IP&#x3D;172.16.2.31</span><br><span class="line">export CP1_HOSTNAME&#x3D;server02</span><br><span class="line">export KUBECONFIG&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;admin.conf </span><br><span class="line"></span><br><span class="line">kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt --cert-file &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt --key-file &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.key --endpoints&#x3D;https:&#x2F;&#x2F;$&#123;CP0_IP&#125;:2379 member add $&#123;CP1_HOSTNAME&#125; https:&#x2F;&#x2F;$&#123;CP1_IP&#125;:2380</span><br><span class="line">kubeadm alpha phase etcd local --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure>
<p>部署组件并将节点标记为主节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure>
<p>执行以下指令来使用kubeconfig</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">cp -rp &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure>

<h2 id="Master3"><a href="#Master3" class="headerlink" title="Master3"></a>Master3</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成配置文件</span><br><span class="line">CP0_IP&#x3D;&quot;172.16.2.30&quot;</span><br><span class="line">CP0_HOSTNAME&#x3D;&quot;server01&quot;</span><br><span class="line">CP1_IP&#x3D;&quot;172.16.2.31&quot;</span><br><span class="line">CP1_HOSTNAME&#x3D;&quot;server02&quot;</span><br><span class="line">CP2_IP&#x3D;&quot;172.16.2.32&quot;</span><br><span class="line">CP2_HOSTNAME&#x3D;&quot;server03&quot;</span><br><span class="line">cat &gt;kubeadm-config.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.0</span><br><span class="line"></span><br><span class="line">apiServerCertSANs:</span><br><span class="line">- &quot;172.16.2.29&quot;</span><br><span class="line"></span><br><span class="line">api:</span><br><span class="line">  controlPlaneEndpoint: &quot;172.16.2.29:8443&quot;</span><br><span class="line"></span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https:&#x2F;&#x2F;127.0.0.1:2379,https:&#x2F;&#x2F;$CP2_IP:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https:&#x2F;&#x2F;$CP2_IP:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https:&#x2F;&#x2F;$CP2_IP:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https:&#x2F;&#x2F;$CP2_IP:2380&quot;</span><br><span class="line">      initial-cluster: &quot;$CP0_HOSTNAME&#x3D;https:&#x2F;&#x2F;$CP0_IP:2380,$CP1_HOSTNAME&#x3D;https:&#x2F;&#x2F;$CP1_IP:2380,$CP2_HOSTNAME&#x3D;https:&#x2F;&#x2F;$CP2_IP:2380&quot;</span><br><span class="line">      initial-cluster-state: existing</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - $CP2_HOSTNAME</span><br><span class="line">      - $CP2_IP</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - $CP2_HOSTNAME</span><br><span class="line">      - $CP2_IP</span><br><span class="line"></span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.244.0.0&#x2F;16</span><br><span class="line">  </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>通过kubeadm alpha来启动master3的kubelet</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm alpha phase certs all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>
<p>将节点添加到etcd集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export CP0_IP&#x3D;172.16.2.30</span><br><span class="line">export CP0_HOSTNAME&#x3D;server01</span><br><span class="line">export CP2_IP&#x3D;172.16.2.32</span><br><span class="line">export CP2_HOSTNAME&#x3D;server03</span><br><span class="line">export KUBECONFIG&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;admin.conf </span><br><span class="line"></span><br><span class="line">kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt --cert-file &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt --key-file &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.key --endpoints&#x3D;https:&#x2F;&#x2F;$&#123;CP0_IP&#125;:2379 member add $&#123;CP2_HOSTNAME&#125; https:&#x2F;&#x2F;$&#123;CP2_IP&#125;:2380</span><br><span class="line">kubeadm alpha phase etcd local --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure>
<p>部署组件并将节点标记为主节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase controlplane all --config kubeadm-config.yaml</span><br><span class="line">kubeadm alpha phase mark-master --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure>
<p>执行以下指令来使用kubeconfig</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">cp -rp &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure>

<h1 id="配置网络"><a href="#配置网络" class="headerlink" title="配置网络"></a>配置网络</h1><p>只有网络插件也安装配置完成之后，才能会显示为ready状态<br>设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io&#x2F;master-</span><br></pre></td></tr></table></figure>

<h2 id="下载flannel"><a href="#下载flannel" class="headerlink" title="下载flannel"></a>下载flannel</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;v0.10.0&#x2F;Documentation&#x2F;kube-flannel.yml</span><br></pre></td></tr></table></figure>
<h2 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改配置</span><br><span class="line"># 此处的ip配置要与上面kubeadm的pod-network一致</span><br><span class="line">  net-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;Network&quot;: &quot;10.244.0.0&#x2F;16&quot;,</span><br><span class="line">      &quot;Backend&quot;: &#123;</span><br><span class="line">        &quot;Type&quot;: &quot;vxlan&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"># 如果Node有多个网卡的话，参考flannel issues 39701，</span><br><span class="line"># https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes&#x2F;issues&#x2F;39701</span><br><span class="line"># 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，</span><br><span class="line"># 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，</span><br><span class="line"># flanneld启动参数加上--iface&#x3D;&lt;iface-name&gt;</span><br><span class="line">    containers:</span><br><span class="line">      - name: kube-flannel</span><br><span class="line">        image: registry.cn-shanghai.aliyuncs.com&#x2F;gcr-k8s&#x2F;flannel:v0.10.0-amd64</span><br><span class="line">        command:</span><br><span class="line">        - &#x2F;opt&#x2F;bin&#x2F;flanneld</span><br><span class="line">        args:</span><br><span class="line">        - --ip-masq</span><br><span class="line">        - --kube-subnet-mgr</span><br><span class="line">        - --iface&#x3D;eth1</span><br><span class="line"></span><br><span class="line"># 修改镜像</span><br><span class="line">image: registry.cn-shenzhen.aliyuncs.com&#x2F;hyman0603&#x2F;flannel:v0.10.0-amd64</span><br></pre></td></tr></table></figure>

<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f kube-flannel.yml</span><br></pre></td></tr></table></figure>

<h1 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h1><p>如不能部署起来，可能是配置文件错误，还有就是切记把swapoff -a把交换分区关闭，否则会无法进行下一步操作</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>利用 kubeadm 部署 Kubernetes v1.13.x</title>
    <url>/posts/1fb681a.html</url>
    <content><![CDATA[<p>官方文档：<a href="https://kubernetes.io/docs/setup/independent/high-availability/">https://kubernetes.io/docs/setup/independent/high-availability/</a></p>
<h1 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="禁用selinux"><a href="#禁用selinux" class="headerlink" title="禁用selinux"></a>禁用selinux</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br></pre></td></tr></table></figure>

<h1 id="启用net-bridge-bridge-nf-call-ip6tables和net-bridge-bridge-nf-call-iptables"><a href="#启用net-bridge-bridge-nf-call-ip6tables和net-bridge-bridge-nf-call-iptables" class="headerlink" title="启用net.bridge.bridge-nf-call-ip6tables和net.bridge.bridge-nf-call-iptables"></a>启用net.bridge.bridge-nf-call-ip6tables和net.bridge.bridge-nf-call-iptables</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;  &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">vm.swappiness&#x3D;0</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br></pre></td></tr></table></figure>

<h1 id="禁用swap"><a href="#禁用swap" class="headerlink" title="禁用swap"></a>禁用swap</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">swapoff -a</span><br><span class="line"></span><br><span class="line">修改&#x2F;etc&#x2F;fstab 文件，注释掉 SWAP 的自动挂载. </span><br><span class="line">使用free -m确认swap已经关闭。</span><br></pre></td></tr></table></figure>

<h1 id="加载ipvs相关模块"><a href="#加载ipvs相关模块" class="headerlink" title="加载ipvs相关模块"></a>加载ipvs相关模块</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &lt;&lt;EOF</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">ipvs_mods_dir&#x3D;&quot;&#x2F;usr&#x2F;lib&#x2F;modules&#x2F;$(uname -r)&#x2F;kernel&#x2F;net&#x2F;netfilter&#x2F;ipvs&quot;</span><br><span class="line">for i in $(ls $ipvs_mods_dir | grep -o &quot;^[^.]*&quot;); do</span><br><span class="line">    &#x2F;sbin&#x2F;modinfo -F filename $i &amp;&gt; &#x2F;dev&#x2F;null</span><br><span class="line">    if [ $? -eq 0 ]; then</span><br><span class="line">        &#x2F;sbin&#x2F;modprobe $i</span><br><span class="line">    fi</span><br><span class="line">done</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">EOF</span><br><span class="line">chmod 755 &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &amp;&amp; bash &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4</span><br></pre></td></tr></table></figure>
<p>上面脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。接下来还需要确保各个节点上已经安装了ipset软件包。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install ipset ipvsadm -y</span><br></pre></td></tr></table></figure>

<h1 id="安装-docke"><a href="#安装-docke" class="headerlink" title="安装 docke"></a>安装 docke</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装docker</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line">yum install -y docker-ce-18.06*</span><br><span class="line"></span><br><span class="line"># 增加加速器</span><br><span class="line">tee &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;bv55mwyn.mirror.aliyuncs.com&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 启动docker</span><br><span class="line">systemctl enable docker &amp;&amp; systemctl start docker</span><br></pre></td></tr></table></figure>

<h1 id="安装kubeadm"><a href="#安装kubeadm" class="headerlink" title="安装kubeadm"></a>安装kubeadm</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加阿里云仓库</span><br><span class="line">cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 安装kubeadm</span><br><span class="line">yum install -y kubelet-1.13.4 kubeadm-1.13.4 kubectl-1.13.4</span><br><span class="line"></span><br><span class="line"># 启动kubectl</span><br><span class="line">systemctl enable kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure>

<h1 id="部署master节点"><a href="#部署master节点" class="headerlink" title="部署master节点"></a>部署master节点</h1><p>注意这里执行初始化用到了- -image-repository选项，指定初始化需要的镜像源从阿里云镜像仓库拉取。<br>Kubenetes默认Registries地址是 k8s.gcr.io，在国内并不能访问 gcr.io，在1.13版本中我们可以增加–image-repository参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm init \</span><br><span class="line">    --image-repository registry.cn-shenzhen.aliyuncs.com&#x2F;hyman0603 \</span><br><span class="line">    --kubernetes-version v1.13.4 \</span><br><span class="line">    --pod-network-cidr&#x3D;10.244.0.0&#x2F;16</span><br></pre></td></tr></table></figure>

<h1 id="KUBE-代理使用IPVS转发"><a href="#KUBE-代理使用IPVS转发" class="headerlink" title="KUBE-代理使用IPVS转发"></a>KUBE-代理使用IPVS转发</h1><h2 id="开启路由转发"><a href="#开启路由转发" class="headerlink" title="开启路由转发"></a>开启路由转发</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;&gt; &#x2F;etc&#x2F;sysctl.conf &lt;&lt; EOF</span><br><span class="line">net.ipv4.ip_forward&#x3D;1</span><br><span class="line">EOF</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure>

<h2 id="修改ConfigMap的kube-system-kube-proxy中的config-conf"><a href="#修改ConfigMap的kube-system-kube-proxy中的config-conf" class="headerlink" title="修改ConfigMap的kube-system/kube-proxy中的config.conf"></a>修改ConfigMap的kube-system/kube-proxy中的config.conf</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl edit cm kube-proxy -n kube-system</span><br><span class="line"># 39行 mode: &quot;&quot; -&gt; mode: &quot;ipvs&quot;</span><br></pre></td></tr></table></figure>

<h2 id="重启所有工作节点的kube-proxy-pod"><a href="#重启所有工作节点的kube-proxy-pod" class="headerlink" title="重启所有工作节点的kube-proxy pod"></a>重启所有工作节点的kube-proxy pod</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system | grep kube-proxy | awk &#39;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&#39;</span><br></pre></td></tr></table></figure>

<h2 id="查看是否生效"><a href="#查看是否生效" class="headerlink" title="查看是否生效"></a>查看是否生效</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system | grep kube-proxy | awk &#39;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&#39;</span><br></pre></td></tr></table></figure>

<h2 id="查看ipvs规则"><a href="#查看ipvs规则" class="headerlink" title="查看ipvs规则"></a>查看ipvs规则</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipvsadm -Ln</span><br></pre></td></tr></table></figure>





]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>利用 kubeadm 部署 Kubernetes v1.15.x</title>
    <url>/posts/57614a8.html</url>
    <content><![CDATA[<p>官方文档：<a href="https://kubernetes.io/docs/setup/independent/high-availability/">https://kubernetes.io/docs/setup/independent/high-availability/</a></p>
<h1 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h1><p>或者开放需要的端口 详见<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="禁用selinux"><a href="#禁用selinux" class="headerlink" title="禁用selinux"></a>禁用selinux</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br></pre></td></tr></table></figure>

<h1 id="启用net-bridge-bridge-nf-call-ip6tables和net-bridge-bridge-nf-call-iptables"><a href="#启用net-bridge-bridge-nf-call-ip6tables和net-bridge-bridge-nf-call-iptables" class="headerlink" title="启用net.bridge.bridge-nf-call-ip6tables和net.bridge.bridge-nf-call-iptables"></a>启用net.bridge.bridge-nf-call-ip6tables和net.bridge.bridge-nf-call-iptables</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;  &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">net.ipv4.ip_forward &#x3D; 1</span><br><span class="line">vm.swappiness&#x3D;0</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br></pre></td></tr></table></figure>

<h1 id="禁用swap"><a href="#禁用swap" class="headerlink" title="禁用swap"></a>禁用swap</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">swapoff -a</span><br><span class="line"></span><br><span class="line">修改&#x2F;etc&#x2F;fstab 文件，注释掉 SWAP 的自动挂载. </span><br><span class="line">使用free -m确认swap已经关闭。</span><br></pre></td></tr></table></figure>

<h1 id="加载ipvs相关模块，kube-proxy开启ipvs的前置条件"><a href="#加载ipvs相关模块，kube-proxy开启ipvs的前置条件" class="headerlink" title="加载ipvs相关模块，kube-proxy开启ipvs的前置条件"></a>加载ipvs相关模块，kube-proxy开启ipvs的前置条件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt; &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &lt;&lt;EOF</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">ipvs_mods_dir&#x3D;&quot;&#x2F;usr&#x2F;lib&#x2F;modules&#x2F;$(uname -r)&#x2F;kernel&#x2F;net&#x2F;netfilter&#x2F;ipvs&quot;</span><br><span class="line">for i in $(ls $ipvs_mods_dir | grep -o &quot;^[^.]*&quot;); do</span><br><span class="line">    &#x2F;sbin&#x2F;modinfo -F filename $i &amp;&gt; &#x2F;dev&#x2F;null</span><br><span class="line">    if [ $? -eq 0 ]; then</span><br><span class="line">        &#x2F;sbin&#x2F;modprobe $i</span><br><span class="line">    fi</span><br><span class="line">done</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">EOF</span><br><span class="line">chmod 755 &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &amp;&amp; bash &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4</span><br></pre></td></tr></table></figure>
<p>上面脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。接下来还需要确保各个节点上已经安装了ipset软件包。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install ipset ipvsadm -y</span><br></pre></td></tr></table></figure>

<h1 id="安装-docker"><a href="#安装-docker" class="headerlink" title="安装 docker"></a>安装 docker</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装docker</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line"></span><br><span class="line"># 查看docker版本</span><br><span class="line">yum list docker-ce.x86_64  --showduplicates |sort -r</span><br><span class="line"></span><br><span class="line"># Kubernetes 1.15当前支持的docker版本列表是1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09</span><br><span class="line">yum install -y --setopt&#x3D;obsoletes&#x3D;0 \</span><br><span class="line">  docker-ce-18.09.7-3.el7</span><br><span class="line"></span><br><span class="line"># 启动docker</span><br><span class="line">systemctl enable docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"># 修改docker cgroup driver为systemd</span><br><span class="line">根据文档中的内容，详见https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;setup&#x2F;production-environment&#x2F;container-runtimes&#x2F;，对于使用systemd作为init system的Linux的发行版，使用systemd作为docker的cgroup driver可以确保服务器节点在资源紧张的情况更加稳定。</span><br><span class="line"></span><br><span class="line">cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check&#x3D;true&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d</span><br><span class="line"></span><br><span class="line"># 重启docker</span><br><span class="line">systemctl restart docker</span><br><span class="line"></span><br><span class="line"># 更改默认存储目录</span><br><span class="line">vim &#x2F;etc&#x2F;docker&#x2F;daemon.json </span><br><span class="line"></span><br><span class="line">&#123;&quot;graph&quot;: &quot;&#x2F;new-path&#x2F;docker&quot;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 查看docker cgroup driver</span><br><span class="line">docker info | grep Cgroup</span><br></pre></td></tr></table></figure>

<h1 id="安装kubeadm"><a href="#安装kubeadm" class="headerlink" title="安装kubeadm"></a>安装kubeadm</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加阿里云仓库</span><br><span class="line">cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 安装kubeadm</span><br><span class="line">yum install -y kubelet-1.15.0 kubeadm-1.15.0 kubectl-1.15.0</span><br><span class="line"></span><br><span class="line">从安装结果可以看出还安装了cri-tools, kubernetes-cni, socat三个依赖：</span><br><span class="line"></span><br><span class="line">1、官方从Kubernetes 1.14开始将cni依赖升级到了0.7.5版本</span><br><span class="line">2、socat是kubelet的依赖</span><br><span class="line">3、cri-tools是CRI(Container Runtime Interface)容器运行时接口的命令行工具</span><br><span class="line"></span><br><span class="line"># 启动kubectl</span><br><span class="line">systemctl enable kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure>

<h1 id="部署master节点"><a href="#部署master节点" class="headerlink" title="部署master节点"></a>部署master节点</h1><p>官方推荐我们使用–config指定配置文件，并在配置文件中指定原来这些flag所配置的内容，详见<a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/。这也是Kubernetes为了支持动态Kubelet配置，详见https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/。">https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/。这也是Kubernetes为了支持动态Kubelet配置，详见https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/。</a></p>
<p>使用kubeadm config print init-defaults可以打印集群初始化默认的使用的配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1beta2</span><br><span class="line">bootstrapTokens:</span><br><span class="line">- groups:</span><br><span class="line">  - system:bootstrappers:kubeadm:default-node-token</span><br><span class="line">  token: abcdef.0123456789abcdef</span><br><span class="line">  ttl: 24h0m0s</span><br><span class="line">  usages:</span><br><span class="line">  - signing</span><br><span class="line">  - authentication</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: 1.2.3.4</span><br><span class="line">  bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: &#x2F;var&#x2F;run&#x2F;dockershim.sock</span><br><span class="line">  name: master</span><br><span class="line">  taints:</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node-role.kubernetes.io&#x2F;master</span><br><span class="line">---</span><br><span class="line">apiServer:</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1beta2</span><br><span class="line">certificatesDir: &#x2F;etc&#x2F;kubernetes&#x2F;pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns:</span><br><span class="line">  type: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    dataDir: &#x2F;var&#x2F;lib&#x2F;etcd</span><br><span class="line">imageRepository: k8s.gcr.io</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.14.0</span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  serviceSubnet: 10.96.0.0&#x2F;12</span><br><span class="line">scheduler: &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件kubeadm.yaml：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1beta2</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: 10.168.4.5</span><br><span class="line">  bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">  taints:</span><br><span class="line">  - effect: PreferNoSchedule</span><br><span class="line">    key: node-role.kubernetes.io&#x2F;master</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1beta2</span><br><span class="line">imageRepository: registry.aliyuncs.com&#x2F;google_containers</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.15.0</span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.244.0.0&#x2F;16</span><br></pre></td></tr></table></figure>

<p>使用kubeadm默认配置初始化的集群，会在master节点打上node-role.kubernetes.io/master:NoSchedule的污点，阻止master节点接受调度运行工作负载。这里测试环境只有两个节点，所以将这个taint修改为node-role.kubernetes.io/master:PreferNoSchedule。</p>
<p>修改advertiseAddress api地址、imageRepository 阿里云仓库地址、podSubnet 网络地址</p>
<h2 id="kubeadm初始化集群"><a href="#kubeadm初始化集群" class="headerlink" title="kubeadm初始化集群"></a>kubeadm初始化集群</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm init --config kubeadm.yaml</span><br></pre></td></tr></table></figure>

<p>查看集群状态，确认个组件都处于healthy状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get cs</span><br><span class="line">NAME                 STATUS    MESSAGE             ERROR</span><br><span class="line">scheduler            Healthy   ok                  </span><br><span class="line">controller-manager   Healthy   ok                  </span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>

<h2 id="安装Pod-Network"><a href="#安装Pod-Network" class="headerlink" title="安装Pod Network"></a>安装Pod Network</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml</span><br></pre></td></tr></table></figure>
<p>如果Node有多个网卡的话，详见<a href="https://github.com/kubernetes/kubernetes/issues/39701，目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上--iface=">https://github.com/kubernetes/kubernetes/issues/39701，目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上--iface=</a><iface-name></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">containers:</span><br><span class="line">      - name: kube-flannel</span><br><span class="line">        image: quay.io&#x2F;coreos&#x2F;flannel:v0.11.0-amd64</span><br><span class="line">        command:</span><br><span class="line">        - &#x2F;opt&#x2F;bin&#x2F;flanneld</span><br><span class="line">        args:</span><br><span class="line">        - --ip-masq</span><br><span class="line">        - --kube-subnet-mgr</span><br><span class="line">        - --iface&#x3D;eth1</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<h2 id="测试集群DNS是否可用"><a href="#测试集群DNS是否可用" class="headerlink" title="测试集群DNS是否可用"></a>测试集群DNS是否可用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create curl --image&#x3D;radial&#x2F;busyboxplus:curl -it -rm</span><br></pre></td></tr></table></figure>
<p>进入后执行nslookup kubernetes.default确认解析正常</p>
<h2 id="如何从集群中移除Node"><a href="#如何从集群中移除Node" class="headerlink" title="如何从集群中移除Node"></a>如何从集群中移除Node</h2><p>如果需要从集群中移除node2这个Node执行下面的命令<br>在master节点上执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl drain node2 --delete-local-data --force --ignore-daemonsets</span><br><span class="line">kubectl delete node node2</span><br></pre></td></tr></table></figure>

<p>在node2上执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm reset</span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ip link delete cni0</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ip link delete flannel.1</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;cni&#x2F;</span><br></pre></td></tr></table></figure>

<p>在master上执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl delete node node2</span><br></pre></td></tr></table></figure>

<h2 id="kube-proxy开启ipvs"><a href="#kube-proxy开启ipvs" class="headerlink" title="kube-proxy开启ipvs"></a>kube-proxy开启ipvs</h2><p>修改ConfigMap的kube-system/kube-proxy中的config.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl edit cm kube-proxy -n kube-system</span><br><span class="line"># 39行 mode: &quot;&quot; -&gt; mode: &quot;ipvs&quot;</span><br></pre></td></tr></table></figure>

<p>重启所有工作节点的kube-proxy pod</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system | grep kube-proxy | awk &#39;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&#39;</span><br></pre></td></tr></table></figure>

<p>查看是否生效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system | grep kube-proxy | awk &#39;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&#39;</span><br></pre></td></tr></table></figure>

<p>查看日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl -n kube-system logs kube-proxy-xddn9</span><br></pre></td></tr></table></figure>

<p>查看ipvs规则</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipvsadm -Ln</span><br></pre></td></tr></table></figure>





]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes 命令记录</title>
    <url>/posts/b006f55c.html</url>
    <content><![CDATA[<h1 id="操作基本命令"><a href="#操作基本命令" class="headerlink" title="操作基本命令"></a>操作基本命令</h1><p>通过yaml文件创建：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create -f xxx.yaml （不建议使用，无法更新，必须先delete）</span><br><span class="line"></span><br><span class="line">kubectl apply -f xxx.yaml （创建+更新，可以重复使用）</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>通过yaml文件删除：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl delete -f xxx.yaml</span><br></pre></td></tr></table></figure>

<p>查看kube-system namespace下面的pod/svc/deployment 等等（-o wide  选项可以查看存在哪个对应的节点）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod&#x2F;svc&#x2F;deployment -n kube-system</span><br></pre></td></tr></table></figure>

<p>查看所有namespace下面的pod/svc/deployment等等</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod&#x2F;svc&#x2F;deployment --all-namcpaces</span><br></pre></td></tr></table></figure>

<p>重启pod（无法删除对应的应用，因为存在deployment/rc之类的副本控制器，删除pod也会重新拉起来）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system</span><br></pre></td></tr></table></figure>

<p>查看pod描述：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl describe pod XXX -n kube-system</span><br></pre></td></tr></table></figure>

<p>查看pod 日志 （如果pod有多个容器需要加-c 容器名）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl logs xxx -n kube-system</span><br></pre></td></tr></table></figure>

<p>删除应用（先确定是由说明创建的，再删除对应的kind）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl delete deployment xxx -n kube-system</span><br></pre></td></tr></table></figure>

<p>根据label删除：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl delete pod -l app&#x3D;flannel -n kube-system</span><br></pre></td></tr></table></figure>

<p>扩容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl scale deployment spark-worker-deployment --replicas&#x3D;8</span><br></pre></td></tr></table></figure>


<p>导出配置文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">导出proxy</span><br><span class="line">kubectl get ds -n kube-system -l k8s-app&#x3D;kube-proxy -o yaml&gt;kube-proxy-ds.yaml</span><br><span class="line">导出kube-dns</span><br><span class="line">kubectl get deployment -n kube-system -l k8s-app&#x3D;kube-dns -o yaml &gt;kube-dns-dp.yaml</span><br><span class="line">kubectl get services -n kube-system -l k8s-app&#x3D;kube-dns -o yaml &gt;kube-dns-services.yaml</span><br><span class="line">导出所有 configmap</span><br><span class="line">kubectl get configmap -n kube-system -o wide -o yaml &gt; configmap.yaml</span><br></pre></td></tr></table></figure>

<p>Kubernetes Dashboard获取admin-user的访问令牌</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &#39;&#123;print $1&#125;&#39;)</span><br></pre></td></tr></table></figure>

<h1 id="复杂操作命令"><a href="#复杂操作命令" class="headerlink" title="复杂操作命令"></a>复杂操作命令</h1><p>删除kube-system 下Evicted状态的所有pod：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pods -n kube-system |grep Evicted| awk &#39;&#123;print $1&#125;&#39;|xargs kubectl delete pod  -n kube-system</span><br></pre></td></tr></table></figure>

<h1 id="维护环境相关命令："><a href="#维护环境相关命令：" class="headerlink" title="维护环境相关命令："></a>维护环境相关命令：</h1><p>回滚记录历史</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl  apply -f xxx.yaml --record</span><br></pre></td></tr></table></figure>

<p>查看当前回滚状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl rollout status statefulset xxx -w</span><br></pre></td></tr></table></figure>

<p>查看回滚历史版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl rollout history statefulset xxx</span><br></pre></td></tr></table></figure>

<p>回滚到指定版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl rollout undo statefulset xxx --to-revision&#x3D;2</span><br></pre></td></tr></table></figure>

<p>重启kubelet服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<p>修改启动参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d&#x2F;10-kubeadm.conf</span><br></pre></td></tr></table></figure>

<p>查看集群信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl cluster-info</span><br></pre></td></tr></table></figure>

<p>查看各组件信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get componentstatuses</span><br></pre></td></tr></table></figure>

<p>查看kubelet进程启动参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -ef | grep kubelet</span><br></pre></td></tr></table></figure>

<p>查看日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">journalctl -u kubelet -f</span><br></pre></td></tr></table></figure>

<p>设为不可调度状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl cordon node1</span><br></pre></td></tr></table></figure>


<p>将pod赶到其他节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl drain node1</span><br></pre></td></tr></table></figure>

<p>解除不可调度状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl uncordon node1</span><br></pre></td></tr></table></figure>

<p>master运行pod</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes master.k8s node-role.kubernetes.io&#x2F;master-</span><br></pre></td></tr></table></figure>

<p>master不运行pod</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes master.k8s node-role.kubernetes.io&#x2F;master&#x3D;:NoSchedule</span><br></pre></td></tr></table></figure>

<p>pod Terminating 删不掉</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl delete pod mpmt-user --namespace&#x3D;lolaage --grace-period&#x3D;0 --force</span><br><span class="line"></span><br><span class="line">https:&#x2F;&#x2F;kubernetes.io&#x2F;zh&#x2F;docs&#x2F;concepts&#x2F;workloads&#x2F;pods&#x2F;pod&#x2F;</span><br></pre></td></tr></table></figure>

<p>获取节点列表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#get nodes</span><br><span class="line">nodes&#x3D;$(kubectl get nodes -o jsonpath&#x3D;&#39;&#123;range.items[*].metadata&#125;&#123;.name&#125; &#123;end&#125;&#39;)</span><br><span class="line"></span><br><span class="line">#get nodes ip</span><br><span class="line">nodes&#x3D;$(kubectl get nodes -o jsonpath&#x3D;&#39;&#123;range .items[*].status.addresses[?(@.type&#x3D;&#x3D;&quot;ExternalIP&quot;)]&#125;&#123;.address&#125; &#123;end&#125;&#39;)</span><br></pre></td></tr></table></figure>

<p>重新生成链接Token</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>

<p>只删除 ReplicaSet,不影响它的 Pod</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">使用kubectl delete 命令并设置 --cascade&#x3D;false 选项</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>日常部署记录</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernets依赖docker images无端端自动被清除</title>
    <url>/posts/81b412ae.html</url>
    <content><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>k8s更新项目时，提示kubernets 出现The connection to the server 172.16.0.17:6443 was refused - did you specify the right host or port? </p>
<a id="more"></a>

<h1 id="原因、解决"><a href="#原因、解决" class="headerlink" title="原因、解决"></a>原因、解决</h1><p>重启了docker、kuberlet 执行kubectl get pods时，依然提示The connection to the server 172.16.0.17:6443 was refused - did you specify the right host or port?，后查看docker ps、docker images时，k8s依赖的有些images自动被清理掉了，后通过下载images，k8s自然恢复正常，后又images自动又被清理，看了硬盘空间df -h，出现了85%，难道是空间满了会自动清理docker images？后通过删除一些没用的文件释放空间，远低于硬盘空间85%，下载images，k8s自然恢复正常，而没出现自动清理images，这个为何自动会清理，查看了网上至今还没找到相关资料，后续发现继而补充。</p>
<p>20200330更新<br>kubelet 配置文件 /var/lib/kubelet/config.yaml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">imageGCHighThresholdPercent: 85  触发镜像垃圾回收的磁盘使用率百分比。默认值为 85%</span><br><span class="line">imageGCLowThresholdPercent: 80   镜像垃圾回收试图释放资源后达到的磁盘使用率百分比。默认值为 80%</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernets</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes通过TLS安全访问</title>
    <url>/posts/a808c5a1.html</url>
    <content><![CDATA[<p>外部访问k8s里的服务，都是直接以http方式进行的，缺少TLS安全</p>
<a id="more"></a>

<h1 id="生成并信任自签名证书"><a href="#生成并信任自签名证书" class="headerlink" title="生成并信任自签名证书"></a>生成并信任自签名证书</h1><p>首先这里生成自签名的服务器证书，官方介绍了easyrsa, openssl 、 cfssl三个工具，这里使用cfssl</p>
<h2 id="安装CFSSL"><a href="#安装CFSSL" class="headerlink" title="安装CFSSL"></a>安装CFSSL</h2><p>直接使用二进制源码包安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl_linux-amd64</span><br><span class="line">chmod +x cfssl_linux-amd64</span><br><span class="line">mv cfssl_linux-amd64 &#x2F;usr&#x2F;sbin&#x2F;cfssl</span><br><span class="line"></span><br><span class="line">wget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssljson_linux-amd64</span><br><span class="line">chmod +x cfssljson_linux-amd64</span><br><span class="line">mv cfssljson_linux-amd64 &#x2F;usr&#x2F;sbin&#x2F;cfssljson</span><br><span class="line"></span><br><span class="line">wget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl-certinfo_linux-amd64</span><br><span class="line">chmod +x cfssl-certinfo_linux-amd64</span><br><span class="line">mv cfssl-certinfo_linux-amd64 &#x2F;usr&#x2F;sbin&#x2F;cfssl-certinfo</span><br></pre></td></tr></table></figure>

<h2 id="生成默认配置文件"><a href="#生成默认配置文件" class="headerlink" title="生成默认配置文件"></a>生成默认配置文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl print-defaults config &gt; config.json</span><br><span class="line">cfssl print-defaults csr &gt; csr.json</span><br></pre></td></tr></table></figure>

<h2 id="生成自定义的config-json文件"><a href="#生成自定义的config-json文件" class="headerlink" title="生成自定义的config.json文件"></a>生成自定义的config.json文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp config.json ca-config.json</span><br></pre></td></tr></table></figure>

<h2 id="生成ca和server的证书请求json文件"><a href="#生成ca和server的证书请求json文件" class="headerlink" title="生成ca和server的证书请求json文件"></a>生成ca和server的证书请求json文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp csr.json ca-csr.json</span><br><span class="line">cp csr.json server-csr.json</span><br></pre></td></tr></table></figure>

<h2 id="编辑ca-config-json"><a href="#编辑ca-config-json" class="headerlink" title="编辑ca-config.json"></a>编辑ca-config.json</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;signing&quot;: &#123;</span><br><span class="line">        &quot;default&quot;: &#123;</span><br><span class="line">            &quot;expiry&quot;: &quot;168h&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;profiles&quot;: &#123;</span><br><span class="line">            &quot;k8s-local&quot;: &#123;</span><br><span class="line">                &quot;expiry&quot;: &quot;8760h&quot;,</span><br><span class="line">                &quot;usages&quot;: [</span><br><span class="line">                    &quot;signing&quot;,</span><br><span class="line">                    &quot;key encipherment&quot;,</span><br><span class="line">                    &quot;server auth&quot;,</span><br><span class="line">                    &quot;client auth&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="编辑ca-csr-json"><a href="#编辑ca-csr-json" class="headerlink" title="编辑ca-csr.json"></a>编辑ca-csr.json</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;k8s-local&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;GuangDong&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;Shenzhen&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;my self signed certificate&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;self signed&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="编辑server-csr-json"><a href="#编辑server-csr-json" class="headerlink" title="编辑server-csr.json"></a>编辑server-csr.json</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;k8s.local&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">        &quot;127.0.0.1&quot;,</span><br><span class="line">        &quot;*.k8s.local&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;GuangDong&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;Shenzhen&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;my self signed certificate&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;self signed&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="生成CA证书及服务器证书"><a href="#生成CA证书及服务器证书" class="headerlink" title="生成CA证书及服务器证书"></a>生成CA证书及服务器证书</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cfssl gencert -initca ca-csr.json | cfssljson -bare ca</span><br><span class="line">cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem --config&#x3D;ca-config.json -profile&#x3D;k8s-local server-csr.json | cfssljson -bare server</span><br></pre></td></tr></table></figure>

<p>得到ca.pem，server-key.pem，server.pem三个证书文件，其中ca.pem是ca的证书，server-key.pem是服务器证书的密钥，server.pem是服务器证书。</p>
<h1 id="在k8s里使用自签名证书"><a href="#在k8s里使用自签名证书" class="headerlink" title="在k8s里使用自签名证书"></a>在k8s里使用自签名证书</h1><h2 id="创建默认的tls-secret"><a href="#创建默认的tls-secret" class="headerlink" title="创建默认的tls secret"></a>创建默认的tls secret</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl -n kube-system create secret tls default-tls-cert --key&#x3D;server-key.pem --cert&#x3D;server.pem</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
        <tag>tls</tag>
      </tags>
  </entry>
  <entry>
    <title>linux面试收集</title>
    <url>/posts/23e04480.html</url>
    <content><![CDATA[<h2 id="Linux系统的开机启动顺序"><a href="#Linux系统的开机启动顺序" class="headerlink" title="Linux系统的开机启动顺序"></a>Linux系统的开机启动顺序</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">加载BIOS–&gt;读取MBR–&gt;Boot Loader–&gt;加载内核–&gt;用户层init用inittab文件来设定系统运行的等级(一般3或者5，3是多用户命令行，5是界面)–&gt;init进程执行rc.syninit–&gt;启动内核模块–&gt;执行不同级别运行的脚本程序–&gt;执行&#x2F;etc&#x2F;rc.d&#x2F;rc.local(本地运行服务)–&gt;执行&#x2F;bin&#x2F;login,就可以登录了。</span><br></pre></td></tr></table></figure>

<h2 id="FTP的主动模式和被动模式"><a href="#FTP的主动模式和被动模式" class="headerlink" title="FTP的主动模式和被动模式"></a>FTP的主动模式和被动模式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FTP协议有两种工作方式：PORT方式和PASV方式，中文意思为主动式和被动式。</span><br><span class="line"></span><br><span class="line">PORT（主动）方式的连接过程是：客户端向服务器的FTP端口（默认是21）发送连接请求，服务器接受连接，建立一条命令链路。当需要传送数据时，客户端在命令链路上用 PORT 命令告诉服务器：“我打开了XX端口，你过来连接我”。于是服务器从20端口向客户端的 XX 端口发送连接请求，建立一条数据链路来传送数据。</span><br><span class="line"></span><br><span class="line">PASV（被动）方式的连接过程是：客户端向服务器的FTP端口（默认是21）发送连接请求，服务器接受连接，建立一条命令链路。当需要传送数据时，服务器在命令链路上用PASV 命令告诉客户端：“我打开了XX端口，你过来连接我”。于是客户端向服务器的 XX 端口发送连接请求，建立一条数据链路来传送数据。</span><br><span class="line"></span><br><span class="line">从上面可以看出，两种方式的命令链路连接方法是一样的，而数据链路的建立方法就完 全不同。</span><br></pre></td></tr></table></figure>

<h2 id="系统中出现大量不可中断进程和僵尸进程怎么办"><a href="#系统中出现大量不可中断进程和僵尸进程怎么办" class="headerlink" title="系统中出现大量不可中断进程和僵尸进程怎么办"></a>系统中出现大量不可中断进程和僵尸进程怎么办</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">进程常见的五种状态：</span><br><span class="line">（1）R 是 Running 或 Runnable 的缩写，表示进程在 CPU 的就绪队列中，正在运行或者正在等待运行。</span><br><span class="line"></span><br><span class="line">（2）D 是 Disk Sleep 的缩写，也就是不可中断状态睡眠（Uninterruptible Sleep），一般表示进程正在跟硬件交互，并且交互过程不允许被其他进程或中断打断。</span><br><span class="line"></span><br><span class="line">注意：进程长时间处于不可中断状态，通常表示系统有 I&#x2F;O 性能问题。</span><br><span class="line"></span><br><span class="line">（3）Z 是 Zombie 的缩写，如果你玩过“植物大战僵尸”这款游戏，应该知道它的意思。它表示僵尸进程，也就是进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID 等）。</span><br><span class="line"></span><br><span class="line">（4）S 是 Interruptible Sleep 的缩写，也就是可中断状态睡眠，表示进程因为等待某个事件而被系统挂起。当进程等待的事件发生时，它会被唤醒并进入 R 状态。</span><br><span class="line"></span><br><span class="line">（5）I 是 Idle 的缩写，也就是空闲状态，用在不可中断睡眠的内核线程上。前面说了，硬件交互导致的不可中断进程用 D 表示，但对某些内核线程来说，它们有可能实际上并没有任何负载，用 Idle 正是为了区分这种情况。要注意，D 状态的进程会导致平均负载升高，I 状态的进程却不会。</span><br><span class="line"></span><br><span class="line">进程不常见的两种状态:</span><br><span class="line">1）T 或者 t，也就是 Stopped 或 Traced 的缩写，表示进程处于暂停或者跟踪状态。</span><br><span class="line"></span><br><span class="line">向一个进程发送 SIGSTOP 信号，它就会因响应这个信号变成暂停状态（Stopped）；再向它发送 SIGCONT 信号，进程又会恢复运行（如果进程是终端里直接启动的，则需要你用 fg 命令，恢复到前台运行）。</span><br><span class="line"></span><br><span class="line">而当你用调试器（如 gdb）调试一个进程时，在使用断点中断进程后，进程就会变成跟踪状态，这其实也是一种特殊的暂停状态，只不过你可以用调试器来跟踪并按需要控制进程的运行。</span><br><span class="line"></span><br><span class="line">（2）X，也就是 Dead 的缩写，表示进程已经消亡，所以你不会在top 或者 ps 命令中看到它。</span><br><span class="line"></span><br><span class="line">注意：Ss+：S表示可中断睡眠状态，s表示这个进程是一个会话的领导进程，而+表示前台进程组。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">安装dstat</span><br><span class="line">（1）CentOS：yum install -y dstat</span><br><span class="line"></span><br><span class="line">（2）Ubuntu：apt install dstat</span><br><span class="line">dstat是一个新的性能工具，它吸收了vmstat、iostat、ifstat等几种工具的优点，可以同时观察系统的CPU、磁盘I&#x2F;O、网络以及内存使用情况。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">（1）dstat 1 10 （间隔1秒输出10组数据）</span><br><span class="line"></span><br><span class="line">（2）pidstat -d -p 4344 1 3 （-d 展示 I&#x2F;O 统计数据，-p 指定进程号，间隔 1 秒输出 3 组数据）</span><br><span class="line"></span><br><span class="line">（3）strace -p 6082 （-p指定进程号）Strace最常用的跟踪进程系统调用的工具。</span><br><span class="line"></span><br><span class="line">（4）perf record -g（终端运行十五分钟左右，再ctrl+c）</span><br><span class="line"></span><br><span class="line">（5）perf report</span><br><span class="line"></span><br><span class="line">僵尸进程的处理</span><br><span class="line">要解决僵尸进程，就要找到它的根儿，也就是找出父进程，然后在父进程里解决。</span><br><span class="line"></span><br><span class="line">（1）pstree -aps 3084（-a表示输出命令行选项，p表示PID，s表示指定进程的父进程）</span><br><span class="line"></span><br><span class="line">（2）接着查看 app 应用程序的代码，看看子进程结束的处理是否正确，比如有没有调用wait() 或 waitpid() ，抑或是，有没有注册 SIGCHLD 信号的处理函数。</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>面试收集</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernets的健康检测</title>
    <url>/posts/997b2908.html</url>
    <content><![CDATA[<h1 id="Pod健康检测机制"><a href="#Pod健康检测机制" class="headerlink" title="Pod健康检测机制"></a>Pod健康检测机制</h1><p>对于Pod的健康状态检测，kubernetes提供了两类探针(Probe)来执行对Pod的健康状态检测：</p>
<a id="more"></a>
<ul>
<li><p>LivenessProbe探针：<br>用于判断容器是否存活，即Pod是否为running状态，如果LivenessProbe探针探测到容器不健康，则kubelet将kill掉容器，并根据容器的重启策略是否重启，如果一个容器不包含LivenessProbe探针，则Kubelet认为容器的LivenessProbe探针的返回值永远成功。</p>
</li>
<li><p>ReadinessProbe探针：<br>用于判断容器是否启动完成，即容器的Ready是否为True，可以接收请求，如果ReadinessProbe探测失败，则容器的Ready将为False，控制器将此Pod的Endpoint从对应的service的Endpoint列表中移除，从此不再将任何请求调度此Pod上，直到下次探测成功。</p>
</li>
</ul>
<p>每类探针都支持三种探测方法：</p>
<ul>
<li><p>exec：通过执行命令来检查服务是否正常，针对复杂检测或无HTTP接口的服务，命令返回值为0则表示容器健康。</p>
</li>
<li><p>httpGet：通过发送http请求检查服务是否正常，返回200-399状态码则表明容器健康。</p>
</li>
<li><p>tcpSocket：通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康。</p>
</li>
</ul>
<p>探针探测的结果有以下三者之一：</p>
<ul>
<li><p>Success：Container通过了检查。</p>
</li>
<li><p>Failure：Container未通过检查。</p>
</li>
<li><p>Unknown：未能执行检查，因此不采取任何措施。</p>
</li>
</ul>
<h1 id="LivenessProbe探针配置"><a href="#LivenessProbe探针配置" class="headerlink" title="LivenessProbe探针配置"></a>LivenessProbe探针配置</h1><h2 id="通过exec方式做健康探测"><a href="#通过exec方式做健康探测" class="headerlink" title="通过exec方式做健康探测"></a>通过exec方式做健康探测</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">exec-liveness.yaml  </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    test: liveness</span><br><span class="line">  name: liveness-exec</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: liveness</span><br><span class="line">    image: k8s.gcr.io&#x2F;busybox</span><br><span class="line">    args:</span><br><span class="line">    - &#x2F;bin&#x2F;sh</span><br><span class="line">    - -c</span><br><span class="line">    - touch &#x2F;tmp&#x2F;healthy; sleep 30; rm -rf &#x2F;tmp&#x2F;healthy; sleep 600</span><br><span class="line">    livenessProbe:</span><br><span class="line">      exec:</span><br><span class="line">        command:</span><br><span class="line">        - cat</span><br><span class="line">        - &#x2F;tmp&#x2F;healthy</span><br><span class="line">      initialDelaySeconds: 5</span><br><span class="line">      periodSeconds: 5</span><br></pre></td></tr></table></figure>

<h2 id="通过HTTP方式做健康探测"><a href="#通过HTTP方式做健康探测" class="headerlink" title="通过HTTP方式做健康探测"></a>通过HTTP方式做健康探测</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pods&#x2F;probe&#x2F;http-liveness.yaml  </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    test: liveness</span><br><span class="line">  name: liveness-http</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: liveness</span><br><span class="line">    image: k8s.gcr.io&#x2F;liveness</span><br><span class="line">    args:</span><br><span class="line">    - &#x2F;server</span><br><span class="line">    livenessProbe:</span><br><span class="line">      httpGet:</span><br><span class="line">        path: &#x2F;healthz</span><br><span class="line">        port: 8080</span><br><span class="line">        httpHeaders:</span><br><span class="line">        - name: X-Custom-Header</span><br><span class="line">          value: Awesome</span><br><span class="line">      initialDelaySeconds: 3</span><br><span class="line">      periodSeconds: 3</span><br></pre></td></tr></table></figure>

<h2 id="通过TCP方式做健康探测"><a href="#通过TCP方式做健康探测" class="headerlink" title="通过TCP方式做健康探测"></a>通过TCP方式做健康探测</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pods&#x2F;probe&#x2F;tcp-liveness-readiness.yaml  </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: goproxy</span><br><span class="line">  labels:</span><br><span class="line">    app: goproxy</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: goproxy</span><br><span class="line">    image: k8s.gcr.io&#x2F;goproxy:0.1</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    readinessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 8080</span><br><span class="line">      initialDelaySeconds: 5</span><br><span class="line">      periodSeconds: 10</span><br><span class="line">    livenessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 8080</span><br><span class="line">      initialDelaySeconds: 15</span><br><span class="line">      periodSeconds: 20</span><br></pre></td></tr></table></figure>

<h1 id="ReadinessProbe探针配置"><a href="#ReadinessProbe探针配置" class="headerlink" title="ReadinessProbe探针配置"></a>ReadinessProbe探针配置</h1><p>ReadinessProbe探针的使用场景livenessProbe稍有不同，有的时候应用程序可能暂时无法接受请求，比如Pod已经Running了，但是容器内应用程序尚未启动成功，在这种情况下，如果没有ReadinessProbe，则Kubernetes认为它可以处理请求了，然而此时，我们知道程序还没启动成功是不能接收用户请求的，所以不希望kubernetes把请求调度给它，则使用ReadinessProbe探针。<br>ReadinessProbe和livenessProbe可以使用相同探测方式，只是对Pod的处置方式不同，ReadinessProbe是将Pod IP:Port从对应的EndPoint列表中删除，而livenessProbe则Kill容器并根据Pod的重启策略来决定作出对应的措施。<br>ReadinessProbe探针探测容器是否已准备就绪，如果未准备就绪则kubernetes不会将流量转发给此Pod。</p>
<p>ReadinessProbe探针与livenessProbe一样也支持exec、httpGet、TCP的探测方式，配置方式相同，只不过是将livenessProbe字段修改为ReadinessProbe。</p>
<h1 id="参数介绍"><a href="#参数介绍" class="headerlink" title="参数介绍"></a>参数介绍</h1><p>第一个参数叫 initialDelaySeconds，它表示的是说这个 pod 启动延迟多久进行一次检查，比如说现在有一个 Java 的应用，它启动的时间可能会比较长，因为涉及到 jvm 的启动，包括 Java 自身 jar 的加载。所以前期，可能有一段时间是没有办法被检测的，而这个时间又是可预期的，那这时可能要设置一下 initialDelaySeconds；<br>第二个是 periodSeconds，它表示的是检测的时间间隔，正常默认的这个值是 10 秒；<br>第三个字段是 timeoutSeconds，它表示的是检测的超时时间，当超时时间之内没有检测成功，那它会认为是失败的一个状态；<br>第四个是 successThreshold，它表示的是：当这个 pod 从探测失败到再一次判断探测成功，所需要的阈值次数，默认情况下是 1 次，表示原本是失败的，那接下来探测这一次成功了，就会认为这个 pod 是处在一个探针状态正常的一个状态；<br>最后一个参数是 failureThreshold，它表示的是探测失败的重试次数，默认值是 3，表示的是当从一个健康的状态连续探测 3 次失败，那此时会判断当前这个pod的状态处在一个失败的状态。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Siege压力测试Web服务器</title>
    <url>/posts/7f792ae3.html</url>
    <content><![CDATA[<p>官方网站：<a href="http://www.joedog.org/">http://www.joedog.org/</a><br>Siege是一款高性能的Http压力测试工具。<br>Siege支持身份验证、cookies、http、https和ftp协议。</p>
<a id="more"></a>
<h1 id="yum安装"><a href="#yum安装" class="headerlink" title="yum安装"></a>yum安装</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install epel-release</span><br><span class="line">yum install siege</span><br></pre></td></tr></table></figure>

<h1 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum groupinstall &#39;Development Tools&#39;</span><br><span class="line">wget http:&#x2F;&#x2F;download.joedog.org&#x2F;siege&#x2F;siege-latest.tar.gz</span><br><span class="line">tar -zxvf siege-latest.tar.gz</span><br><span class="line">cd siege-*&#x2F;</span><br><span class="line">sudo .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local --with-ssl&#x3D;&#x2F;usr&#x2F;bin&#x2F;openssl</span><br><span class="line">sudo make &amp;&amp; make install</span><br></pre></td></tr></table></figure>

<h1 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &#x2F;etc&#x2F;siege&#x2F;siegerc |egrep -v &quot;^$|#&quot;</span><br><span class="line"></span><br><span class="line">verbose &#x3D; true</span><br><span class="line">color &#x3D; on</span><br><span class="line">quiet &#x3D; false</span><br><span class="line">show-logfile &#x3D; true</span><br><span class="line">logging &#x3D; false</span><br><span class="line">logfile &#x3D; $&#123;HOME&#125;&#x2F;siege.log</span><br><span class="line">gmethod &#x3D; HEAD</span><br><span class="line">parser &#x3D; true</span><br><span class="line">nofollow &#x3D; ad.doubleclick.net</span><br><span class="line">nofollow &#x3D; pagead2.googlesyndication.com</span><br><span class="line">nofollow &#x3D; ads.pubsqrd.com</span><br><span class="line">nofollow &#x3D; ib.adnxs.com</span><br><span class="line">limit &#x3D; 255</span><br><span class="line">protocol &#x3D; HTTP&#x2F;1.1</span><br><span class="line">chunked &#x3D; true</span><br><span class="line">cache &#x3D; false</span><br><span class="line">connection &#x3D; close</span><br><span class="line">concurrent &#x3D; 25</span><br><span class="line">delay &#x3D; 0.5 </span><br><span class="line">internet &#x3D; false</span><br><span class="line">benchmark &#x3D; false</span><br><span class="line">accept-encoding &#x3D; gzip;deflate</span><br><span class="line">url-escaping &#x3D; true</span><br><span class="line">unique &#x3D; true</span><br><span class="line"></span><br><span class="line">注意：siege默认只支持255个并发数，可以自己自定义，修改limit数值。</span><br></pre></td></tr></table></figure>

<h1 id="命令参数"><a href="#命令参数" class="headerlink" title="命令参数"></a>命令参数</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-V, --version             打印版本号</span><br><span class="line">-h, --help                打印帮助信息</span><br><span class="line">-C, --config              打印当前配置信息</span><br><span class="line">-g, --get                 拉取http头信息</span><br><span class="line">-p, --print               打印整个页面的内容</span><br><span class="line">-c, --concurrent&#x3D;NUM      并发用户数量，默认10个</span><br><span class="line">-r, --reps&#x3D;NUM            运行次数</span><br><span class="line">-d, --delay&#x3D;NUM           随机时间延迟(秒）</span><br><span class="line">-b, --benchmark           请求没有延迟</span><br><span class="line">-i, --internet            模拟网络用户随机点击URL</span><br><span class="line">-f, --file&#x3D;FILE           选择指定的URL文件</span><br><span class="line">-R, --rc&#x3D;FILE             指定siegerc文件</span><br><span class="line">-l, --log[&#x3D;FILE]          日志文件，默认是 PREFIX&#x2F;var&#x2F;siege.log</span><br><span class="line">-H, --header&#x3D;&quot;text&quot;       给请求添加头，支持多个</span><br><span class="line">-A, --user-agent&#x3D;&quot;text&quot;   给请求设置User-Agent</span><br><span class="line">-T, --content-type&#x3D;&quot;text&quot; 给请求设置Content-Type</span><br></pre></td></tr></table></figure>
<h1 id="性能参数"><a href="#性能参数" class="headerlink" title="性能参数"></a>性能参数</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Transactions				命中次数</span><br><span class="line">Availability				命中率</span><br><span class="line">Elapsed time				整个压测花费的时间，从第一个开始到最后一个结束</span><br><span class="line">Data transferred			整个压测数据传输的总和</span><br><span class="line">Response time				响应时间是响应每个模拟用户请求所花费的平均时间</span><br><span class="line">Transaction rate			事务速率是服务器每秒能够处理的平均事务数. 简而言之：事务除以经过的时间。</span><br><span class="line">Throughput				吞吐量是从服务器到所有模拟用户每秒传输的平均字节数</span><br><span class="line">Concurrency				并发是同时连接的平均数，这是一个随服务器性能下降而上升的数字。</span><br><span class="line">Successful transactions			成功事务次数</span><br><span class="line">Failed transactions			失败事务次数</span><br><span class="line">Longest transaction			最长事务时间</span><br><span class="line">Shortest transaction			最短事务时间</span><br></pre></td></tr></table></figure>

<h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">siege example.com</span><br><span class="line"></span><br><span class="line">siege -f &#x2F;etc&#x2F;siege&#x2F;urls.txt</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>Siege</tag>
      </tags>
  </entry>
  <entry>
    <title>负载均衡之LVS-基本概念和三种模式</title>
    <url>/posts/30eb423b.html</url>
    <content><![CDATA[<h1 id="LVS简介"><a href="#LVS简介" class="headerlink" title="LVS简介"></a>LVS简介</h1><p>LVS中文官方手册：<a href="http://www.linuxvirtualserver.org/zh/index.html。这个手册对于了解lvs的背景知识很有帮助。">http://www.linuxvirtualserver.org/zh/index.html。这个手册对于了解lvs的背景知识很有帮助。</a></p>
<p>LVS英文官方手册：<a href="http://www.linuxvirtualserver.org/Documents.html。这个手册比较全面，对于了解和学习lvs的原理、配置很有帮助。">http://www.linuxvirtualserver.org/Documents.html。这个手册比较全面，对于了解和学习lvs的原理、配置很有帮助。</a></p>
<p>LVS是章文嵩开发的一个国产开源负载均衡软件。LVS最初是他在大学期间的玩具，随着后来使用的用户越来越多，LVS也越来越完善，最终集成到了Linux的内核中。有不少开源牛人都为LVS开发过辅助工具和辅助组件，最出名的就是Alexandre为LVS编写的Keepalived，它最初专门用于监控LVS，后来加入了通过VRRP实现高可用的功能。</p>
<a id="more"></a>
<p>LVS的全称是Linux virtual server，即Linux虚拟服务器。之所以是虚拟服务器，是因为LVS自身是个负载均衡器(director)，不直接处理请求，而是将请求转发至位于它后端真正的服务器realserver上。</p>
<p>LVS是四层(传输层tcp/udp)、七层(应用层)的负载均衡工具，只不过大众一般都使用它的四层负载均衡功能ipvs，而七层的内容分发负载工具ktcpvs(kernel tcp virtual server)不怎么完善，使用的人并不多。</p>
<p>ipvs是集成在内核中的框架，可以通过用户空间的程序ipvsadm工具来管理，该工具可以定义一些规则来管理内核中的ipvs。就像iptables和netfilter的关系一样。</p>
<h1 id="LVS-ipvs三种模式的工作原理"><a href="#LVS-ipvs三种模式的工作原理" class="headerlink" title="LVS-ipvs三种模式的工作原理"></a>LVS-ipvs三种模式的工作原理</h1><p>首先要解释的是LVS相关的几种IP：</p>
<p>VIP:virtual IP，LVS服务器上接收外网数据包的网卡IP地址。<br>DIP:director IP，LVS服务器上转发数据包到realserver的网卡IP地址。<br>RIP:realserver(常简称为RS)上接收Director转发数据包的IP，即提供服务的服务器IP。<br>CIP:客户端的IP。</p>
<img src="/images/733013-20180211225301232-212054825.png" width="100%" height="100%">
LVS的三种工作模式：通过网络地址转换(NAT)将一组服务器构成一个高性能的、高可用的虚拟服务器，是VS/NAT技术。在分析VS/NAT的缺点和网络服务的非对称性的基础上，提出了通过IP隧道实现虚拟服务器的方法VS/TUN（Virtual Server via IP Tunneling），和通过直接路由实现虚拟服务器的方法VS/DR（Virtual Server via Direct Routing），它们可以极大地提高系统的伸缩性。

<h2 id="VS-NAT模式"><a href="#VS-NAT模式" class="headerlink" title="VS/NAT模式"></a>VS/NAT模式</h2><p>客户端发送的请求到达Director后，Director根据负载均衡算法改写目标地址为后端某个RIP(web服务器池中主机之一)并转发给该后端主机，就像NAT一样。当后端主机处理完请求后，后端主机将响应数据交给Director，并由director改写源地址为VIP后传输给客户端。大多数商品化的IP负载均衡硬件都是使用此方法，如Cisco的LocalDirector、F5的Big/IP。<br><img src="/images/733013-20180211234714295-412364937.png" width="100%" height="100%"><br><strong>这种模式下：</strong></p>
<ol>
<li><p>RIP和DIP一般处于同一私有网段中。但并非必须，只要它们能通信即可。</p>
</li>
<li><p>各RealServer的网关指向DIP，这样能保证将响应数据交给Director。</p>
</li>
<li><p>VS/NAT模式的最大缺点是Director负责所有进出数据：不仅处理客户端发起的请求，还负责将响应传输给客户端。而响应数据一般比请求数据大得多，调度器Director容易出现瓶颈。(也就是像7层负载的处理方式一样，但却没有7层负载那么”多功能”)</p>
</li>
<li><p>这种模式配置起来最简单。</p>
</li>
</ol>
<h2 id="VS-TUN模式"><a href="#VS-TUN模式" class="headerlink" title="VS/TUN模式"></a>VS/TUN模式</h2><p>采用NAT技术时，由于请求和响应报文都必须经过调度器地址重写，当客户请求越来越多时，调度器的处理能力将成为瓶颈。为了解决这个问题，调度器把请求报文通过IP隧道转发至真实服务器，而真实服务器将响应直接返回给客户，所以调度器只处理请求报文。由于一般网络服务响应报文比请求报文大许多，采用VS/TUN技术后，调度器得到极大的解放，集群系统的最大吞吐量可以提高10倍。<br><img src="/images/733013-20180213102345546-471661305.png" width="100%" height="100%"><br><strong>VS/TUN模式的工作原理：</strong></p>
<ul>
<li><p>(1)IP隧道技术又称为IP封装技术，它可以将带有源和目标IP地址的数据报文使用新的源和目标IP进行第二次封装，这样这个报文就可以发送到一个指定的目标主机上；</p>
</li>
<li><p>(2)VS/TUN模式下，调度器和后端服务器组之间使用IP隧道技术。当客户端发送的请求(CIP–&gt;VIP)被director接收后，director修改该报文，加上IP隧道两端的IP地址作为新的源和目标地址，并将请求转发给后端被选中的一个目标；</p>
</li>
<li><p>(3)当后端服务器接收到报文后，首先解封报文得到原有的CIP–&gt;VIP，该后端服务器发现自身的tun接口上配置了VIP，因此接受该数据包。</p>
</li>
<li><p>(4)当请求处理完成后，结果将不会重新交给director，而是直接返回给客户端。此时响应数据包的源IP为VIP，目标IP为CIP。</p>
</li>
</ul>
<p><strong>采用VS/TUN模式时的基本属性和要求：</strong></p>
<ol>
<li><p>RealServer的RIP和director的DIP不用处于同一物理网络中，且RIP必须可以和公网通信。也就是说集群节点可以跨互联网实现。</p>
</li>
<li><p>realserver的tun接口上需要配置VIP地址，以便接收director转发过来的数据包，以及作为响应报文的源IP。</p>
</li>
<li><p>director给realserver时需要借助隧道，隧道外层的IP头部的源IP是DIP，目标IP是RIP。而realsever响应给客户端的IP头部是根据隧道内层的IP头分析得到的，源IP是VIP，目标IP是CIP。这样客户端就无法区分这个VIP到底是director的还是服务器组中的。</p>
</li>
<li><p>director只处理入站请求，响应请求由realserver完成。</p>
</li>
<li><p>一般来说，VS/TUN模式会用来负载调度缓存服务器组，这些缓存服务器一般放置在不同网络环境，可以就近返回数据给客户端。在请求对象不能在Cache服务器本地命中的情况下，Cache服务器要向源服务器发请求，将结果取回，最后将结果返回给客户。</p>
</li>
</ol>
<h2 id="VS-DR模式"><a href="#VS-DR模式" class="headerlink" title="VS/DR模式"></a>VS/DR模式</h2><p>VS/TUN模式下，调度器对数据包的处理是使用IP隧道技术进行二次封装。VS/DR模式和VS/TUN模式很类似，只不过调度器对数据包的处理是改写数据帧的目标MAC地址，通过链路层来负载均衡。</p>
<p>VS/DR通过改写请求报文的目标MAC地址，将请求发送到真实服务器，而真实服务器将响应直接返回给客户。同VS/TUN技术一样，VS/DR技术可极大地提高集群系统的伸缩性。这种方法没有IP隧道的开销，对集群中的真实服务器也没有必须支持IP隧道协议的要求，但是要求调度器与真实服务器都有一块网卡连在同一物理网段上，以便使用MAC地址通信转发数据包。<br><img src="/images/733013-20180213102610124-276805076.png" width="100%" height="100%"><br><strong>VS/DR模式的工作原理：</strong></p>
<ul>
<li><p>(1)客户端发送的请求被director接收后，director根据负载均衡算法，改写数据帧的目标MAC地址为后端某RS的MAC地址，并将该数据包转发给该RS(实际上是往整个局域网发送，但只有该MAC地址的RS才不会丢弃)。</p>
</li>
<li><p>(2)RS接收到数据包后，发现数据包的目标IP地址为VIP，而RS本身已经将VIP配置在了某个接口上，因此RS会接收下这个数据包并进行处理。</p>
</li>
<li><p>(3)处理完毕后，RS直接将响应报文响应给客户端。此时数据包源IP为VIP，目标IP为CIP。<br>也就是说，客户端请求发送到LB上，源和目标IP为CIP:VIP，LB上有VIP和DIP，重新改写MAC地址后通过DIP发送给某个realserver，如RS1，此时源和目标IP还是CIP:VIP，但是目标MAC地址改写为RIP1所在网卡的MAC地址”RS1_MAC”，RS1发现自身有VIP地址，所以收下此数据报文(所以在RS上必须配置VIP)。返回时，RS1根据路由表直接返回给客户端，此时，源和目标IP是VIP–&gt;CIP。但由于流出接口为RIP所在网卡接口，因此源MAC地址为RIP所在接口的MAC地址。这一细节在考虑CIP、RIP不同网段时的配置时很重要。</p>
</li>
</ul>
<p><strong>采用VS/DR模式时的基本属性和要求：</strong></p>
<ul>
<li><p>RealServer的RIP和director的DIP必须处于同一网段中，以便使用MAC地址进行通信。</p>
</li>
<li><p>realserver上必须配置VIP地址，以便接收director转发过来的数据包，以及作为响应报文的源IP。</p>
</li>
<li><p>realsever响应给客户端的数据包的源和目标IP为VIP–&gt;CIP。</p>
</li>
<li><p>director只处理入站请求，响应请求由realserver完成。</p>
</li>
</ul>
<h2 id="lvs-ipvs的三种模式比较"><a href="#lvs-ipvs的三种模式比较" class="headerlink" title="lvs-ipvs的三种模式比较"></a>lvs-ipvs的三种模式比较</h2><img src="/images/733013-20180213110217531-1638772778.png" width="100%" height="100%">
在性能上，VS/DR和VS/TUN远高于VS/NAT，因为调度器只处于从客户到服务器的半连接中，按照半连接的TCP有限状态机进行状态迁移，极大程度上减轻了调度器的压力(真正建立TCP连接的是RS和Client)。VS/DR性能又稍高于VS/TUN，因为少了隧道的开销。但是，VS/DR和VS/TUN的主要区别是VS/TUN可以跨网络实现后端服务器负载均衡(也可以局域网内)，而VS/DR只能和director在局域网内进行负载均衡。

<h1 id="VS-TUN和VS-DR模式中的ARP问题"><a href="#VS-TUN和VS-DR模式中的ARP问题" class="headerlink" title="VS/TUN和VS/DR模式中的ARP问题"></a>VS/TUN和VS/DR模式中的ARP问题</h1><p>当一个目标IP地址为VIP的数据包进入Director前端的路由器时，路由器会向局域网内发送ARP广播，以找出VIP地址的MAC地址在哪台主机上。<br><img src="/images/733013-20180213170727562-1125645743.png" width="60%" height="60%"></p>
<p>Director和各RS都配置了VIP。当路由器发送ARP广播后，Director和RS都会收到这个广播包，且都认为这个广播包找的就是自己，于是都回应给路由器，这样路由器上的ARP缓存表中的条目VIP&lt;–&gt;vip_MAC就不断被覆盖直到最后一个回应。这样一来，路由器将把客户端的数据包发送给最后一个回应的主机，这台主机的VIP可能是Director上的，也可能是某个RS上的。在一定时间内，路由器收到目标IP为VIP的数据包都会发送给该主机。但路由器会定时发送ARP广播包，这样一来ARP缓存表中的VIP对应的MAC地址可能会换成另一台主机。</p>
<p>因此，必须要保证路由器只保存Director上VIP对应的MAC地址，即只允许Director才对路由器的ARP广播进行回应。也就是说，所有RS上的VIP必须隐藏起来。</p>
<p>一般通过将Real Server上的VIP设置在lo接口的别名接口上(如lo:0)，并设置arp_ignore=1和arp_announce=2的方式来隐藏RS上的VIP。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo 1 &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;all&#x2F;arp_ignore</span><br><span class="line">echo 2 &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;all&#x2F;arp_announce</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sysctl -w net.ipv4.conf.all.arp_ignore&#x3D;1</span><br><span class="line">sysctl -w net.ipv4.conf.all.arp_announce&#x3D;2</span><br></pre></td></tr></table></figure>
<p>或者将arp参数设置到内核参数配置文件中以让其永久生效。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;net.ipv4.conf.all.arp_ignore&#x3D;1&quot; &gt;&gt;&#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">echo &quot;net.ipv4.conf.all.arp_announce&#x3D;2&quot; &gt;&gt;&#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure>
<p>在网上几乎所有文章还设置了lo接口上的arp参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo 1 &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;lo&#x2F;arp_ignore</span><br><span class="line">echo 2 &gt;&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;conf&#x2F;lo&#x2F;arp_announce</span><br></pre></td></tr></table></figure>
<p>但这没有任何意义，因为从lo接口不受arp参数的影响。</p>
<p>应该在配置VIP之前就设置arp参数，以防止配置VIP后、设置arp抑制之前被外界主机发现。</p>
<h1 id="LVS负载均衡的调度算法"><a href="#LVS负载均衡的调度算法" class="headerlink" title="LVS负载均衡的调度算法"></a>LVS负载均衡的调度算法</h1><p>LVS的调度算法，详细内容见官方手册：<a href="http://www.linuxvirtualserver.org/zh/lvs4.html">http://www.linuxvirtualserver.org/zh/lvs4.html</a> 。</p>
<p>IPVS在内核中的负载均衡调度是以连接为粒度的。在HTTP协议（非持久）中，每次从WEB服务器上获取资源都需要建立一个TCP连接，同一用户的不同请求会被调度到不同的服务器上，所以这种细粒度的调度在一定程度上可以避免单个用户访问的突发性引起服务器间的负载不平衡。</p>
<p>LVS分为两种调度方式：静态调度和动态反馈调度。</p>
<p>静态调度方式是指不管RS的繁忙程度，根据调度算法计算后轮到谁就调度谁。例如两台realserver，一开始双方都在处理500个连接，下一个请求到来前，server1只断开了10个，而server2断开了490个，但是此时轮到了server1，就会调度server1而不管繁忙的程度。</p>
<p>动态调度方式是指根据RS的繁忙程度反馈，计算出下一个连接应该调度谁(动态反馈负载均衡算法考虑服务器的实时负载和响应情况，不断调整服务器间处理请求的比例，来避免有些服务器超载时依然收到大量请求，从而提高整个系统的吞吐率)。</p>
<p>在内核中的连接调度算法上，IPVS已实现了以下八种调度算法：默认的算法为wlc。</p>
<p><strong>静态调度：</strong></p>
<ul>
<li><p>轮叫调度（Round-Robin Scheduling,rr）</p>
</li>
<li><p>加权轮叫调度（Weighted Round-Robin Scheduling,wrr），按照权重比例作为轮询标准</p>
</li>
<li><p>目标地址散列调度（Destination Hashing Scheduling,dh），目标地址哈希，对于同一目标IP的请求总是发往同一服务器</p>
</li>
<li><p>源地址散列调度（Source Hashing Scheduling,sh），源地址哈希，在一定时间内，只要是来自同一个客户端的请求，就发送至同一个realserver</p>
</li>
</ul>
<p><strong>动态反馈调度：</strong></p>
<ul>
<li><p>最小连接调度（Least-Connection Scheduling,lc），调度器需要记录各个服务器已建立连接的数目，当一个请求被调度到某服务器，其连接数加1；当连接中止或超时，其连接数减1。当各个服务器的处理能力不同时，该算法不理想。</p>
</li>
<li><p>加权最小连接调度（Weighted Least-Connection Scheduling,wlc）</p>
</li>
<li><p>基于本地的最少连接（Locality-Based Least Connections Scheduling,lblc），目前该算法主要用于cache集群系统。</p>
</li>
<li><p>带复制的基于局部性最少连接（Locality-Based Least Connections with Replication Scheduling,lblcr），目前主要用于Cache集群系统。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>LVS</category>
      </categories>
      <tags>
        <tag>LVS</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS 7上使用Tripwire监控和检测修改的文件</title>
    <url>/posts/219600b9.html</url>
    <content><![CDATA[<p>Tripwire是一个免费的开源入侵检测系统（IDS）。 它是用于监视和警告系统上文件更改的安全工具。 Tripwire是一个功能强大的IDS，可以保护您的系统免受不必要的更改。 您可以使用它来监控您的系统文件，包括网站文件，因此当有不需要的文件更改时，Tripwire会检查您的系统，如果设置正确，可以通过电子邮件提醒您。</p>
<a id="more"></a>

<h1 id="在CentOS-7上安装Tripwire"><a href="#在CentOS-7上安装Tripwire" class="headerlink" title="在CentOS 7上安装Tripwire"></a>在CentOS 7上安装Tripwire</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum update</span><br><span class="line">yum install epel-release</span><br><span class="line">yum install tripwire</span><br></pre></td></tr></table></figure>

<p>安装完成后，我们需要生成新的密钥文件。</p>
<p>Tripwire使用2个关键文件。</p>
<ol>
<li>site-key：它用于保护Tripwire配置。 因此，除非我们再次生成配置，否则对tripwire配置所做的任何更改都不会生效，我们会提示您输入“site-key”密码。</li>
<li>local-key：它用于验证tripwire二进制文件。 当我们想要更新tripwire系统数据库时，我们需要运行tripwire命令，并且会提示我们输入’local-key’的密码。</li>
</ol>
<p>我们使用下面的命令生成新的tripwire密钥文件（站点和本地密钥）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tripwire-setup-keyfiles</span><br></pre></td></tr></table></figure>

<p>该命令将生成两个密钥文件“site-key”和“local-key”，并且您将被要求输入每个密码。</p>
<p>输入您自己的“ 网站密钥 ”密码，然后按Enter键。<br><img src="/images/tripwire-site.png" width="100%" height="100%"></p>
<p>输入您自己的“ 本地密钥 ”密码并再次按Enter键。<br><img src="/images/tripwire-local.png" width="100%" height="100%"></p>
<p>接下来，使用’site-key’签署tripwire配置,输入您的“ 网站密钥 ”密码。<br><img src="/images/tripwire-site-key.png" width="100%" height="100%"><br>现在，为了签署Tripwire政策，请输入您的“ 本地密钥 ”密码。<br><img src="/images/tripwire-local-key.png" width="100%" height="100%"></p>
<h1 id="配置Tripwire策略"><a href="#配置Tripwire策略" class="headerlink" title="配置Tripwire策略"></a>配置Tripwire策略</h1><p>使用下面的tripwire命令初始化tripwire数据库。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tripwire --init</span><br></pre></td></tr></table></figure>

<p>您可能会收到错误消息“no such directory”，如下所示</p>
<img src="/images/tripwire-error.png" width="100%" height="100%">
是因为系统没有在tripwire配置中已经定义的目录和文件。 为了解决这个错误，我们需要编辑tripwire配置'twpol.txt'并重新签署tripwire配置。

<p>现在使用下面的命令从tripwire生成日志错误。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sh -c &quot;tripwire --check | grep Filename &gt; no-directory.txt&quot;</span><br></pre></td></tr></table></figure>

<p>所有不存在于CentOS 7系统上的目录和文件都列在文件no-directory.txt中<br>使用以下bash脚本编辑tripwire配置’twpol.txt’ - 在终端上运行此脚本。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for f in $(grep &quot;Filename:&quot; no-directory.txt | cut -f2 -d:); do</span><br><span class="line">sed -i &quot;s|\($f\) |#\\1|g&quot; &#x2F;etc&#x2F;tripwire&#x2F;twpol.txt</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>毕竟，我们需要使用twadmin命令重新生成并重新签署tripwire配置，如下所示。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">twadmin -m P &#x2F;etc&#x2F;tripwire&#x2F;twpol.txt</span><br></pre></td></tr></table></figure>

<p>输入您的“网站密钥”密码。重新初始化tripwire数据库，并确保没有错误。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tripwire --init</span><br></pre></td></tr></table></figure>

<h1 id="验证Tripwire配置和检查系统"><a href="#验证Tripwire配置和检查系统" class="headerlink" title="验证Tripwire配置和检查系统"></a>验证Tripwire配置和检查系统</h1><p>要验证tripwire配置，我们可以运行系统检查命令如下。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tripwire --check</span><br></pre></td></tr></table></figure>
<h1 id="将新规则添加到Tripwire策略"><a href="#将新规则添加到Tripwire策略" class="headerlink" title="将新规则添加到Tripwire策略"></a>将新规则添加到Tripwire策略</h1><p>在这一步中，我们将向您展示如何将新规则添加到tripwire策略配置“twpol.txt”。</p>
<p>要执行这项工作，我们需要定义规则名称，严重程度，监视目录和文件类型。 在这一步中，我们将在/var/www/目录下为我们的WordPress安装创建一个名为Wordpress Data的新规则，严重程度为HIGH/SIG_HI,并且该目录中的所有文件都是关键的以及源代码不能更改）。</p>
<p>转到tripwire配置目录/etc/tripwire并使用vim编辑配置文件twpol.txt。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;etc&#x2F;tripwire&#x2F;</span><br><span class="line">vim twpol.txt</span><br></pre></td></tr></table></figure>
<p>转到该行的末尾，并在那里粘贴以下WordPress规则。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Ruleset for Wordpress</span><br><span class="line"> (</span><br><span class="line">   rulename &#x3D; &quot;Wordpress Data&quot;,</span><br><span class="line">   severity&#x3D; $(SIG_HI)</span><br><span class="line"> )</span><br><span class="line"> &#123;</span><br><span class="line">         &#x2F;var&#x2F;www        -&gt; $(SEC_CRIT);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>


<p>保存并退出。</p>
<p>使用twadmin命令重新生成并重新签名配置，如下所示。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">twadmin -m P &#x2F;etc&#x2F;tripwire&#x2F;twpol.txt</span><br></pre></td></tr></table></figure>

<p>输入您的“网站密钥”密码。现在我们需要再次重新生成tripwire数据库。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tripwire --init</span><br></pre></td></tr></table></figure>
<p>输入“本地密钥”密码。新的规则集已添加并应用于Tripwire策略配置。</p>
<h1 id="安装Tripwire电子邮件通知和Cron"><a href="#安装Tripwire电子邮件通知和Cron" class="headerlink" title="安装Tripwire电子邮件通知和Cron"></a>安装Tripwire电子邮件通知和Cron</h1><p>在这一步中，我们将为特定tripwire规则集策略配置通知，并配置用于自动系统检查的cronjob。 我们会将任何违反WordPress数据规则的报告发送到电子邮件地址<a href="mailto:myemail@gmail.com">myemail@gmail.com</a>。</p>
<p>对于电子邮件通知，tripwire在配置中提供了一个emailto功能。 默认情况下，tripwire使用Postfix或Sendmail通过电子邮件发送报告。</p>
<p>在配置电子邮件通知之前，请使用以下命令测试tripwire通知功能。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tripwire --test --email email@gmail.com</span><br></pre></td></tr></table></figure>
<p>现在进入/etc/tripwire目录并编辑twpol.txt配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;etc&#x2F;tripwire&#x2F;</span><br><span class="line">vim twpol.txt</span><br></pre></td></tr></table></figure>
<p>在WordPress数据规则中添加新行emailto，如下所示。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Ruleset for Wordpress</span><br><span class="line"> (</span><br><span class="line">   rulename &#x3D; &quot;Wordpress Data&quot;,</span><br><span class="line">   severity&#x3D; $(SIG_HI),</span><br><span class="line">   emailto &#x3D; myemail@gmail.com</span><br><span class="line"> )</span><br><span class="line"> &#123;</span><br><span class="line">         &#x2F;var&#x2F;www        -&gt; $(SEC_CRIT);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>保存并退出。使用twadmin命令重新生成并签署配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">twadmin -m P &#x2F;etc&#x2F;tripwire&#x2F;twpol.txt</span><br></pre></td></tr></table></figure>
<p>输入您的“网站密钥”密码。并重新生成tripwire数据库。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tripwire --init</span><br></pre></td></tr></table></figure>
<p>输入您的tripwire’local-key’密码。Tripwire电子邮件通知的配置已完成。<br><strong>注意：</strong></p>
<ul>
<li>–email-report：将系统报告发送到每个规则中定义的电子邮件地址。</li>
</ul>
<p>接下来，我们将使用cron setup启用自动Tripwire系统检查。 为此，请使用下面的crontab命令在root用户下创建一个新的cron脚本。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">crontab -e -u root</span><br></pre></td></tr></table></figure>

<p>粘贴以下cron配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0 0 * * * tripwire --check --email-report</span><br></pre></td></tr></table></figure>
<p>保存并退出。</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Tripwire</tag>
      </tags>
  </entry>
  <entry>
    <title>通过maven本地安装第三方jar包</title>
    <url>/posts/1fb3bc1d.html</url>
    <content><![CDATA[<p>  由于项目需要，拉取不到仓库的私有jar包，需要本地导入jar包</p>
<a id="more"></a>
<h1 id="pom文件"><a href="#pom文件" class="headerlink" title="pom文件"></a>pom文件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;groupId&gt;com.lolaage&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;JavaLogic&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br></pre></td></tr></table></figure>

<h1 id="jar包"><a href="#jar包" class="headerlink" title="jar包"></a>jar包</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JavaLogic-1.0-SNAPSHOT.jar  修改为JavaLogic.jar，不修改则导入命令修改-Dfile</span><br></pre></td></tr></table></figure>

<h1 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn install:install-file -Dfile&#x3D;JavaLogic.jar -DgroupId&#x3D;com.lolaage -DartifactId&#x3D;JavaLogic -Dversion&#x3D;1.0-SNAPSHOT -Dpackaging&#x3D;jar</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title>离线部署yum源</title>
    <url>/posts/541ec4d9.html</url>
    <content><![CDATA[<h3 id="下载相关镜像文件"><a href="#下载相关镜像文件" class="headerlink" title="下载相关镜像文件"></a>下载相关镜像文件</h3><ul>
<li>Centos7.x.iso</li>
</ul>
<a id="more"></a>

<h3 id="挂载到每台机器上"><a href="#挂载到每台机器上" class="headerlink" title="挂载到每台机器上"></a>挂载到每台机器上</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;mnt&#x2F;centos7 &#x2F;mnt&#x2F;centos7-epel</span><br><span class="line">mount -t iso9660 xxxx.iso &#x2F;mnt&#x2F;centos7</span><br><span class="line">mount -t iso9660 xxx.iso &#x2F;mnt&#x2F;centos7-epel</span><br></pre></td></tr></table></figure>

<h3 id="配置离线-repo"><a href="#配置离线-repo" class="headerlink" title="配置离线 repo"></a>配置离线 repo</h3><ul>
<li><p>/etc/yum.repos.d/offline-centos7.repo</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[offline-centos7]</span><br><span class="line">name=CentOS-<span class="variable">$releasever</span> - blueking</span><br><span class="line">baseurl=file:///mnt/centos7</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</span><br></pre></td></tr></table></figure>
</li>
<li><p>/etc/yum.repos.d/offline-centos7-epel.repo</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[offline-centos7-epel]</span><br><span class="line">name=CentOS-<span class="variable">$releasever</span> - blueking</span><br><span class="line">baseurl=file:///mnt/centos7-epel</span><br><span class="line">enabled=1</span><br><span class="line">exclude=epel-release</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7</span><br></pre></td></tr></table></figure>



</li>
</ul>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title>Memcached基于repcached的主从复制</title>
    <url>/posts/93879e18.html</url>
    <content><![CDATA[<p>  由于 Memcached 自己没有防止单点的措施，因为为了保障 Memcached 服务的高可用，我们需要借助外部的工具来实现高可用的功能。本文引入 Repcached 这个工具，通过使用该工具我们可以完成 Memcached 服务的主从功能。</p>
<a id="more"></a>
<p>  Repcached 它是由日本人开发的，用来实现 Memcached 复制功能的一个工具。它所构建的主从方案是一个单主单从的方案，不支持多主多从。但是，它的特点是，主从两个节点可以互相读写，从而可以达到互相同步的效果。</p>
<p>假设主节点坏掉，从节点会很快侦测到连接断开，然后它会自动切换到监听状态( listen)从而成为主节点，并且等待新的从节点加入。</p>
<p>假设原来挂掉的主节点恢复之后，我们只能人工手动以从节点的方式去启动。原来的主节点并不能抢占成为新的主节点，除非新的主节点挂掉。这也就意味着，基于 Repcached 实现的 Memcached 主从，针对主节点并不具备抢占功能。</p>
<p>假设从节点坏掉，主节点也会很快侦测到连接断开，然后它就会重新切换到监听状态(listen),并且等待新的从节点加入。</p>
<p>假设主从节点都挂掉，则数据就丢失了！因此，这是 Repcached 的一个短板，不过后期我们可以通过结合其它的工具来弥补这个缺点。</p>
<h1 id="基础环境准备"><a href="#基础环境准备" class="headerlink" title="基础环境准备"></a>基础环境准备</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">172.16.0.8 node1</span><br><span class="line">172.16.0.9 node2</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install gcc gcc-c++</span><br><span class="line">yum install libevent libevent-devel</span><br></pre></td></tr></table></figure>

<h1 id="repcached安装"><a href="#repcached安装" class="headerlink" title="repcached安装"></a>repcached安装</h1><p>这种方式集成了memcached，由于版本比较老，memcached为1.2.8，暂时未找到memcached1.5.X匹配的补丁包repcached</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;sourceforge.net&#x2F;projects&#x2F;repcached&#x2F;files&#x2F;repcached&#x2F;2.2.1-1.2.8&#x2F;memcached-1.2.8-repcached-2.2.1.tar.gz</span><br><span class="line">tar zxf memcached-1.2.8-repcached-2.2.1.tar.gz</span><br><span class="line">cd memcached-1.2.8-repcached-2.2.1</span><br><span class="line">.&#x2F;configure --enable-replication</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">备注：</span><br><span class="line">如make安装错误，提示</span><br><span class="line">memcached.c: In function &#39;add_iov&#39;:</span><br><span class="line">memcached.c:697: error: &#39;IOV_MAX&#39; undeclared (first use in this function)</span><br><span class="line">memcached.c:697: error: (Each undeclared identifier is reported only once</span><br><span class="line">memcached.c:697: error: for each function it appears in.)</span><br><span class="line"></span><br><span class="line">则修改memcached.c，大概60行</span><br><span class="line">&#x2F;* FreeBSD 4.x doesn&#39;t have IOV_MAX exposed. *&#x2F;</span><br><span class="line">#ifndef IOV_MAX</span><br><span class="line">#if defined(__FreeBSD__) || defined(__APPLE__) </span><br><span class="line"># define IOV_MAX 1024</span><br><span class="line">#endif </span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">删掉</span><br><span class="line">#if defined(__FreeBSD__) || defined(__APPLE__)</span><br><span class="line">#endif</span><br><span class="line">即可</span><br></pre></td></tr></table></figure>

<h1 id="启动配置"><a href="#启动配置" class="headerlink" title="启动配置"></a>启动配置</h1><h2 id="启动master"><a href="#启动master" class="headerlink" title="启动master"></a>启动master</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# memcached -v -d -p 11211 -l 172.16.0.8 -u root -P &#x2F;tmp&#x2F;memcached1.pid </span><br><span class="line">[root@node1 ~]# replication: listen </span><br><span class="line">[root@node1 ~]# replication: accept   # node2 启动正常后，node1 将 accept</span><br></pre></td></tr></table></figure>
<h2 id="启动salve"><a href="#启动salve" class="headerlink" title="启动salve"></a>启动salve</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]# memcached -v -d -p 11211 -l 172.16.0.9 -u root -x 172.16.0.8 -P &#x2F;tmp&#x2F;memcached1.pid </span><br><span class="line">[root@node2 ~]# replication: connect (peer&#x3D;172.16.0.8:11212) </span><br><span class="line">replication: marugoto copying </span><br><span class="line">replication: start</span><br></pre></td></tr></table></figure>

<h1 id="repcache的参数说明"><a href="#repcache的参数说明" class="headerlink" title="repcache的参数说明"></a>repcache的参数说明</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@public01 ~]# memcached -help</span><br><span class="line">memcached 1.2.8</span><br><span class="line">repcached 2.2.1</span><br><span class="line">-p &lt;num&gt;      TCP port number to listen on (default: 11211)</span><br><span class="line">-U &lt;num&gt;      UDP port number to listen on (default: 11211, 0 is off)</span><br><span class="line">-s &lt;file&gt;     unix socket path to listen on (disables network support)</span><br><span class="line">-a &lt;mask&gt;     access mask for unix socket, in octal (default 0700)</span><br><span class="line">-l &lt;ip_addr&gt;  interface to listen on, default is INDRR_ANY</span><br><span class="line">-d            run as a daemon</span><br><span class="line">-r            maximize core file limit</span><br><span class="line">-u &lt;username&gt; assume identity of &lt;username&gt; (only when run as root)</span><br><span class="line">-m &lt;num&gt;      max memory to use for items in megabytes, default is 64 MB</span><br><span class="line">-M            return error on memory exhausted (rather than removing items)</span><br><span class="line">-c &lt;num&gt;      max simultaneous connections, default is 1024</span><br><span class="line">-k            lock down all paged memory.  Note that there is a</span><br><span class="line">              limit on how much memory you may lock.  Trying to</span><br><span class="line">              allocate more than that would fail, so be sure you</span><br><span class="line">              set the limit correctly for the user you started</span><br><span class="line">              the daemon with (not for -u &lt;username&gt; user;</span><br><span class="line">              under sh this is done with &#39;ulimit -S -l NUM_KB&#39;).</span><br><span class="line">-v            verbose (print errors&#x2F;warnings while in event loop)</span><br><span class="line">-vv           very verbose (also print client commands&#x2F;reponses)</span><br><span class="line">-h            print this help and exit</span><br><span class="line">-i            print memcached and libevent license</span><br><span class="line">-P &lt;file&gt;     save PID in &lt;file&gt;, only used with -d option</span><br><span class="line">-f &lt;factor&gt;   chunk size growth factor, default 1.25</span><br><span class="line">-n &lt;bytes&gt;    minimum space allocated for key+value+flags, default 48</span><br><span class="line">-R            Maximum number of requests per event</span><br><span class="line">              limits the number of requests process for a given con nection</span><br><span class="line">              to prevent starvation.  default 20</span><br><span class="line">-b            Set the backlog queue limit (default 1024)</span><br><span class="line">-x &lt;ip_addr&gt;  hostname or IP address of peer repcached</span><br><span class="line">-X &lt;num:num&gt;  TCP port number for replication. &lt;listen:connect&gt; (default: 11212)</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>memcached</tag>
      </tags>
  </entry>
  <entry>
    <title>记一次mongodb被删库</title>
    <url>/posts/4d3400f.html</url>
    <content><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>早上产品说XX平台不能打开，就自行打开平台，F12查看了接口提示500，说明服务异常，登陆服务器看了负载，磁盘空间无异常，查看应用日志提示mongodb无法连接导致。<a id="more"></a>于是登陆mongodb，出现了一个新的数据库，原来的数据库丢失，查看数据库的内容原来是被清空数据勒索了，如图所示<br><img src="/images/20200415094927.png" width="100%" height="100%"></p>
<p>查看mongodb日志，可以看到大量了外网地址连接</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2020-04-14T04:56:09.550+0800 I NETWORK  [conn1511714] end connection 39.107.14.208:40666 (43 connections now open)</span><br><span class="line">2020-04-14T05:20:30.311+0800 I NETWORK  [initandlisten] connection accepted from 104.152.52.34:51853 #1512195 (44 connections now open)</span><br><span class="line">2020-04-14T05:21:01.296+0800 I NETWORK  [conn1512195] end connection 104.152.52.34:51853 (43 connections now open)</span><br><span class="line">2020-04-14T07:15:53.163+0800 I NETWORK  [initandlisten] connection accepted from 45.227.255.233:61000 #1514516 (44 connections now open)</span><br><span class="line">2020-04-14T07:16:24.155+0800 I NETWORK  [conn1514516] end connection 45.227.255.233:61000 (43 connections now open)</span><br><span class="line">2020-04-14T11:48:18.433+0800 I NETWORK  [initandlisten] connection accepted from 3.80.64.27:60438 #1519957 (44 connections now open)</span><br><span class="line">2020-04-14T11:48:18.660+0800 I NETWORK  [conn1519957] end connection 3.80.64.27:60438 (43 connections now open)</span><br><span class="line">2020-04-14T12:40:03.620+0800 I NETWORK  [initandlisten] connection accepted from 106.15.76.92:35372 #1520998 (44 connections now open)</span><br><span class="line">2020-04-14T12:40:03.692+0800 I NETWORK  [conn1520998] end connection 106.15.76.92:35372 (43 connections now open)</span><br><span class="line">2020-04-14T15:19:26.779+0800 I NETWORK  [initandlisten] connection accepted from 45.76.69.227:52032 #1524191 (44 connections now open)</span><br><span class="line">2020-04-14T15:19:27.293+0800 I NETWORK  [initandlisten] connection accepted from 45.76.69.227:52036 #1524192 (45 connections now open)</span><br><span class="line">2020-04-14T15:19:30.424+0800 I NETWORK  [conn1524191] end connection 45.76.69.227:52032 (44 connections now open)</span><br><span class="line">2020-04-14T15:19:30.424+0800 I NETWORK  [conn1524192] end connection 45.76.69.227:52036 (43 connections now open)</span><br><span class="line">2020-04-14T15:37:25.731+0800 I NETWORK  [initandlisten] connection accepted from 218.17.161.71:58517 #1524553 (44 connections now open)</span><br><span class="line">2020-04-14T15:53:33.064+0800 I NETWORK  [conn1488049] end connection 119.137.53.77:35317 (43 connections now open)</span><br><span class="line">2020-04-14T16:27:03.334+0800 I NETWORK  [initandlisten] connection accepted from 47.97.16.76:59748 #1525554 (44 connections now open)</span><br><span class="line">2020-04-14T16:27:03.458+0800 I NETWORK  [conn1525554] end connection 47.97.16.76:59748 (43 connections now open)</span><br><span class="line">2020-04-14T17:34:56.440+0800 I NETWORK  [initandlisten] connection accepted from 47.93.57.242:47316 #1526915 (44 connections now open)</span><br><span class="line">2020-04-14T17:34:56.440+0800 I NETWORK  [conn1526915] end connection 47.93.57.242:47316 (43 connections now open)</span><br><span class="line">2020-04-14T17:34:56.482+0800 I NETWORK  [initandlisten] connection accepted from 47.93.57.242:47322 #1526916 (44 connections now open)</span><br><span class="line">2020-04-14T17:34:56.560+0800 I NETWORK  [initandlisten] connection accepted from 47.93.57.242:47332 #1526917 (45 connections now open)</span><br><span class="line">2020-04-14T17:34:56.631+0800 I NETWORK  [conn1526917] end connection 47.93.57.242:47332 (44 connections now open)</span><br><span class="line">2020-04-14T17:34:56.633+0800 I NETWORK  [conn1526916] end connection 47.93.57.242:47322 (43 connections now open)</span><br><span class="line">2020-04-14T20:03:12.999+0800 I NETWORK  [initandlisten] connection accepted from 47.97.16.76:54368 #1529878 (44 connections now open)</span><br><span class="line">2020-04-14T20:03:13.135+0800 I NETWORK  [conn1529878] end connection 47.97.16.76:54368 (43 connections now open)</span><br><span class="line">2020-04-15T00:03:54.045+0800 I NETWORK  [initandlisten] connection accepted from 184.105.247.196:46242 #1534686 (44 connections now open)</span><br><span class="line">2020-04-15T00:03:54.776+0800 I NETWORK  [conn1534686] end connection 184.105.247.196:46242 (43 connections now open)</span><br><span class="line">2020-04-15T00:04:02.802+0800 I NETWORK  [initandlisten] connection accepted from 184.105.247.196:7576 #1534687 (44 connections now open)</span><br><span class="line">2020-04-15T00:04:03.003+0800 I NETWORK  [conn1534687] end connection 184.105.247.196:7576 (43 connections now open)</span><br><span class="line">2020-04-15T02:11:39.270+0800 I NETWORK  [initandlisten] connection accepted from 51.38.140.6:35767 #1537241 (44 connections now open)</span><br><span class="line">2020-04-15T02:11:39.723+0800 I NETWORK  [conn1537241] AssertionException handling request, closing client connection: 34348 cannot translate opcode 2013</span><br><span class="line">2020-04-15T04:20:26.768+0800 I NETWORK  [initandlisten] connection accepted from 60.190.226.178:37104 #1539809 (44 connections now open)</span><br><span class="line">2020-04-15T04:20:26.826+0800 I NETWORK  [conn1539809] end connection 60.190.226.178:37104 (43 connections now open)</span><br><span class="line">2020-04-15T06:18:45.449+0800 I NETWORK  [initandlisten] connection accepted from 45.227.255.190:60790 #1542185 (44 connections now open)</span><br><span class="line">2020-04-15T06:18:45.903+0800 I NETWORK  [initandlisten] connection accepted from 45.227.255.190:60822 #1542204 (45 connections now open)</span><br><span class="line">2020-04-15T06:18:47.381+0800 I NETWORK  [conn1542185] end connection 45.227.255.190:60790 (44 connections now open)</span><br><span class="line">2020-04-15T06:18:47.386+0800 I NETWORK  [conn1542204] end connection 45.227.255.190:60822 (43 connections now open)</span><br><span class="line">2020-04-15T09:40:11.357+0800 I NETWORK  [conn1524553] end connection 218.17.161.71:58517 (3 connections now open)</span><br><span class="line">2020-04-15T09:40:11.954+0800 I NETWORK  [initandlisten] waiting for connections on port 27017</span><br><span class="line">2020-04-15T09:52:25.378+0800 I NETWORK  [initandlisten] waiting for connections on port 27017</span><br></pre></td></tr></table></figure>

<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>由于这平台目前还处于调试阶段，还没正式使用，所以就没设置mongodb帐号密码，端口也没有作限制才导致被人批量扫描了。只能从本地测试坏境导入数据解决。</p>
]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql使用binlog2sql恢复数据</title>
    <url>/posts/f85e60a0.html</url>
    <content><![CDATA[<h1 id="binlog2sql"><a href="#binlog2sql" class="headerlink" title="binlog2sql"></a>binlog2sql</h1><p>从MySQL binlog解析出你要的SQL。根据不同选项，你可以得到原始SQL、回滚SQL、去除主键的INSERT SQL等。</p>
<a id="more"></a>

<h1 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h1><ul>
<li>数据快速回滚(闪回)</li>
<li>主从切换后新master丢数据的修复</li>
<li>从binlog生成标准SQL，带来的衍生功能</li>
</ul>
<ul>
<li>已测试环境<ul>
<li>Python 2.7, 3.4+</li>
<li>MySQL 5.6, 5.7</li>
</ul>
</li>
</ul>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">shell&gt; git clone https:&#x2F;&#x2F;github.com&#x2F;danfengcao&#x2F;binlog2sql.git &amp;&amp; cd binlog2sql</span><br><span class="line">shell&gt; pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>git与pip的安装问题请自行搜索解决。</p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h3 id="MySQL-server必须设置以下参数"><a href="#MySQL-server必须设置以下参数" class="headerlink" title="MySQL server必须设置以下参数:"></a>MySQL server必须设置以下参数:</h3><pre><code>[mysqld]
server_id = 1
log_bin = /var/log/mysql/mysql-bin.log
max_binlog_size = 1G
binlog_format = row
binlog_row_image = full</code></pre><h3 id="user需要的最小权限集合："><a href="#user需要的最小权限集合：" class="headerlink" title="user需要的最小权限集合："></a>user需要的最小权限集合：</h3><pre><code>select, super/replication client, replication slave

建议授权
GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO </code></pre><p><strong>权限说明</strong></p>
<ul>
<li>select：需要读取server端information_schema.COLUMNS表，获取表结构的元信息，拼接成可视化的sql语句</li>
<li>super/replication client：两个权限都可以，需要执行’SHOW MASTER STATUS’, 获取server端的binlog列表</li>
<li>replication slave：通过BINLOG_DUMP协议获取binlog内容的权限</li>
</ul>
<h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><p><strong>解析出标准SQL</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">shell&gt; python binlog2sql.py -h127.0.0.1 -P3306 -uadmin -p<span class="string">'admin'</span> -dtest -t test3 test4 --start-file=<span class="string">'mysql-bin.000002'</span></span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">INSERT INTO `<span class="built_in">test</span>`.`test3`(`addtime`, `data`, `id`) VALUES (<span class="string">'2016-12-10 13:03:38'</span>, <span class="string">'english'</span>, 4); <span class="comment">#start 570 end 736</span></span><br><span class="line">UPDATE `<span class="built_in">test</span>`.`test3` SET `addtime`=<span class="string">'2016-12-10 12:00:00'</span>, `data`=<span class="string">'中文'</span>, `id`=3 WHERE `addtime`=<span class="string">'2016-12-10 13:03:22'</span> AND `data`=<span class="string">'中文'</span> AND `id`=3 LIMIT 1; <span class="comment">#start 763 end 954</span></span><br><span class="line">DELETE FROM `<span class="built_in">test</span>`.`test3` WHERE `addtime`=<span class="string">'2016-12-10 13:03:38'</span> AND `data`=<span class="string">'english'</span> AND `id`=4 LIMIT 1; <span class="comment">#start 981 end 1147</span></span><br></pre></td></tr></table></figure>

<p><strong>解析出回滚SQL</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">shell&gt; python binlog2sql.py --flashback -h127.0.0.1 -P3306 -uadmin -p<span class="string">'admin'</span> -dtest -ttest3 --start-file=<span class="string">'mysql-bin.000002'</span> --start-position=763 --stop-position=1147</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">INSERT INTO `<span class="built_in">test</span>`.`test3`(`addtime`, `data`, `id`) VALUES (<span class="string">'2016-12-10 13:03:38'</span>, <span class="string">'english'</span>, 4); <span class="comment">#start 981 end 1147</span></span><br><span class="line">UPDATE `<span class="built_in">test</span>`.`test3` SET `addtime`=<span class="string">'2016-12-10 13:03:22'</span>, `data`=<span class="string">'中文'</span>, `id`=3 WHERE `addtime`=<span class="string">'2016-12-10 12:00:00'</span> AND `data`=<span class="string">'中文'</span> AND `id`=3 LIMIT 1; <span class="comment">#start 763 end 954</span></span><br></pre></td></tr></table></figure>

<h3 id="选项"><a href="#选项" class="headerlink" title="选项"></a>选项</h3><p><strong>mysql连接配置</strong></p>
<p>-h host; -P port; -u user; -p password</p>
<p><strong>解析模式</strong></p>
<p>–stop-never 持续解析binlog。可选。默认False，同步至执行命令时最新的binlog位置。</p>
<p>-K, –no-primary-key 对INSERT语句去除主键。可选。默认False</p>
<p>-B, –flashback 生成回滚SQL，可解析大文件，不受内存限制。可选。默认False。与stop-never或no-primary-key不能同时添加。</p>
<p>–back-interval -B模式下，每打印一千行回滚SQL，加一句SLEEP多少秒，如不想加SLEEP，请设为0。可选。默认1.0。</p>
<p><strong>解析范围控制</strong></p>
<p>–start-file 起始解析文件，只需文件名，无需全路径 。必须。</p>
<p>–start-position/–start-pos 起始解析位置。可选。默认为start-file的起始位置。</p>
<p>–stop-file/–end-file 终止解析文件。可选。默认为start-file同一个文件。若解析模式为stop-never，此选项失效。</p>
<p>–stop-position/–end-pos 终止解析位置。可选。默认为stop-file的最末位置；若解析模式为stop-never，此选项失效。</p>
<p>–start-datetime 起始解析时间，格式’%Y-%m-%d %H:%M:%S’。可选。默认不过滤。</p>
<p>–stop-datetime 终止解析时间，格式’%Y-%m-%d %H:%M:%S’。可选。默认不过滤。</p>
<p><strong>对象过滤</strong></p>
<p>-d, –databases 只解析目标db的sql，多个库用空格隔开，如-d db1 db2。可选。默认为空。</p>
<p>-t, –tables 只解析目标table的sql，多张表用空格隔开，如-t tbl1 tbl2。可选。默认为空。</p>
<p>–only-dml 只解析dml，忽略ddl。可选。默认False。</p>
<p>–sql-type 只解析指定类型，支持INSERT, UPDATE, DELETE。多个类型用空格隔开，如–sql-type INSERT DELETE。可选。默认为增删改都解析。用了此参数但没填任何类型，则三者都不解析。</p>
<h3 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a>应用案例</h3><h4 id="误删整张表数据，需要紧急回滚"><a href="#误删整张表数据，需要紧急回滚" class="headerlink" title="误删整张表数据，需要紧急回滚"></a><strong>误删整张表数据，需要紧急回滚</strong></h4><p>闪回详细介绍可参见example目录下《闪回原理与实战》<a href="./example/mysql-flashback-priciple-and-practice.md">example/mysql-flashback-priciple-and-practice.md</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">test</span>库tbl表原有数据</span><br><span class="line">mysql&gt; select * from tbl;</span><br><span class="line">+----+--------+---------------------+</span><br><span class="line">| id | name   | addtime             |</span><br><span class="line">+----+--------+---------------------+</span><br><span class="line">|  1 | 小赵   | 2016-12-10 00:04:33 |</span><br><span class="line">|  2 | 小钱   | 2016-12-10 00:04:48 |</span><br><span class="line">|  3 | 小孙   | 2016-12-13 20:25:00 |</span><br><span class="line">|  4 | 小李   | 2016-12-12 00:00:00 |</span><br><span class="line">+----+--------+---------------------+</span><br><span class="line">4 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; delete from tbl;</span><br><span class="line">Query OK, 4 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">20:28时，tbl表误操作被清空</span><br><span class="line">mysql&gt; select * from tbl;</span><br><span class="line">Empty <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure>

<p><strong>恢复数据步骤</strong>：</p>
<ol>
<li><p>登录mysql，查看目前的binlog文件</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show master status;</span><br><span class="line">+------------------+-----------+</span><br><span class="line">| Log_name         | File_size |</span><br><span class="line">+------------------+-----------+</span><br><span class="line">| mysql-bin.000051 |       967 |</span><br><span class="line">| mysql-bin.000052 |       965 |</span><br><span class="line">+------------------+-----------+</span><br></pre></td></tr></table></figure>
</li>
<li><p>最新的binlog文件是mysql-bin.000052，我们再定位误操作SQL的binlog位置。误操作人只能知道大致的误操作时间，我们根据大致时间过滤数据。</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">shell&gt; python binlog2sql/binlog2sql.py -h127.0.0.1 -P3306 -uadmin -p<span class="string">'admin'</span> -dtest -ttbl --start-file=<span class="string">'mysql-bin.000052'</span> --start-datetime=<span class="string">'2016-12-13 20:25:00'</span> --stop-datetime=<span class="string">'2016-12-13 20:30:00'</span></span><br><span class="line">输出：</span><br><span class="line">INSERT INTO `<span class="built_in">test</span>`.`tbl`(`addtime`, `id`, `name`) VALUES (<span class="string">'2016-12-13 20:26:00'</span>, 4, <span class="string">'小李'</span>); <span class="comment">#start 317 end 487 time 2016-12-13 20:26:26</span></span><br><span class="line">UPDATE `<span class="built_in">test</span>`.`tbl` SET `addtime`=<span class="string">'2016-12-12 00:00:00'</span>, `id`=4, `name`=<span class="string">'小李'</span> WHERE `addtime`=<span class="string">'2016-12-13 20:26:00'</span> AND `id`=4 AND `name`=<span class="string">'小李'</span> LIMIT 1; <span class="comment">#start 514 end 701 time 2016-12-13 20:27:07</span></span><br><span class="line">DELETE FROM `<span class="built_in">test</span>`.`tbl` WHERE `addtime`=<span class="string">'2016-12-10 00:04:33'</span> AND `id`=1 AND `name`=<span class="string">'小赵'</span> LIMIT 1; <span class="comment">#start 728 end 938 time 2016-12-13 20:28:05</span></span><br><span class="line">DELETE FROM `<span class="built_in">test</span>`.`tbl` WHERE `addtime`=<span class="string">'2016-12-10 00:04:48'</span> AND `id`=2 AND `name`=<span class="string">'小钱'</span> LIMIT 1; <span class="comment">#start 728 end 938 time 2016-12-13 20:28:05</span></span><br><span class="line">DELETE FROM `<span class="built_in">test</span>`.`tbl` WHERE `addtime`=<span class="string">'2016-12-13 20:25:00'</span> AND `id`=3 AND `name`=<span class="string">'小孙'</span> LIMIT 1; <span class="comment">#start 728 end 938 time 2016-12-13 20:28:05</span></span><br><span class="line">DELETE FROM `<span class="built_in">test</span>`.`tbl` WHERE `addtime`=<span class="string">'2016-12-12 00:00:00'</span> AND `id`=4 AND `name`=<span class="string">'小李'</span> LIMIT 1; <span class="comment">#start 728 end 938 time 2016-12-13 20:28:05</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>我们得到了误操作sql的准确位置在728-938之间，再根据位置进一步过滤，使用flashback模式生成回滚sql，检查回滚sql是否正确(注：真实环境下，此步经常会进一步筛选出需要的sql。结合grep、编辑器等)</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">shell&gt; python binlog2sql/binlog2sql.py -h127.0.0.1 -P3306 -uadmin -p<span class="string">'admin'</span> -dtest -ttbl --start-file=<span class="string">'mysql-bin.000052'</span> --start-position=3346 --stop-position=3556 -B &gt; rollback.sql | cat</span><br><span class="line">输出：</span><br><span class="line">INSERT INTO `<span class="built_in">test</span>`.`tbl`(`addtime`, `id`, `name`) VALUES (<span class="string">'2016-12-12 00:00:00'</span>, 4, <span class="string">'小李'</span>); <span class="comment">#start 728 end 938 time 2016-12-13 20:28:05</span></span><br><span class="line">INSERT INTO `<span class="built_in">test</span>`.`tbl`(`addtime`, `id`, `name`) VALUES (<span class="string">'2016-12-13 20:25:00'</span>, 3, <span class="string">'小孙'</span>); <span class="comment">#start 728 end 938 time 2016-12-13 20:28:05</span></span><br><span class="line">INSERT INTO `<span class="built_in">test</span>`.`tbl`(`addtime`, `id`, `name`) VALUES (<span class="string">'2016-12-10 00:04:48'</span>, 2, <span class="string">'小钱'</span>); <span class="comment">#start 728 end 938 time 2016-12-13 20:28:05</span></span><br><span class="line">INSERT INTO `<span class="built_in">test</span>`.`tbl`(`addtime`, `id`, `name`) VALUES (<span class="string">'2016-12-10 00:04:33'</span>, 1, <span class="string">'小赵'</span>); <span class="comment">#start 728 end 938 time 2016-12-13 20:28:05</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>确认回滚sql正确，执行回滚语句。登录mysql确认，数据回滚成功。</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">shell&gt; mysql -h127.0.0.1 -P3306 -uadmin -p<span class="string">'admin'</span> &lt; rollback.sql</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from tbl;</span><br><span class="line">+----+--------+---------------------+</span><br><span class="line">| id | name   | addtime             |</span><br><span class="line">+----+--------+---------------------+</span><br><span class="line">|  1 | 小赵   | 2016-12-10 00:04:33 |</span><br><span class="line">|  2 | 小钱   | 2016-12-10 00:04:48 |</span><br><span class="line">|  3 | 小孙   | 2016-12-13 20:25:00 |</span><br><span class="line">|  4 | 小李   | 2016-12-12 00:00:00 |</span><br><span class="line">+----+--------+---------------------+</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="限制（对比mysqlbinlog）"><a href="#限制（对比mysqlbinlog）" class="headerlink" title="限制（对比mysqlbinlog）"></a>限制（对比mysqlbinlog）</h3><ul>
<li>mysql server必须开启，离线模式下不能解析</li>
<li>参数 <em>binlog_row_image</em> 必须为FULL，暂不支持MINIMAL</li>
<li>解析速度不如mysqlbinlog</li>
</ul>
<h3 id="优点（对比mysqlbinlog）"><a href="#优点（对比mysqlbinlog）" class="headerlink" title="优点（对比mysqlbinlog）"></a>优点（对比mysqlbinlog）</h3><ul>
<li>纯Python开发，安装与使用都很简单</li>
<li>自带flashback、no-primary-key解析模式，无需再装补丁</li>
<li>flashback模式下，更适合<a href="./example/mysql-flashback-priciple-and-practice.md">闪回实战</a></li>
<li>解析为标准SQL，方便理解、筛选</li>
<li>代码容易改造，可以支持更多个性化解析</li>
</ul>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>mongodb面试收集</title>
    <url>/posts/45b4ba0b.html</url>
    <content><![CDATA[<h1 id="mongodb"><a href="#mongodb" class="headerlink" title="mongodb"></a>mongodb</h1><h2 id="为什么需要-compact"><a href="#为什么需要-compact" class="headerlink" title="为什么需要 compact"></a>为什么需要 compact</h2><img src="/images/mongodb-compact.jpg" width="100%" height="100%">
<a id="more"></a>

<h2 id="remove-与-drop-的区别"><a href="#remove-与-drop-的区别" class="headerlink" title="remove 与 drop 的区别"></a>remove 与 drop 的区别</h2><p>MongoDB 里删除一个集合里所有文档，有两种方式</p>
<ul>
<li>db.collection.remove({}, {multi: true})，逐个文档从 btree 里删除，最后所有文档被删除，但文件物理空间不会被回收</li>
<li>db.collection.drop() 删除集合的物理文件，空间立即被回收</li>
</ul>
<p>总的来说，remove 会产生逻辑的空闲空间，这些空间能立即用于写入新数据，但文件占用的总物理空间不会立即回收；通常只要持续在写入数据，有物理空间碎片问题并不大，不需要去 compact 集合，有的场景，remove 了大量的数据后，后续的写入可能并不多，这时如果想回收空间，就需要显式的调用 compact。</p>
<h2 id="compact-命令对读写的影响"><a href="#compact-命令对读写的影响" class="headerlink" title="compact 命令对读写的影响"></a>compact 命令对读写的影响</h2><p>compact 一个集合，会加集合所在DB的互斥写锁，会导致该DB上所有的读写请求都阻塞；因为 compact 执行的时间可能很长，跟集合的数据量相关，所以强烈建议在业务低峰期执行，避免影响业务。</p>
<h2 id="compact-具体做了什么？"><a href="#compact-具体做了什么？" class="headerlink" title="compact 具体做了什么？"></a>compact 具体做了什么？</h2><p>Compact 动作最终由存储引擎 WiredTiger 完成，WiredTiger 在执行 compact 时，会不断将集合文件后面的数据往前面空闲的空间写，然后逐步 truancate 文件回收物理空间。每一轮 compact 前，WT 都会先检查是否符合 comapact 条件。</p>
<ul>
<li>前面80%的空间里，是否有20%的空闲空间，用于写入文件后面20%的数据，或者</li>
<li>前面90%的空间里，是否有10%的空闲空间，用于写入文件后面10%的数据</li>
</ul>
<p>如果上面都不满足，说明执行compact肯定无法回收10%的物理空间，此时 compact 就回退出。所以有时候遇到对一个大集合进行 compact，compact立马就返回ok，集合的物理空间也没有变化，就是因为 WiredTiger 认为这个集合没有 compact 的必要。</p>
]]></content>
      <categories>
        <category>面试收集</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 中的 netcat 网络工具简介</title>
    <url>/posts/6e6dc2cb.html</url>
    <content><![CDATA[<p>netcat 是 Linux 系统中的网络工具，其通过 TCP 和 UDP 协议在网络中读写数据。如果与其他工具结合，以及加上重定向功能，还可以实现很多不同的功能。所以其以体积小功能灵活而著称，可以用来做很多网络相关的工作。Posix 版本的 netcat 主要有 GNU 版本和 OpenBSD 两种，都可以在 debian/ubuntu 系统下面安装，但 Windows 系统下则只有 GNU 版本的。</p>
<a id="more"></a>
<p>可以通过如下命令查看其版本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master01 ~]# readlink -f $(which nc)</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;ncat</span><br></pre></td></tr></table></figure>

<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y nc</span><br></pre></td></tr></table></figure>

<h1 id="常用参数"><a href="#常用参数" class="headerlink" title="常用参数"></a>常用参数</h1><p>netcat 使用的基本形式为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc 参数 目的地址 端口</span><br></pre></td></tr></table></figure>

<p>常用的参数说明如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-k  在当前连接结束后保持继续监听</span><br><span class="line">-l  用作端口监听，而不是发送数据</span><br><span class="line">-n  不使用 DNS 解析</span><br><span class="line">-N  在遇到 EOF 时关闭网络连接</span><br><span class="line">-p  指定源端口</span><br><span class="line">-u  使用 UDP 协议传输</span><br><span class="line">-v  (Verbose)显示更多的详细信息</span><br><span class="line">-w  指定连接超时时间</span><br><span class="line">-z  不发送数据</span><br></pre></td></tr></table></figure>

<h1 id="常用的实例"><a href="#常用的实例" class="headerlink" title="常用的实例"></a>常用的实例</h1><h2 id="端口测试"><a href="#端口测试" class="headerlink" title="端口测试"></a>端口测试</h2><p>测试远程注意端口是否打开</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc -vz 192.168.1.8 8080</span><br></pre></td></tr></table></figure>

<p>还可以指定同时制定多个端口或者一个端口范围</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc -v -z -w 3 192.168.56.1 22 80 8000 8080</span><br><span class="line"></span><br><span class="line">nc -v -z -w 3 192.168.1.8 8080-8088</span><br></pre></td></tr></table></figure>

<h2 id="连接测试"><a href="#连接测试" class="headerlink" title="连接测试"></a>连接测试</h2><p>如果在主机上配置了防火墙，想要测试一下开放的端口是否可以与外界联通，可以用 netcat 监听该端口，然后从外界尝试连接。</p>
<p>在主机上执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc -l -p 8080</span><br></pre></td></tr></table></figure>

<p>在其它主机上可以尝试连接以上主机打开的端口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc 192.168.1.8 8080</span><br></pre></td></tr></table></figure>

<p>如果两台主机能够正常连通的话，就如同打开了一个聊天室，可以相互发送数据并显示。</p>
<h2 id="测试-UDP"><a href="#测试-UDP" class="headerlink" title="测试 UDP"></a>测试 UDP</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc -vuz 192.168.1.8 8080</span><br></pre></td></tr></table></figure>

<h2 id="文件传输"><a href="#文件传输" class="headerlink" title="文件传输"></a>文件传输</h2><p>如何两台主机间想要传输文件，而又不能使用 scp/szrz 等工具时，则可以用 netcat 代替。在需要接受文件的主机上执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc -l -p 8080 &gt; test.txt</span><br></pre></td></tr></table></figure>

<p>然后再另一台主机上向其传输文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc 192.168.1.8 8080 &lt; test.txt</span><br></pre></td></tr></table></figure>

<p>此外，也可以用 pv 命令来查看文件传输的进度。pv 命令即 Pipe Viewer，意思是通过管道显示数据处理进度的信息。</p>
<p>安装pv</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y pv</span><br></pre></td></tr></table></figure>

<p>其甚至可以用来代替 cp 命令拷贝文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@master02 ~]# pv etcd-v3.3.15-linux-amd64.tar.gz &gt;&#x2F;data&#x2F;etcd-v3.3.15-linux-amd64.tar.gz</span><br><span class="line">13.5MiB 0:00:00 [ 122MiB&#x2F;s] [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;] 100%</span><br></pre></td></tr></table></figure>

<p>使用 netcat 来做文件传输时，可以使用 pv 来显示传输的进度：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 传输端</span><br><span class="line">pv test.txt | nc 192.168.1.8 8080</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">甚至还可以传输一个目录：</span><br><span class="line"># 服务端</span><br><span class="line">tar -zcf - debian-10.0.0-amd64-xfce-CD-1.iso  | pv | nc -l -p 8080</span><br><span class="line"></span><br><span class="line"># 客户端</span><br><span class="line">nc 192.168.1.8 8080 | pv | tar -zxf -</span><br></pre></td></tr></table></figure>

<p>如何对传输的数据不放心，还可以进行加密，如使用 mcrypt 工具加密数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 服务端，使用 mcrypt 工具加密数据，需要输入密码</span><br><span class="line">nc localhost 1567 | mcrypt –flush –bare -F -q -d -m ecb &gt; file.txt</span><br><span class="line"></span><br><span class="line"># 客户端，使用 mcrypt 工具解密数据，需要输入密码</span><br><span class="line">mcrypt –flush –bare -F -q -m ecb &lt; file.txt | nc -l 1567</span><br></pre></td></tr></table></figure>

<h1 id="克隆设备"><a href="#克隆设备" class="headerlink" title="克隆设备"></a>克隆设备</h1><p>使用 netcat 可以用来对整个磁盘设备进行复制：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 服务端</span><br><span class="line">dd if&#x3D;&#x2F;dev&#x2F;sda | nc -l 1567</span><br><span class="line"></span><br><span class="line"># 客户端</span><br><span class="line">nc -n 182.168.1.8 | dd of&#x3D;&#x2F;dev&#x2F;sda</span><br></pre></td></tr></table></figure>

<h1 id="HTTP-服务器"><a href="#HTTP-服务器" class="headerlink" title="HTTP 服务器"></a>HTTP 服务器</h1><p>用 netcat 可以实现一个简单的 HTTP 服务器，用以测试一些功能。首先，创建一个简单地的 index.html 文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Test Page&lt;&#x2F;title&gt;</span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;p&gt;Serving this file using Netcat Basic HTTP server!&lt;&#x2F;p&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure>

<p>然后运行服务端：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while : ; do ( echo -ne &quot;HTTP&#x2F;1.1 200 OK\r\n&quot; ; cat index.html; ) | nc -l -p 8080 ; done</span><br></pre></td></tr></table></figure>

<h1 id="HTTP-客户端"><a href="#HTTP-客户端" class="headerlink" title="HTTP 客户端"></a>HTTP 客户端</h1><p>用 netcat 也可以实现一个简单的 HTTP 服务器，用于请求 HTTP 数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">printf &quot;GET &#x2F; HTTP&#x2F;1.0\r\n\r\n&quot; | nc konghy.cn 80</span><br></pre></td></tr></table></figure>

<h1 id="端口扫描"><a href="#端口扫描" class="headerlink" title="端口扫描"></a>端口扫描</h1><p>TCP端口扫描</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc -v -z -w2 192.168.0.3 1-100</span><br></pre></td></tr></table></figure>

<p>扫描UDP端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc -u -z -w2 192.168.0.1 1-1000 &#x2F;&#x2F;扫描192.168.0.3 的端口 范围是 1-1000</span><br></pre></td></tr></table></figure>






]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nc</tag>
        <tag>netcat</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql面试收集</title>
    <url>/posts/971f1f49.html</url>
    <content><![CDATA[<h2 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h2><h3 id="主从复制的原理"><a href="#主从复制的原理" class="headerlink" title="主从复制的原理"></a>主从复制的原理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MySql主库在事务提交时会把数据变更作为事件记录在二进制日志Binlog中；</span><br><span class="line">主库推送二进制日志文件Binlog中的事件到从库的中继日志Relay Log中，之后从库根据中继日志重做数据变更操作，通过逻辑复制来达到主库和从库的数据一致性；</span><br><span class="line">MySql通过三个线程来完成主从库间的数据复制，其中Binlog Dump线程跑在主库上，I&#x2F;O线程和SQL线程跑着从库上；</span><br><span class="line">当在从库上启动复制时，首先创建I&#x2F;O线程连接主库，主库随后创建Binlog Dump线程读取数据库事件并发送给I&#x2F;O线程，I&#x2F;O线程获取到事件数据后更新到从库的中继日志Relay Log中去，之后从库上的SQL线程读取中继日志Relay Log中更新的数据库事件并应用</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="主从复制的延迟问题"><a href="#主从复制的延迟问题" class="headerlink" title="主从复制的延迟问题"></a>主从复制的延迟问题</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">进行主从同步的过程中，如果使用异步或半异步模式，均会有主从节点数据不一致的窗口时间。同时，从节点上的 SQL Thread 只能串行执行 relay-log 中的记录，当某条 DDL&#x2F;DML 耗时较长时，会加剧这个窗口时间；再者在某些场景下会使用 slave 节点进行数据读取，这也可能导致数据加锁等待。基于以上原因在处理主从复制延迟问题上有以下几种方向：</span><br><span class="line"></span><br><span class="line">优化主从节点之间的网络延迟</span><br><span class="line">降低 master 负载，以减少 TPS</span><br><span class="line">降低 slave 负载，slave 只做备份使用，不提供服务</span><br><span class="line">调整 slave 参数：关闭 slave bin-log 等</span><br><span class="line">多线程的主从复制：不同 schema 下的表并发提交时的数据不会相互影响，即 slave 节点可以用对 relay log 中不同的 schema 各分配一个SQL Thread，来重放 relay log 中主库已经提交的事务</span><br></pre></td></tr></table></figure>

<h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><h3 id="https"><a href="#https" class="headerlink" title="https"></a>https</h3><img src="/images/627325-dc83fef6ac2e6c88.png" width="65%" height="65%">
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一个HTTPS请求实际上包含了两次HTTP传输，可以细分为8步。</span><br><span class="line">1.客户端向服务器发起HTTPS请求，连接到服务器的443端口</span><br><span class="line"></span><br><span class="line">2.服务器端有一个密钥对，即公钥和私钥，是用来进行非对称加密使用的，服务器端保存着私钥，不能将其泄露，公钥可以发送给任何人。</span><br><span class="line"></span><br><span class="line">3.服务器将自己的公钥发送给客户端。</span><br><span class="line"></span><br><span class="line">4.客户端收到服务器端的公钥之后，会对公钥进行检查，验证其合法性，如果发现发现公钥有问题，那么HTTPS传输就无法继续。严格的说，这里应该是验证服务器发送的数字证书的合法性，关于客户端如何验证数字证书的合法性，下文会进行说明。如果公钥合格，那么客户端会生成一个随机值，这个随机值就是用于进行对称加密的密钥，我们将该密钥称之为client key，即客户端密钥，这样在概念上和服务器端的密钥容易进行区分。然后用服务器的公钥对客户端密钥进行非对称加密，这样客户端密钥就变成密文了，至此，HTTPS中的第一次HTTP请求结束。</span><br><span class="line"></span><br><span class="line">5.客户端会发起HTTPS中的第二个HTTP请求，将加密之后的客户端密钥发送给服务器。</span><br><span class="line"></span><br><span class="line">6.服务器接收到客户端发来的密文之后，会用自己的私钥对其进行非对称解密，解密之后的明文就是客户端密钥，然后用客户端密钥对数据进行对称加密，这样数据就变成了密文。</span><br><span class="line"></span><br><span class="line">7.然后服务器将加密后的密文发送给客户端。</span><br><span class="line"></span><br><span class="line">8.客户端收到服务器发送来的密文，用客户端密钥对其进行对称解密，得到服务器发送的数据。这样HTTPS中的第二个HTTP请求结束，整个HTTPS传输完成。</span><br></pre></td></tr></table></figure>

<h3 id="tcp-三次握手"><a href="#tcp-三次握手" class="headerlink" title="tcp 三次握手"></a>tcp 三次握手</h3><img src="/images/tcp_handshake.png" width="65%" height="65%">
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一次握手：建立连接时，客户端发送 SYN 包（seq&#x3D;j）到服务器，并进入SYN_SENT状态，等待服务器确认。</span><br><span class="line">第二次握手：服务器收到 SYN 包，必须确认客户的 SYN（ack&#x3D;j+1），同时自己也发送一个 SYN 包（seq&#x3D;k），即 SYN + ACK 包，此时服务器进入 SYN_RECV 状态；</span><br><span class="line">第三次握手：客户端收到服务器的 SYN+ACK 包，向服务器发送确认包 ACK（ack&#x3D;k+1），此包发送完毕，客户端和服务器进入ESTABLISHED（TCP连接成功）状态，完成三次握手。</span><br></pre></td></tr></table></figure>

<h3 id="tcp-四次挥手"><a href="#tcp-四次挥手" class="headerlink" title="tcp 四次挥手"></a>tcp 四次挥手</h3><img src="/images/tcp_finish.jpg" width="65%" height="65%">
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。</span><br><span class="line"></span><br><span class="line">客户端A发送一个FIN，用来关闭客户A到服务器B的数据传送（报文段4）。</span><br><span class="line">服务器B收到这个FIN，它发回一个ACK，确认序号为收到的序号加1（报文段5）。和SYN一样，一个FIN将占用一个序号。</span><br><span class="line">服务器B关闭与客户端A的连接，发送一个FIN给客户端A（报文段6）。</span><br><span class="line">客户端A发回ACK报文确认，并将确认序号设置为收到序号加1（报文段7）</span><br></pre></td></tr></table></figure>


<h3 id="Pod-的终止"><a href="#Pod-的终止" class="headerlink" title="Pod 的终止"></a>Pod 的终止</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">流程示例：</span><br><span class="line"></span><br><span class="line">用户发送命令删除 Pod，使用的是默认的宽限期（30秒）</span><br><span class="line">API 服务器中的 Pod 会随着宽限期规定的时间进行更新，过了这个时间 Pod 就会被认为已 “死亡”。</span><br><span class="line">当使用客户端命令查询 Pod 状态时，Pod 显示为 “Terminating”。</span><br><span class="line">（和第 3 步同步进行）当 Kubelet 看到 Pod 由于步骤 2 中设置的时间而被标记为 terminating 状态时，它就开始执行关闭 Pod 流程。</span><br><span class="line">如果 Pod 定义了 preStop 钩子，就在 Pod 内部调用它。如果宽限期结束了，但是 preStop 钩子还在运行，那么就用小的（2 秒）扩展宽限期调用步骤 2。</span><br><span class="line">给 Pod 内的进程发送 TERM 信号。请注意，并不是所有 Pod 中的容器都会同时收到 TERM 信号，如果它们关闭的顺序很重要，则每个容器可能都需要一个 preStop 钩子。</span><br><span class="line">（和第 3 步同步进行）从服务的端点列表中删除 Pod，Pod 也不再被视为副本控制器的运行状态的 Pod 集的一部分。因为负载均衡器（如服务代理）会将其从轮换中删除，所以缓慢关闭的 Pod 无法继续为流量提供服务。</span><br><span class="line">当宽限期到期时，仍在 Pod 中运行的所有进程都会被 SIGKILL 信号杀死。</span><br><span class="line">kubelet 将通过设置宽限期为 0 （立即删除）来完成在 API 服务器上删除 Pod 的操作。该 Pod 从 API 服务器中消失，并且在客户端中不再可见。</span><br><span class="line">默认情况下，所有删除操作宽限期是 30 秒。kubectl delete 命令支持 --grace-period&#x3D;&lt;seconds&gt; 选项，允许用户覆盖默认值并声明他们自己的宽限期。设置为 0 会强制删除 Pod。您必须指定一个附加标志 --force 和 --grace-period&#x3D;0 才能执行强制删除操作。</span><br></pre></td></tr></table></figure>

<h3 id="ReplicaSet-Replication-Controller"><a href="#ReplicaSet-Replication-Controller" class="headerlink" title="ReplicaSet Replication Controller"></a>ReplicaSet Replication Controller</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ReplicaSet 是下一代的 Replication Controller。 ReplicaSet 和 Replication Controller 的唯一区别是选择器的支持</span><br><span class="line">ReplicaSet 支持新的基于集合的选择器需求，这在标签用户指南中有描述。而 Replication Controller 仅支持基于相等选择器的需求。</span><br></pre></td></tr></table></figure>

<h3 id="ReplicaSet-Deployment"><a href="#ReplicaSet-Deployment" class="headerlink" title="ReplicaSet Deployment"></a>ReplicaSet Deployment</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、Deployment 是一个更高级的概念，它管理 ReplicaSet，并向 Pod 提供声明式的更新以及许多其他有用的功能</span><br><span class="line">2、对于重启策略，.spec.template.spec.restartPolicy 唯一允许的取值是 Always，这也是默认值</span><br><span class="line">3、.spec.template.metadata.labels 必须匹配 .spec.selector，否则它将被 API 拒绝</span><br><span class="line">4、ReplicaSet 将本地容器重启的任务委托给了节点上的某个代理（例如，Kubelet 或 Docker）去完成。</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>面试收集</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>记一次阿里云更新回滚操作</title>
    <url>/posts/6d01a25c.html</url>
    <content><![CDATA[<pre><code>由于执行更新update语句时，没有选中where语句，导致全表更新，万幸的是该表只是操作记录表，数据量也不大。</code></pre><a id="more"></a>
<h2 id="数据库日志格式"><a href="#数据库日志格式" class="headerlink" title="数据库日志格式"></a>数据库日志格式</h2><p>通过日志恢复数据的日志格式必须是Row。</p>
<h2 id="下载binlog"><a href="#下载binlog" class="headerlink" title="下载binlog"></a>下载binlog</h2><p>通过阿里云管理界面下载binlog时间段内的文件</p>
<h2 id="解析binlog"><a href="#解析binlog" class="headerlink" title="解析binlog"></a>解析binlog</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqlbinlog -vv --base64-output&#x3D;decode-rows mysql-bin.004489 | grep -B 15 &#39;此处替换成要搜索的关键字&#39;</span><br><span class="line"></span><br><span class="line">参数说明:</span><br><span class="line">-v, --verbose</span><br><span class="line">      用于输出基于row模式的binlog日志，-vv为列数据类型添加注释</span><br><span class="line">--base64-output&#x3D;decode-rows</span><br><span class="line">      解码binlog里经过base64编码的内容</span><br><span class="line"></span><br><span class="line">-A 15 : 输出符合查询条件的日志后的15行</span><br><span class="line"></span><br><span class="line">-B 15 : 输出符合查询条件的日志前的15行</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mysqlbinlog -vv --base64-output&#x3D;decode-rows mysql-bin.004489 | sed -n &#39;&#x2F;# at 244950283&#x2F;,&#x2F;COMMIT&#x2F;p&#39; &gt; update.log</span><br></pre></td></tr></table></figure>

<p><strong>注：</strong></p>
<p>如果通过mysqlbinlog解析binlog时遇到类似如下提示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ERROR: Error in Log_event::read_log_event(): &#39;Sanity check failed&#39;, data_len: 151, event_type: 35</span><br><span class="line">ERROR: Could not read entry at offset 120: Error in log format or read error.</span><br></pre></td></tr></table></figure>
<p>请用户检查使用的mysqlbinlog是否版本较低，比如使用3.3版本会遇到上述错误提示无法正常解析binlog日志，使用较高版本，如3.4版本可以正常查看，这种情况下用户可以使用较高版本的mysqlbinlog</p>
<h2 id="将binlog转换成SQL语句"><a href="#将binlog转换成SQL语句" class="headerlink" title="将binlog转换成SQL语句"></a>将binlog转换成SQL语句</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;&#x2F;WHERE&#x2F;&#123;:a;N;&#x2F;SET&#x2F;!ba;s&#x2F;\([^\n]*\)\n\(.*\)\n\(.*\)&#x2F;\3\n\2\n\1&#x2F;&#125;&#39; 3.log | sed &#39;s&#x2F;### &#x2F;&#x2F;g;s&#x2F;\&#x2F;\*.*&#x2F;,&#x2F;g&#39; | sed  &#x2F;@10&#x2F;s&#x2F;,&#x2F;&#x2F;g | sed &#39;&#x2F;WHERE&#x2F;&#123;:a;N;&#x2F;@10&#x2F;!ba;s&#x2F;,&#x2F;AND&#x2F;g&#125;;s&#x2F;#.*&#x2F;&#x2F;g;s&#x2F;COMMIT,&#x2F;&#x2F;g&#39; | sed &#39;&#x2F;^$&#x2F;d&#39;  &gt;  rollback.sql</span><br></pre></td></tr></table></figure>

<p><strong>注：</strong></p>
<p>@3 为最后一个变量。如果有20个变量，则此处为@20。<br>@2 此处#与@之前空格数为3个，不能多不能少。</p>
<h3 id="分段解析"><a href="#分段解析" class="headerlink" title="分段解析"></a>分段解析</h3><p>第一个sed 命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;&#x2F;WHERE&#x2F;&#123;:a;N;&#x2F;SET&#x2F;!ba;s&#x2F;\([^\n]*\)\n\(.*\)\n\(.*\)&#x2F;\3\n\2\n\1&#x2F;&#125;&#39; update.sql</span><br><span class="line"></span><br><span class="line">功能：将where 和set未知对调</span><br><span class="line"></span><br><span class="line">&#x2F;WHERE&#x2F;  #包含WHERE</span><br><span class="line"></span><br><span class="line">:a;      #创建一个labela；</span><br><span class="line"></span><br><span class="line">N;       #追加下一个输入行到读取行的末尾,读入到模式空间</span><br><span class="line"></span><br><span class="line">&#x2F;SET&#x2F;!ba;   # 如果不是&#x2F;SET&#x2F;，返回a，也就是重复读，一直读到&#x2F;SET&#x2F;之前（buffer的内容是WHERE\n.......\nSET）</span><br><span class="line">    </span><br><span class="line">s&#x2F;\([^\n]*\)\n\(.*\)\n\(.*\)&#x2F;\3\n\2\n\1&#x2F;   这块可以分三部分来读</span><br><span class="line"></span><br><span class="line">第1步: </span><br><span class="line">	s  #替换命令，例如s&#x2F;a&#x2F;b  将a替换为b</span><br><span class="line"></span><br><span class="line">第2步:</span><br><span class="line">	\([^\n]*\)\n\(.*\)\n\(.*\) </span><br><span class="line"></span><br><span class="line">	\        #转义字符</span><br><span class="line"></span><br><span class="line">	[^\n]* &#x3D;&#x3D; buffer中的where</span><br><span class="line"></span><br><span class="line">	(.*\)    #单符号(.)匹配除换行符以外的单个字符，*同上；</span><br><span class="line"></span><br><span class="line">	[^\n]*\  #代表非换行符（回车）开头，*表示匹配零或多个字符</span><br><span class="line"></span><br><span class="line">	\n       #换行</span><br><span class="line"></span><br><span class="line">第3步：</span><br><span class="line">	\3\n\2\n\1   </span><br><span class="line"></span><br><span class="line">	\3  &#x3D;&#x3D; 内存中的set,第三个括号中的内容</span><br><span class="line"></span><br><span class="line">	\2  &#x3D;&#x3D; 内存中原来where与set之间的内容，第二个括号中的内容</span><br><span class="line"></span><br><span class="line">	\1  &#x3D;&#x3D; 内存中的where，第一个括号中的内容</span><br></pre></td></tr></table></figure>
<p>第二个sed 命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;s&#x2F;### &#x2F;&#x2F;g;s&#x2F;\&#x2F;\*.*&#x2F;,&#x2F;g&#39;</span><br><span class="line"></span><br><span class="line">功能：这句做了两个事情1.把字符串### 替换成 空格 2.把&#x2F;*往后的内容 替换成,</span><br><span class="line"></span><br><span class="line">s&#x2F;### &#x2F;&#x2F;g        #将### 替换成空串，</span><br><span class="line"></span><br><span class="line">\                #转义字符</span><br><span class="line"></span><br><span class="line">\&#x2F;\*.*           #匹配&#x2F;*之后出换行符外所有内容</span><br></pre></td></tr></table></figure>

<p>第三个sed 命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed  &#x2F;@10&#x2F;s&#x2F;,&#x2F;&#x2F;g</span><br><span class="line"></span><br><span class="line">功能：这句把字符串包含@3的行中的全部（，）换成空格</span><br><span class="line"></span><br><span class="line">&#x2F;@10&#x2F;    #匹配包含@10的行</span><br><span class="line"></span><br><span class="line">s&#x2F;,&#x2F;&#x2F;    #将,替换为空串</span><br><span class="line"></span><br><span class="line">g        #全部替换</span><br></pre></td></tr></table></figure>

<p>第四个sed 命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;&#x2F;WHERE&#x2F;&#123;:a;N;&#x2F;@3&#x2F;!ba;s&#x2F;,&#x2F;AND&#x2F;g&#125;;s&#x2F;#.*&#x2F;&#x2F;g;s&#x2F;COMMIT,&#x2F;&#x2F;g&#39;</span><br><span class="line"></span><br><span class="line">功能：这句做了三件事 1.就是把WHERE 至@3之间的所有逗号，替换成分号 AND 2.#.* 就是把#在的行替换为空格 3.就是把匹配到的COMMIT, 替换为空格</span><br><span class="line"></span><br><span class="line">&#x2F;@1&#x2F;!ba;s&#x2F;,&#x2F;;&#x2F;g       #将@1及之前的行尾的（，）替换为（；）</span><br><span class="line"></span><br><span class="line">s&#x2F;#.*&#x2F;&#x2F;g              #将#号开头的整行字符替换为空串。</span><br><span class="line"></span><br><span class="line">s&#x2F;COMMIT,&#x2F;&#x2F;g          #将(COMMIT,)替换为空行；</span><br></pre></td></tr></table></figure>

<p>第五个sed 命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;&#x2F;^$&#x2F;d&#39; &gt; rollback.sql</span><br><span class="line"></span><br><span class="line">功能：删除缓存中所有的空行。</span><br><span class="line"></span><br><span class="line">&#x2F;^$&#x2F;      #查找缓存内容中所有的空行</span><br><span class="line"></span><br><span class="line">d         #删除</span><br><span class="line"></span><br><span class="line"> &gt;  rollback.sql   #输出缓存中的内容到rollback.sql</span><br></pre></td></tr></table></figure>
<h2 id="将rollback-sql中的where语句后加（-）"><a href="#将rollback-sql中的where语句后加（-）" class="headerlink" title="将rollback.sql中的where语句后加（;）"></a>将rollback.sql中的where语句后加（;）</h2><p>如果后执行这句请将@10换成对应的列名即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed  -i -r  &#39;&#x2F;WHERE&#x2F;&#123;:a;N;&#x2F;@3&#x2F;!ba;s&#x2F;(@3&#x3D;.*)&#x2F;\1\;&#x2F;g&#125;&#39; rollback.sql</span><br></pre></td></tr></table></figure>

<h2 id="将-1-2-3…列转换为对应的列名"><a href="#将-1-2-3…列转换为对应的列名" class="headerlink" title="将@1,@2,@3…列转换为对应的列名"></a>将@1,@2,@3…列转换为对应的列名</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -i &#39;s&#x2F;@1&#x2F;f_id&#x2F;g;s&#x2F;@2&#x2F;f_outing_id&#x2F;g;s&#x2F;@3&#x2F;f_outing_date_id&#x2F;;s&#x2F;@4&#x2F;f_order_id&#x2F;;s&#x2F;@5&#x2F;f_order_member_id&#x2F;;s&#x2F;@6&#x2F;f_operater_type&#x2F;;s&#x2F;@7&#x2F;f_user_id&#x2F;;s&#x2F;@8&#x2F;f_system_user_id&#x2F;;s&#x2F;@9&#x2F;f_remark&#x2F;;s&#x2F;@10&#x2F;f_create_time&#x2F;g&#39; rollback.sql</span><br></pre></td></tr></table></figure>

<h2 id="数据格式化"><a href="#数据格式化" class="headerlink" title="数据格式化"></a>数据格式化</h2><p>将所有的换行替换成空格，此处用tr命令，因我的数据量比较大，tr执行效率相对较高，也可以用sed命令sed -i ‘:label;N;s/\n/ /;b label’ rollback.sql，效果都是一样的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat rollback.sql | tr  &quot;\n&quot; &quot; &quot; &gt; new_rollback.sql</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>network面试收集</title>
    <url>/posts/2ed8fca5.html</url>
    <content><![CDATA[<h2 id="如何查看http的并发请求数与其TCP连接状态？"><a href="#如何查看http的并发请求数与其TCP连接状态？" class="headerlink" title="如何查看http的并发请求数与其TCP连接状态？"></a>如何查看http的并发请求数与其TCP连接状态？</h2><a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aly-99 ~]# netstat -n | awk &#39;&#x2F;^tcp&#x2F; &#123;++state[$NF]&#125; END &#123;for(key in state) print key,&quot;\t&quot;,state[key]&#125;&#39;</span><br><span class="line">LAST_ACK 	 59</span><br><span class="line">CLOSE_WAIT 	 121</span><br><span class="line">ESTABLISHED 	 479</span><br><span class="line">FIN_WAIT1 	 1</span><br><span class="line">FIN_WAIT2 	 3</span><br><span class="line">TIME_WAIT 	 4341</span><br><span class="line"></span><br><span class="line">状态描述：</span><br><span class="line"></span><br><span class="line">CLOSED：无连接是活动的或正在进行</span><br><span class="line">LISTEN：服务器在等待进入呼叫</span><br><span class="line">SYN_RECV：一个连接请求已经到达，等待确认</span><br><span class="line">SYN_SENT：应用已经开始，打开一个连接</span><br><span class="line">ESTABLISHED：正常数据传输状态</span><br><span class="line">FIN_WAIT1：应用说它已经完成</span><br><span class="line">FIN_WAIT2：另一边已同意释放</span><br><span class="line">ITMED_WAIT：等待所有分组死掉</span><br><span class="line">CLOSING：两边同时尝试关闭</span><br><span class="line">TIME_WAIT：另一边已初始化一个释放</span><br><span class="line">LAST_ACK：等待所有分组死掉</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如发现系统存在大量TIME_WAIT状态的连接，通过调整内核参数解决:</span><br><span class="line">vim &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line"></span><br><span class="line">编辑文件，加入以下内容：</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_syncookies &#x3D; 1 &#x2F;&#x2F;表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN***，默认为0，表示关闭；</span><br><span class="line">net.ipv4.tcp_tw_reuse &#x3D; 1  &#x2F;&#x2F;表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；</span><br><span class="line">net.ipv4.tcp_tw_recycle &#x3D; 1  &#x2F;&#x2F;表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。</span><br><span class="line">net.ipv4.tcp_fin_timeout &#x3D; 30  &#x2F;&#x2F;修改系統默认的 TIMEOUT 时间</span><br><span class="line"></span><br><span class="line">TIME_WAIT状态的意义：</span><br><span class="line">客户端与服务器端建立TCP&#x2F;IP连接后关闭SOCKET后，服务器端连接的端口状态为TIME_WAIT。是不是所有执行主动关闭的socket都会进入TIME_WAIT状态呢？有没有什么情况使主动关闭的socket直接进入CLOSED状态呢？</span><br><span class="line">主动关闭的一方在发送最后一个 ack 后，就会进入TIME_WAIT 状态 停留2MSL（max segment lifetime）时间，这个是TCP&#x2F;IP必不可少的，也就是“解决”不了的。</span><br><span class="line"></span><br><span class="line">也就是TCP&#x2F;IP设计者本来是这么设计的，主要有两个原因：</span><br><span class="line"></span><br><span class="line">（1）防止上一次连接中的包，迷路后重新出现，影响新连接。经过2MSL，上一次连接中所有的重复包都会消失。</span><br><span class="line"></span><br><span class="line">（2）可靠的关闭TCP连接。在主动关闭方发送的最后一个 ack(fin) ，有可能丢失，这时被动方会重新发 fin，如果这时主动方处于 CLOSED 状态 ，就会响应 rst 而不是 ack。所以主动方要处于 TIME_WAIT 状态，而不能是 CLOSED 。</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>面试收集</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos7 安装nginx1.16</title>
    <url>/posts/5f85ab13.html</url>
    <content><![CDATA[<p>官网 <a href="http://nginx.org/en/download.html">http://nginx.org/en/download.html</a></p>
<a id="more"></a>
<h1 id="GCC编译器"><a href="#GCC编译器" class="headerlink" title="GCC编译器"></a>GCC编译器</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y gcc</span><br></pre></td></tr></table></figure>

<h1 id="PCRE库"><a href="#PCRE库" class="headerlink" title="PCRE库"></a>PCRE库</h1><p>Nginx的HTTP模块要用它来解析正则表达式。<br>pcre-devel是使用PCRE做二次开发时所需要的开发库。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y pcre pcre-devel</span><br></pre></td></tr></table></figure>

<h1 id="zlib库"><a href="#zlib库" class="headerlink" title="zlib库"></a>zlib库</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y zlib zlib-devel</span><br></pre></td></tr></table></figure>

<h1 id="OpenSSL库"><a href="#OpenSSL库" class="headerlink" title="OpenSSL库"></a>OpenSSL库</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y openssl openssl-devel</span><br></pre></td></tr></table></figure>

<h1 id="安装nginx"><a href="#安装nginx" class="headerlink" title="安装nginx"></a>安装nginx</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.16.1.tar.gz</span><br><span class="line">tar -zxvf nginx-1.16.1.tar.gz &amp;&amp; cd nginx-1.16.1&#x2F;</span><br><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --with-http_ssl_module --with-http_v2_module --with-http_stub_status_module --with-pcre --with-stream </span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure>

<h1 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx           #默认启动方式 start</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -t        #测试配置信息</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -v        #显示版本信息，-V（大V）显示编译时的参数</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -s stop   #快速停止服务</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -s quit   #正常停止服务</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -s reload #重新加载配置文件</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -s reopen #重新打开日志文件</span><br></pre></td></tr></table></figure>

<h1 id="启动命令脚本"><a href="#启动命令脚本" class="headerlink" title="启动命令脚本"></a>启动命令脚本</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;init.d&#x2F;nginx </span><br><span class="line"></span><br><span class="line">#! &#x2F;bin&#x2F;bash</span><br><span class="line"># chkconfig: - 85 15</span><br><span class="line">PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx</span><br><span class="line">DESC&#x3D;&quot;nginx daemon&quot;</span><br><span class="line">NAME&#x3D;nginx</span><br><span class="line">DAEMON&#x3D;$PATH&#x2F;sbin&#x2F;$NAME</span><br><span class="line">CONFIGFILE&#x3D;$PATH&#x2F;conf&#x2F;$NAME.conf</span><br><span class="line">PIDFILE&#x3D;$PATH&#x2F;logs&#x2F;$NAME.pid</span><br><span class="line">SCRIPTNAME&#x3D;&#x2F;etc&#x2F;init.d&#x2F;$NAME</span><br><span class="line">set -e</span><br><span class="line">[ -x &quot;$DAEMON&quot; ] || exit 0</span><br><span class="line">do_start() &#123;</span><br><span class="line">$DAEMON -c $CONFIGFILE || echo -n &quot;nginx already running&quot;</span><br><span class="line">&#125;</span><br><span class="line">do_stop() &#123;</span><br><span class="line">$DAEMON -s stop || echo -n &quot;nginx not running&quot;</span><br><span class="line">&#125;</span><br><span class="line">do_reload() &#123;</span><br><span class="line">$DAEMON -s reload || echo -n &quot;nginx can&#39;t reload&quot;</span><br><span class="line">&#125;</span><br><span class="line">case &quot;$1&quot; in</span><br><span class="line">start)</span><br><span class="line">echo -n &quot;Starting $DESC: $NAME&quot;</span><br><span class="line">do_start</span><br><span class="line">echo &quot;.&quot;</span><br><span class="line">;;</span><br><span class="line">stop)</span><br><span class="line">echo -n &quot;Stopping $DESC: $NAME&quot;</span><br><span class="line">do_stop</span><br><span class="line">echo &quot;.&quot;</span><br><span class="line">;;</span><br><span class="line">reload|graceful)</span><br><span class="line">echo -n &quot;Reloading $DESC configuration...&quot;</span><br><span class="line">do_reload</span><br><span class="line">echo &quot;.&quot;</span><br><span class="line">;;</span><br><span class="line">restart)</span><br><span class="line">echo -n &quot;Restarting $DESC: $NAME&quot;</span><br><span class="line">do_stop</span><br><span class="line">do_start</span><br><span class="line">echo &quot;.&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">echo &quot;Usage: $SCRIPTNAME &#123;start|stop|reload|restart&#125;&quot; &gt;&amp;2</span><br><span class="line">exit 3</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line">exit 0</span><br><span class="line"></span><br><span class="line">#设置执行权限 </span><br><span class="line">chmod a+x &#x2F;etc&#x2F;init.d&#x2F;nginx </span><br><span class="line">#注册成服务 </span><br><span class="line">chkconfig --add nginx </span><br><span class="line">#设置开机启动 </span><br><span class="line">chkconfig nginx on</span><br></pre></td></tr></table></figure>

<h1 id="平滑升级"><a href="#平滑升级" class="headerlink" title="平滑升级"></a>平滑升级</h1><h2 id="解压高版本的nginx压缩包-如nginx1-16-1"><a href="#解压高版本的nginx压缩包-如nginx1-16-1" class="headerlink" title="解压高版本的nginx压缩包,如nginx1.16.1"></a>解压高版本的nginx压缩包,如nginx1.16.1</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.16.1.tar.gz</span><br><span class="line">tar zxf nginx-1.16.1.tar.gz &amp;&amp; cd nginx-1.16.1</span><br></pre></td></tr></table></figure>

<h2 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h2><p><strong>注意：不要make install，会导致原先的主程失控</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --with-http_ssl_module --with-http_v2_module --with-http_stub_status_module --with-pcre --with-stream</span><br><span class="line">make</span><br></pre></td></tr></table></figure>

<h2 id="备份复制"><a href="#备份复制" class="headerlink" title="备份复制"></a>备份复制</h2><p>将原来的二进制系统程序文件备份一下，并复制新的nginx文件，替换原来的nginx二进制文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd objs&#x2F;</span><br><span class="line">cp -f nginx &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx</span><br></pre></td></tr></table></figure>
<h2 id="进行平滑升级"><a href="#进行平滑升级" class="headerlink" title="进行平滑升级"></a>进行平滑升级</h2><ul>
<li><p>-HUP 平滑启动（相当于reload）</p>
</li>
<li><p>-USR2 平滑升级可执行程序，主要用在版本升级</p>
</li>
<li><p>-WINCH 从容关闭工作进程</p>
</li>
<li><p>-USR1 重新打开日志文件，主要用在日志切割（相当于reopen） </p>
</li>
</ul>
<h3 id="kill-USR2-旧版本主进程号"><a href="#kill-USR2-旧版本主进程号" class="headerlink" title="kill -USR2 旧版本主进程号"></a>kill -USR2 旧版本主进程号</h3><p>执行新的主进程（新版本）和新的工作进程，依次启动新的主进程和新的工作进程，现在新，旧版本的nginx实例会同时运行，共同处理请求</p>
<h3 id="kill-WINCH-旧版本主进程号"><a href="#kill-WINCH-旧版本主进程号" class="headerlink" title="kill -WINCH 旧版本主进程号"></a>kill -WINCH 旧版本主进程号</h3><p>发送WINCH信号给旧版主进程，旧版主进程就开始从容关闭</p>
<p><strong>如果在版本升级完成后，没有任何问题，需要关闭老的master进程的话，可以使用下面的命令</strong></p>
<h3 id="kill-QUIT-旧版本主进程号"><a href="#kill-QUIT-旧版本主进程号" class="headerlink" title="kill -QUIT  旧版本主进程号"></a>kill -QUIT  旧版本主进程号</h3><h2 id="回退版本"><a href="#回退版本" class="headerlink" title="回退版本"></a>回退版本</h2><p>原来备份好旧版本的二进制文件的再复制回去</p>
<h3 id="kill-HUP-旧版本的主进程号"><a href="#kill-HUP-旧版本的主进程号" class="headerlink" title="kill -HUP 旧版本的主进程号"></a>kill -HUP 旧版本的主进程号</h3><p>nginx将在不重载配置文件的情况下启动旧版的worker进程</p>
<h3 id="kill-USR2-新版本的主进程号"><a href="#kill-USR2-新版本的主进程号" class="headerlink" title="kill -USR2 新版本的主进程号"></a>kill -USR2 新版本的主进程号</h3><p>依次启动新的主进程（旧版本）和新的工作进程，现在两个版本的nginx实例会同时运行，共同处理请求</p>
<h3 id="kill-WINCH-新版本主进程号"><a href="#kill-WINCH-新版本主进程号" class="headerlink" title="kill -WINCH 新版本主进程号"></a>kill -WINCH 新版本主进程号</h3><p>发送WINCH信号给新版主进程，新版主进程就开始从容关闭</p>
<h1 id="配置案例"><a href="#配置案例" class="headerlink" title="配置案例"></a>配置案例</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># tcp loadbalance</span><br><span class="line">stream &#123;</span><br><span class="line">	server &#123;</span><br><span class="line">                listen 8081 so_keepalive&#x3D;on;  #保持长连接</span><br><span class="line">                proxy_pass 10.0.2.48:3306;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">备注：[so_keepalive&#x3D;on|off|keepidle:keepintvl:keepcnt|proxy_protocol]</span><br><span class="line">关于TCP探活机制的几个参数的说明：</span><br><span class="line"></span><br><span class="line">keepcnt 关闭一个非活跃连接之前进行探测的最大次数t</span><br><span class="line">keepidle 对一个连接进行有效性探测之前运行的最大非活跃时间间隔</span><br><span class="line">keepintvl 两个探测的时间间隔</span><br><span class="line">设置如下参数：</span><br><span class="line"></span><br><span class="line">listen 1936 so_keepalive&#x3D;5s:2:2; </span><br><span class="line"></span><br><span class="line"># http loadbalance</span><br><span class="line">upstream  gateway&#123;</span><br><span class="line">        server 10.168.4.5:31113;</span><br><span class="line">        &#125;</span><br><span class="line">server &#123;</span><br><span class="line">        listen       16400;</span><br><span class="line">        server_name  localhost;</span><br><span class="line">        location &#x2F; &#123;</span><br><span class="line">            proxy_pass  http:&#x2F;&#x2F;gateway;</span><br><span class="line">            proxy_set_header Host $host;</span><br><span class="line">            proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">            proxy_set_header X-Forwarded-Proto $scheme;</span><br><span class="line">            proxy_set_header X-Forwarded-Port $server_port;</span><br><span class="line">        &#125;</span><br><span class="line">		location ~ (^&#x2F;f&#x2F;|^&#x2F;mpmt-user&#x2F;|^&#x2F;mpmt-mytask&#x2F;) &#123;</span><br><span class="line">            proxy_pass  http:&#x2F;&#x2F;gateway;</span><br><span class="line">            proxy_set_header Host $host;</span><br><span class="line">            proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">            proxy_set_header X-Forwarded-Proto $scheme;</span><br><span class="line">            proxy_set_header X-Forwarded-Port $server_port;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># rewrite</span><br><span class="line"></span><br><span class="line"># 访问域名带有&#x2F;mpmt&#x2F;webinfo&#x2F;</span><br><span class="line">location ^~ &#x2F;mpmt&#x2F;webinfo&#x2F; &#123;</span><br><span class="line">                return 301 https:&#x2F;&#x2F;xxx.cn$request_uri;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"># 访问具体页面进行rewrite</span><br><span class="line">location ^~ &#x2F;mpmt&#x2F;app&#x2F;index.html &#123;</span><br><span class="line">                rewrite &quot;(.*)$&quot; https:&#x2F;&#x2F;xxx.com&#x2F;mpmt&#x2F;app&#x2F;download&#x2F;href.html;</span><br><span class="line">        &#125;</span><br><span class="line"># 访问具体页面进行rewrite</span><br><span class="line">location &#x2F;download.html &#123;</span><br><span class="line">           rewrite ^&#x2F;download.html$   https:&#x2F;&#x2F;xxx.com&#x2F;mpmt&#x2F;app&#x2F;download&#x2F;download.html;</span><br><span class="line">         &#125;</span><br><span class="line"># 访问带有apk后缀进行rewrite</span><br><span class="line">location ~ .*\.(apk)$ &#123;</span><br><span class="line">        rewrite &quot;(.*)$&quot; https:&#x2F;&#x2F;xxx.com&#x2F;mpmt&#x2F;webinfo&#x2F;version&#x2F;sz.apk;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>平滑升级nginx1.14.1以支持http2.0</title>
    <url>/posts/68a1789d.html</url>
    <content><![CDATA[<h1 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h1><p>HTTP/2也被称为HTTP 2.0，是最新的HTTP协议。目前，Chrome、 IE11、Safari以及Firefox 等主流浏览器已经支持 HTTP/2协议。HTTP/2优化了性能，兼容了HTTP/1.1的语义，与SPDY相似，与HTTP/1.1有巨大区别。</p>
<p>SPDY是Google开发的基于TCP的应用层协议，用以最小化网络延迟，提升网络速度，优化用户的网络使用体验。SPDY并不是一种用于替代HTTP的协议，而是对HTTP协议的增强。新协议的功能包括数据流的多路复用、请求优先级以及HTTP报头压缩，与HTTP/2相似。</p>
<p>HTTP/2的优势</p>
<ol>
<li>二进制协议：相比于HTTP 1.x 基于文本的解析，HTTP/2将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。基于二进制可以让协议有更多的扩展性，比如引入了帧来传输数据和指令。</li>
<li>内容安全：HTTP/2基于HTTPS，因此天然具有安全特性。通过HTTP/2的特性可以避免单纯使用HTTPS的性能下降。</li>
<li>多路复用（MultiPlexing）：通过该功能，在一条连接上，您的浏览器可以同时发起无数个请求，并且响应可以同时返回。另外，多路复用中支持了流的优先级（Stream dependencies）设置，允许客户端告诉服务器哪些内容是更优先级的资源，可以优先传输。</li>
<li>Header压缩（Header compression）：HTTP请求头带有大量信息，而且每次都要重复发送。HTTP/2 采用HPACK格式进行压缩传输，通讯双方各自缓存一份头域索引表，相同的消息头只发送索引号，从而提高效率和速度。</li>
<li>服务端推送（Server push）：同SPDY一样，HTTP/2 也具有客户端推送功能。目前，有大多数网站已经启用HTTP/2，如淘宝。使用Chrome浏览器登陆控制台，您可以查看是否启用 HTTP/2 。</li>
</ol>
<a id="more"></a>

<h1 id="升级HTTP2必要条件"><a href="#升级HTTP2必要条件" class="headerlink" title="升级HTTP2必要条件"></a>升级HTTP2必要条件</h1><ol>
<li>nginx 1.9.5版本以上</li>
<li>nginx基于1.0.2以上版本的openssl编译</li>
<li>必须支持https</li>
</ol>
<h1 id="升级openssl"><a href="#升级openssl" class="headerlink" title="升级openssl"></a>升级openssl</h1><p>openssl官网地址：<a href="https://www.openssl.org/source/">https://www.openssl.org/source/</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载openssl</span><br><span class="line">wget http:&#x2F;&#x2F;www.openssl.org&#x2F;source&#x2F;openssl-1.0.2p.tar.gz</span><br><span class="line">tar -zxvf openssl-1.0.2p.tar.gz</span><br><span class="line">cd  openssl-1.0.2p</span><br><span class="line">.&#x2F;config shared zlib</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line"># 修改历史的OpenSSL文件设置备份</span><br><span class="line">mv &#x2F;usr&#x2F;bin&#x2F;openssl &#x2F;usr&#x2F;bin&#x2F;openssl.old</span><br><span class="line">mv &#x2F;usr&#x2F;include&#x2F;openssl &#x2F;usr&#x2F;include&#x2F;openssl.old</span><br><span class="line"></span><br><span class="line"># 设置软连接使其使用新的OpenSSL版本 刚刚安装的OpenSSL默认安装在&#x2F;usr&#x2F;local&#x2F;ssl</span><br><span class="line">ln -s &#x2F;usr&#x2F;local&#x2F;ssl&#x2F;bin&#x2F;openssl &#x2F;usr&#x2F;bin&#x2F;openssl</span><br><span class="line">ln -s &#x2F;usr&#x2F;local&#x2F;ssl&#x2F;include&#x2F;openssl &#x2F;usr&#x2F;include&#x2F;openssl</span><br><span class="line"> </span><br><span class="line"># 更新动态链接库数据</span><br><span class="line">echo &quot;&#x2F;usr&#x2F;local&#x2F;ssl&#x2F;lib&quot; &gt;&gt; &#x2F;etc&#x2F;ld.so.conf</span><br><span class="line">ldconfig -v</span><br><span class="line"></span><br><span class="line"># OpenSSL版本信息</span><br><span class="line">openssl version -a</span><br></pre></td></tr></table></figure>

<h1 id="升级nginx"><a href="#升级nginx" class="headerlink" title="升级nginx"></a>升级nginx</h1><p>nginx官网地址：<a href="http://nginx.org/en/download.html">http://nginx.org/en/download.html</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载nginx</span><br><span class="line">wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.14.1.tar.gz</span><br><span class="line">tar zxvf nginx-1.14.1.tar.gz</span><br><span class="line">cd nginx-1.14.1</span><br><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --with-openssl&#x3D;..&#x2F;openssl-1.0.2p --with-http_stub_status_module --with-http_v2_module --with-http_gzip_static_module --with-http_sub_module --with-pcre --with-http_ssl_module</span><br><span class="line"></span><br><span class="line"># 编译nginx</span><br><span class="line">make</span><br><span class="line"></span><br><span class="line"># 复制编译后的nginx到原nginx sbin目录下</span><br><span class="line">cd nginx-1.14.1</span><br><span class="line">cp objs&#x2F;nginx &#x2F;usr&#x2F;local&#x2F;nginx</span><br><span class="line"></span><br><span class="line"># nginx平滑升级</span><br><span class="line">make upgrade</span><br></pre></td></tr></table></figure>

<h1 id="配置nginx"><a href="#配置nginx" class="headerlink" title="配置nginx"></a>配置nginx</h1><p>配置文件增加以下配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">listen 443 ssl http2；</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>Consul集群搭建，配合nginx完成服务动态发现和健康检查</title>
    <url>/posts/5042a493.html</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>consul是一个服务发现和配置共享的服务软件，结合nginx的主动健康检查模块nginx_upstream_check_module和服务发现模块nginx-upsync-module，实现一套服务动态发现机制。nginx的upstream不再通过手动配置，而是定时向consul发送请求，获取consul数据中心的配置文件，动态更新upstream地址池。</p>
<a id="more"></a>
<h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><p>consul：是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件</p>
<p>nginx_upstream_check_module：nginx主动健康检查模块</p>
<p>nginx-upsync-module：nginx服务发现模块</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="nginx"><a href="#nginx" class="headerlink" title="nginx"></a>nginx</h2><p>nginx需要编译两个模块：<br>nginx_upstream_check_module：nginx主动健康检查模块</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;xiaokai-wang&#x2F;nginx_upstream_check_module</span><br></pre></td></tr></table></figure>
<p>nginx-upsync-module：nginx服务发现模块</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;weibocom&#x2F;nginx-upsync-module</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --user&#x3D;nginx --group&#x3D;nginx --with-http_ssl_module --with-http_flv_module --with-http_stub_status_module --with-http_gzip_static_module --with-http_realip_module --http-client-body-temp-path&#x3D;&#x2F;var&#x2F;tmp&#x2F;nginx&#x2F;client&#x2F; --http-proxy-temp-path&#x3D;&#x2F;var&#x2F;tmp&#x2F;nginx&#x2F;proxy&#x2F; --http-fastcgi-temp-path&#x3D;&#x2F;var&#x2F;tmp&#x2F;nginx&#x2F;fcgi&#x2F; --http-uwsgi-temp-path&#x3D;&#x2F;var&#x2F;tmp&#x2F;nginx&#x2F;uwsgi --http-scgi-temp-path&#x3D;&#x2F;var&#x2F;tmp&#x2F;nginx&#x2F;scgi --with-pcre --add-module&#x3D;..&#x2F;nginx-upsync-module-master --add-module&#x3D;&#x2F;root&#x2F;nginx-module-vts</span><br></pre></td></tr></table></figure>

<h2 id="consul"><a href="#consul" class="headerlink" title="consul"></a>consul</h2><p>官网 <a href="https://www.consul.io">https://www.consul.io</a></p>
<p>下载consul,linux 64位</p>
<p>下载解压即可，产生一个consul可执行文件。</p>
<p>./consul 列出一些常用指令。</p>
<h2 id="consul启动"><a href="#consul启动" class="headerlink" title="consul启动"></a>consul启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;consul agent -server –bootstrap-expect 1 –data-dir &#x2F;tmp&#x2F;consul –bind&#x3D;172.16.2.30 –ui –client 0.0.0.0 &amp;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">server： 以server身份启动。</span><br><span class="line"></span><br><span class="line">bootstrap-expect：集群要求的最少server数量，当低于这个数量，集群即失效。经测试，低于这个数量也不影响访问</span><br><span class="line"></span><br><span class="line">data-dir：data存放的目录，更多信息请参阅consul数据同步机制</span><br><span class="line"></span><br><span class="line">node：节点id，在同一集群不能重复。</span><br><span class="line"></span><br><span class="line">bind：监听的ip地址。</span><br><span class="line"></span><br><span class="line">client 客户端的ip地址</span><br><span class="line"></span><br><span class="line">&amp;  ：在后台运行，此为linux脚本语法</span><br><span class="line"></span><br><span class="line">ui：启动webui，端口8500</span><br></pre></td></tr></table></figure>
<p>访问ip:8500/</p>
<h2 id="consul其它命令"><a href="#consul其它命令" class="headerlink" title="consul其它命令"></a>consul其它命令</h2><h3 id="关闭"><a href="#关闭" class="headerlink" title="关闭"></a>关闭</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;consul leave</span><br></pre></td></tr></table></figure>
<h3 id="查看成员"><a href="#查看成员" class="headerlink" title="查看成员"></a>查看成员</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;consul members</span><br></pre></td></tr></table></figure>
<h2 id="启动consul集群"><a href="#启动consul集群" class="headerlink" title="启动consul集群"></a>启动consul集群</h2><p>以上介绍的都是以单机模式启动，实战中consul多以集群模式存在，建议server节点数为3~5个。以下以3台为例，分别为ip1、ip2、ip3：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;consul agent -server -bootstrap-expect 2 -data-dir &#x2F;tmp&#x2F;consul -node&#x3D;consul1 -bind&#x3D;ip1 -ui -client&#x3D;0.0.0.0 &amp;</span><br><span class="line"></span><br><span class="line">.&#x2F;consul agent -server -bootstrap-expect 2 -data-dir &#x2F;tmp&#x2F;consul -node&#x3D;consul2 -bind&#x3D;ip2 -join&#x3D;ip1 -ui -client&#x3D;0.0.0.0 &amp;</span><br><span class="line"></span><br><span class="line">.&#x2F;consul agent -server -bootstrap-expect 2 -data-dir &#x2F;tmp&#x2F;consul -node&#x3D;consul3 -bind&#x3D;ip3 -join&#x3D;ip1 -ui -client&#x3D;0.0.0.0 &amp;</span><br><span class="line"></span><br><span class="line">-join 加入一个集群</span><br></pre></td></tr></table></figure>

<h2 id="加入后端服务器-或可以在界面KEY-VALUE操作"><a href="#加入后端服务器-或可以在界面KEY-VALUE操作" class="headerlink" title="加入后端服务器(或可以在界面KEY/VALUE操作)"></a>加入后端服务器(或可以在界面KEY/VALUE操作)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在任一节点上执行如下命令，即可添加2个key-value信息：</span><br><span class="line">curl -X PUT -d &#39;&#123;&quot;weight&quot;:10, &quot;max_fails&quot;:2, &quot;fail_timeout&quot;:10, &quot;down&quot;:0&#125;&#39; http:&#x2F;&#x2F;172.16.2.30:8500&#x2F;v1&#x2F;kv&#x2F;upstreams&#x2F;test&#x2F;172.16.2.31:80</span><br><span class="line"></span><br><span class="line">curl -X PUT -d &#39;&#123;&quot;weight&quot;:10, &quot;max_fails&quot;:2, &quot;fail_timeout&quot;:10, &quot;down&quot;:0&#125;&#39; http:&#x2F;&#x2F;172.16.2.30:8500&#x2F;v1&#x2F;kv&#x2F;upstreams&#x2F;test&#x2F;172.16.2.32:80</span><br></pre></td></tr></table></figure>

<h2 id="删除后端服务器-或可以在界面KEY-VALUE操作"><a href="#删除后端服务器-或可以在界面KEY-VALUE操作" class="headerlink" title="删除后端服务器(或可以在界面KEY/VALUE操作)"></a>删除后端服务器(或可以在界面KEY/VALUE操作)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X DELETE http:&#x2F;&#x2F;172.16.2.30:8500&#x2F;v1&#x2F;kv&#x2F;upstreams&#x2F;test&#x2F;172.16.2.31:80</span><br><span class="line"></span><br><span class="line">curl -X DELETE http:&#x2F;&#x2F;172.16.2.30:8500&#x2F;v1&#x2F;kv&#x2F;upstreams&#x2F;test&#x2F;172.16.2.32:80</span><br></pre></td></tr></table></figure>

<h2 id="调整后端服务的参数-或可以在界面KEY-VALUE操作"><a href="#调整后端服务的参数-或可以在界面KEY-VALUE操作" class="headerlink" title="调整后端服务的参数(或可以在界面KEY/VALUE操作)"></a>调整后端服务的参数(或可以在界面KEY/VALUE操作)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X PUT -d &#39;&#123;&quot;weight&quot;:10, &quot;max_fails&quot;:2, &quot;fail_timeout&quot;:10, &quot;down&quot;:0&#125;&#39; http:&#x2F;&#x2F;172.16.2.30:8500&#x2F;v1&#x2F;kv&#x2F;upstreams&#x2F;test&#x2F;172.16.2.31:80</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="nginx-amp-upstream配置"><a href="#nginx-amp-upstream配置" class="headerlink" title="nginx&amp;upstream配置"></a>nginx&amp;upstream配置</h2><p>consul是针对nginx的upstream所做的一项改善，地址池不再需要手动配置，而是从consul的数据中心抓取。新的upstream配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream tomcat_http_server &#123;</span><br><span class="line">        server 127.0.0.1:11111;</span><br><span class="line">        upsync 172.16.2.30:8500&#x2F;v1&#x2F;kv&#x2F;upstreams&#x2F;test upsync_timeout&#x3D;6m upsync_interval&#x3D;500ms upsync_type&#x3D;consul strong_dependency&#x3D;off;</span><br><span class="line">        upsync_dump_path &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;server&#x2F;server_test.conf;</span><br><span class="line"> </span><br><span class="line">        check interval&#x3D;1000 rise&#x3D;2 fall&#x3D;2 timeout&#x3D;3000 type&#x3D;http default_down&#x3D;false;</span><br><span class="line">        check_http_send &quot;HEAD &#x2F; HTTP&#x2F;1.0\r\n\r\n&quot;;</span><br><span class="line">        check_http_expect_alive http_2xx http_3xx;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>server 127.0.0.1:11111是占位机器，这个配置必须要有不然校验配置文件不通过。</p>
<p>upsync配置语法：</p>
<p>upsync $consul/etcd.api.com:$port/v1/kv/upstreams/$upstream_name/ [upsync_type=consul/etcd] [upsync_interval=second/minutes] [upsync_timeout=second/minutes] [strong_dependency=off/on]</p>
<p>默认upsync_interval=5s upsync_timeout=6m strong_dependency=off</p>
<p>172.16.2.30:8500/v1/kv/upstreams/tomcat_http_server为同步地址；upsync_timeout同步超时时间；upsync_interval同步间隔；upsync_type同步类型，默认为consul；strong_dependency，配置为on时，每次启动或重启nginx，都会强制去consul拉一次upstream servers。</p>
<p>upsync_dump_path将拉取到的upstreams地址池写入一个文件；</p>
<p>此处想要多说两句，即使consul中途挂掉，nginx仍然可以从upsync_dump_path配置的文件中取到数据，继续分发流量，只是此时upstream池变为静态了，跟之前的情形一样，启停重启nginx等操作并没有问题。所以consul单节点配置中心的可用性也是很高的。</p>
<p>check代表健康检查；interval检查间隔，单位为毫秒；rise成功该次数后，标记为up；fall失败该次数后，标记为down；timeout；type包括tcp、ssl_hello、http、mysql、ajp、fastcgi；default_down设置后端server的初始状态；</p>
<p>默认配置interval=30000 fall=5 rise=2 timeout=1000 default_down=true type=tcp</p>
<p>check_http_send 健康检查发送的请求包；</p>
<p>check_http_expect_alive 这些状态代表后端server是活着的；</p>
<h2 id="查询健康检查状态"><a href="#查询健康检查状态" class="headerlink" title="查询健康检查状态"></a>查询健康检查状态</h2><p>健康检查模块提供了一个接口check_status，用于检查consul数据中心配置的所有server的健康检查状态。需要在nginx稍作配置：</p>
<p>在80端口下，配置nstatus的接口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location &#x2F;nstatus &#123;</span><br><span class="line">        check_status;</span><br><span class="line">        access_log off;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>Ngins</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx优化</title>
    <url>/posts/37c10181.html</url>
    <content><![CDATA[<h1 id="Nginx-安全优化"><a href="#Nginx-安全优化" class="headerlink" title="Nginx 安全优化"></a>Nginx 安全优化</h1><h2 id="隐藏Nginx版本号"><a href="#隐藏Nginx版本号" class="headerlink" title="隐藏Nginx版本号"></a>隐藏Nginx版本号</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http</span><br><span class="line">&#123;</span><br><span class="line">server_tokens off;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="Nginx更改默认用户"><a href="#Nginx更改默认用户" class="headerlink" title="Nginx更改默认用户"></a>Nginx更改默认用户</h2><p>nginx配置中指定用户启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ vim &#x2F;etc&#x2F;nginx&#x2F;nginx.conf</span><br><span class="line">user  nginx;   # 指定用户启动nginx</span><br></pre></td></tr></table></figure>

<h1 id="优化Nginx服务性能"><a href="#优化Nginx服务性能" class="headerlink" title="优化Nginx服务性能"></a>优化Nginx服务性能</h1><h2 id="Nginx指定进程数"><a href="#Nginx指定进程数" class="headerlink" title="Nginx指定进程数"></a>Nginx指定进程数</h2><p>worker_processes语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">syntax:         worker_processes number;    #此行为参数语法，number为数量</span><br><span class="line">default:        worker_processes 1;     #此行意思是不匹配该参数，软件默认情况数量为1</span><br><span class="line">context:        main;   #此行为worker_processes参数可以放置的位置</span><br></pre></td></tr></table></figure>

<p>优化Nginx进程个数的策略：<br>worker进程数最开始的设置可以等于CPU的核数，且worker进程数要多一些，这样起始提供服务时就不会出现因为访问量快速增加而临时启动新进程提供服务的问题，缩短了系统的瞬时开销和提供服务的时间，提升了服务用户的速度。高流量高并发场合也可以考虑将进程数提高至CPU核数*2，具体情况要根据实际的业务来选择，因为这个参数除了要和CPU核数匹配外，也和硬盘存储的数据及系统的负载有关，设置为CPU的核数是一个好的起始配置，这也是官方的建议。</p>
<h2 id="Nginx-进程优化"><a href="#Nginx-进程优化" class="headerlink" title="Nginx 进程优化"></a>Nginx 进程优化</h2><p>默认情况下，Nginx的多个进程有可能跑在某一个CPU或CPU的某一核上，导致Nginx进程使用硬件的资源不均，本节的优化是尽可能地分配不同的Nginx进程给不同的CPU处理，达到充分有效利用硬件的多CPU多核资源的目的。</p>
<p>在优化不同的Nginx进程对应不同的CPU配置时，四核CPU服务器的参数配置参考如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">worker_processes 4;</span><br><span class="line">worker_cpu_affinity 0001 0010 0100 1000;</span><br><span class="line"># worker_cpu_affinity就是配置Nginx进程与CPU亲和力的参数，即把不同的进程分给不同的CPU处理。这里0001 0010 0100 1000是掩码，分别代表第1，2，3，4核CPU，由于worker_processes进程数为4，因此，上述配置会把每个进程分配一核CPU处理，默认情况下进程不会绑定任何CPU，参数位置为main段。</span><br></pre></td></tr></table></figure>

<p>四核和八核CPU服务器的参数配置参考如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 八核掩码</span><br><span class="line">worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000；</span><br><span class="line"># 四核掩码</span><br><span class="line">worker_cpu_affinity 0001 0010 0100 1000；</span><br><span class="line"># worker_cpu_affinity 的作用是绑定不同的worker进程数到一组CPU上。通过设置bitmask控制进程允许使用的CPU，默认worker进程不会绑定到任何CPU（自动平均分配。）</span><br></pre></td></tr></table></figure>

<p>下面是绑定的实例配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">worker_processes  4;</span><br><span class="line">worker_cpu_affinity 0001 0010 0100 1000;</span><br><span class="line">worker_processes  2;</span><br><span class="line">worker_cpu_affinity 0101 1010;</span><br><span class="line"></span><br><span class="line"># 最佳方式绑定方式 </span><br><span class="line">worker_processes auto;</span><br><span class="line">worker_cpu_affinity auto;</span><br></pre></td></tr></table></figure>

<h2 id="Nginx-事件模型优化"><a href="#Nginx-事件模型优化" class="headerlink" title="Nginx 事件模型优化"></a>Nginx 事件模型优化</h2><p>Nginx的连接处理机制在不同的操作系统会采用不同的I/O模型，在Linux下，Nginx使用epoll的I/O多路复用模型，在Freebsd中使用kqueue的I/O多路复用模型，在Solaris中使用/dev/poll方式的I/O多路复用模型，在Windows中使用的是icop，等等。要根据系统类型选择不同的事件处理模型，可供使用的选择有“use [kqueue|rtsig|epoll|/dev/poll|select|poll];”。</p>
<p>事件模型语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">synstax:  use method;  # 络模型配置，method选择模型之一</span><br><span class="line">default:   --  		     # 默认没有设置</span><br><span class="line">context:  events        # 网络模型配置放置events区块内</span><br></pre></td></tr></table></figure>

<p>实践修改Nginx配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ vi &#x2F;etc&#x2F;nginx&#x2F;nginx.conf</span><br><span class="line"># events 区块是一个用来设置连接进程的区块，例如：设置Nginx的网络I&#x2F;O模型，以及连接数等。</span><br><span class="line">events   &#123; </span><br><span class="line">  use epoll;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>根据Nginx官方文档建议，也可以不指定事件处理模型，Nginx会自动选择最佳的事件处理模型服务。</p>
<h2 id="Nginx-最大连接数"><a href="#Nginx-最大连接数" class="headerlink" title="Nginx 最大连接数"></a>Nginx 最大连接数</h2><p>调整Nginx单个进程允许的客户端最大连接数，这个控制连接数的参数为work_connections，worker_connections的值要根据具体服务器性能和程序的内存使用量来指定（一个进程启动使用的内存根据程序确定）</p>
<p>worker_connections语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">synstax: worker_connections number</span><br><span class="line">default: worker_connections 512</span><br><span class="line">context: events</span><br></pre></td></tr></table></figure>

<p>实践修改Nginx配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ vi &#x2F;etc&#x2F;nginx&#x2F;nginx.conf</span><br><span class="line">events &#123;</span><br><span class="line">  worker_connections 20480;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>说明：<br>worker_connections用来设置一个worker process支持的最大并发连接数，这个连接数包括了所有链接，例如：代理服务器的连接，客户端的连接等，实际的并发连接数除了受worker_connections参数控制外，还和最大打开文件数worker_rlimit_nofile有关(见下文)，Nginx总并发连接=worker数量*worker_connections。</p>
<p>参考资料：<a href="http://nginx.org/en/docs/ngx_core_module.html">http://nginx.org/en/docs/ngx_core_module.html</a></p>
<h2 id="Nginx-文件描述符"><a href="#Nginx-文件描述符" class="headerlink" title="Nginx 文件描述符"></a>Nginx 文件描述符</h2><p>调整配置Nginx worker进程的最大打开文件数，这个控制连接数的参数为worker_rlimit_nofile。</p>
<p>worker_rlimit_nofile语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">synstax: worker_rlimit_nofile number</span><br><span class="line">default: 无</span><br><span class="line">context: main</span><br></pre></td></tr></table></figure>

<p>实践修改Nginx配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ vi &#x2F;etc&#x2F;nginx&#x2F;nginx.conf</span><br><span class="line"># 最大打开文件数，可设置为系统优化后的ulimit-HSn的结果</span><br><span class="line">worker_rlimit_nofile 65535;</span><br></pre></td></tr></table></figure>

<h2 id="Nginx高效传输"><a href="#Nginx高效传输" class="headerlink" title="Nginx高效传输"></a>Nginx高效传输</h2><h3 id="设置参数：sendfile-on"><a href="#设置参数：sendfile-on" class="headerlink" title="设置参数：sendfile on;"></a>设置参数：sendfile on;</h3><p>sendfile参数用于开启文件的高效传输模式。同时将tcp_nopush和tcp_nodelay两个指令设置为on，可防止网络及磁盘I/O阻塞，提升Nginx工作效率。</p>
<p>参数作用：激活或禁用sendfile()功能功能。sendfile()是作用于两个文件描述符之间的数据拷贝函数，这个拷贝操作是在内核之中的，被称为“零拷贝”，sendfile()比read和write函数要高效很多，因为，read和write函数要把数据拷贝到应用层再进行操作。相关控制参数还有sendfile_max_chunk。</p>
<p>参考资料：<a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#sendfile">http://nginx.org/en/docs/http/ngx_http_core_module.html#sendfile</a></p>
<h3 id="设置参数：tcp-nopush-on"><a href="#设置参数：tcp-nopush-on" class="headerlink" title="设置参数：tcp_nopush on;"></a>设置参数：tcp_nopush on;</h3><p>提高网络的“传输效率”</p>
<p>参数作用：激活或禁用Linux上的TCP_CORK socket选项，此选项仅仅当开启sendfile时才生效，激活这个tcp_nopush参数，数据包不会马上转发出去，而是等数据包最大时，一次性转发出去，有效解决网络堵塞。</p>
<p>参考资料：<a href="http://nginx.org/en/docs/http/ngx_http_core_module.html">http://nginx.org/en/docs/http/ngx_http_core_module.html</a></p>
<h3 id="设置参数：tcp-nodelay-on"><a href="#设置参数：tcp-nodelay-on" class="headerlink" title="设置参数：tcp_nodelay on;"></a>设置参数：tcp_nodelay on;</h3><p>在keeplive连接下，提高网络的传输“实时性”<br>用于激活tcp_ondelay功能，提高I/O性能。</p>
<p>参数作用：默认情况下当数据发送时，内核并不会马上发送，可能会等待更多的字节组成一个数据包，这样可以提高I/O性能。但是，在每次只发送很少字节的业务场景中，使用 tcp_nodelay功能能够让数据包立刻转发出去。</p>
<p>参数生效条件：<br>激活或禁用TCP_NODELAY选项，当一个连接进入keep-alive状态时生效。</p>
<p>参考资料：<a href="http://nginx.org/en/docs/http/ngx_http_core_module.html">http://nginx.org/en/docs/http/ngx_http_core_module.html</a></p>
<h1 id="Nginx连接参数优化"><a href="#Nginx连接参数优化" class="headerlink" title="Nginx连接参数优化"></a>Nginx连接参数优化</h1><h2 id="keepalive-timeout"><a href="#keepalive-timeout" class="headerlink" title="keepalive_timeout"></a>keepalive_timeout</h2><p>用于设置客户端连接保持会话的超时时间为60秒。超过这个时间，服务器会关闭该连接，此数值为参考值。</p>
<p>参数作用：keep-alive可以使客户端到服务器端已经建立的连接一直工作不退出，当服务器有持续请求时，keep-alive会使用已经建立的连接提供服务，从而避免服务器重新建立新连接处理请求。</p>
<p>此参数设置一个keep-alive(客户端连接在服务器端保持多久后退出)，其单位是秒，和HTTP响应header域的“Keep-Alive:timeout=time”参数有关，这些header信息也会被客户端浏览器识别并处理，不过有些客户端并不能按照服务器端的设置来处理，例如：MSIE大约60秒后会关闭keep-alive连接。</p>
<p>参考资料：<a href="http://nginx.org/en/docs/http/ngx_http_core_module.html">http://nginx.org/en/docs/http/ngx_http_core_module.html</a></p>
<h2 id="client-header-timeout"><a href="#client-header-timeout" class="headerlink" title="client_header_timeout"></a>client_header_timeout</h2><p>用于设置读取客户端请求头数据的超时时间。此处的数值15，其单位是秒，为经验参考值。</p>
<p>参数作用：设置读取客户端请求头数据的超时时间。如果超过这个时间，客户端还没有发送完整的header数据，服务器端将返回“Request time out (408)”错误，可指定一个超时时间，防止客户端利用http协议进行攻击。</p>
<p>参考资料：<a href="http://nginx.org/en/docs/http/ngx_http_core_module.html">http://nginx.org/en/docs/http/ngx_http_core_module.html</a></p>
<h2 id="client-body-timeout"><a href="#client-body-timeout" class="headerlink" title="client_body_timeout"></a>client_body_timeout</h2><p>用于设置读取客户端请求主体的超时时间，默认值60</p>
<p>参数作用：设置读取客户端请求主体的超时时间。这个超时仅仅为两次成功的读取操作之间的一个超时，非请求整个主体数据的超时时间，如果在这个超时时间内，客户端没有发送任何数据，Nginx将返回“Request time out(408)”错误，默认值60，</p>
<h2 id="send-timeout-25"><a href="#send-timeout-25" class="headerlink" title="send_timeout 25"></a>send_timeout 25</h2><p>用于指定响应客户端的超时时间。这个超时仅限于两个连接活动之间的时间，如果超过这个时间，客户端没有任何活动或者Nginx数据没有发送完，Nginx都将会关闭连接，默认值为60秒，可以改为参考值25秒。</p>
<p>参数作用：设置服务器端发送HTTP响应信息到客户端的超时时间，这个超时仅仅为两次成功握手后的一个超时，非请求整个响应数据的超时时间，如在这个超时时间内，客户端没有接收任何数据，连接将被关闭。</p>
<h1 id="Nginx上传文件大小限制"><a href="#Nginx上传文件大小限制" class="headerlink" title="Nginx上传文件大小限制"></a>Nginx上传文件大小限制</h1><p>参数作用：设置最大的允许的客户端请求主体大小，在请求头域有“Content-Length”，如果超过了此配置值，客户端会受到413错误，意思是请求的条目过大，有可能浏览器不能正确显示。设置为0表示禁止检查客户端请求主体大小。此参数对提高服务器端的安全性有一定作用。</p>
<h1 id="FastCGI调优"><a href="#FastCGI调优" class="headerlink" title="FastCGI调优"></a>FastCGI调优</h1><p>FastCGI参数是配合Nginx向后请求PHP动态引擎服务的相关参数。<br>FastCGI常见参数的Nginx配置示例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat &#x2F;etc&#x2F;nginx&#x2F;nginx.conf</span><br><span class="line">worker_processes  4;</span><br><span class="line">worker_cpu_affinity 0001 0010 0100 1000;</span><br><span class="line">worker_rlimit_nofile 65535;</span><br><span class="line">user nginx;</span><br><span class="line">events &#123;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  10240;</span><br><span class="line">&#125;</span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application&#x2F;octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line">    tcp_nopush      on;</span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line">    tcp_nodelay     on;</span><br><span class="line">    client_header_timeout 15;</span><br><span class="line">    client_body_timeout   15;</span><br><span class="line">    send_timeout          15;</span><br><span class="line">    server_tokens off;</span><br><span class="line">    fastcgi_connect_timeout 30s;        #Nginx允许fcgi连接超时时间</span><br><span class="line">    fastcgi_send_timeout 30s;           #Nginx允许fcgi返回数据的超时时间</span><br><span class="line">    fastcgi_read_timeout 30s;           #Nginx读取fcgi响应信息的超时时间</span><br><span class="line">    fastcgi_buffer_size 64k;            #Nginx读取响应信息的缓冲区大小</span><br><span class="line">    fastcgi_buffers 4 64k;              #指定Nginx缓冲区的数量和大小</span><br><span class="line">    fastcgi_busy_buffers_size 128k;     #当系统繁忙时buffer的大小</span><br><span class="line">    fastcgi_temp_file_write_size 128k;  #Nginx临时文件的大小</span><br><span class="line">    fastcgi_temp_path   &#x2F;data&#x2F;ngx_fcgi_tmp; #指定Nginx临时文件放置路径</span><br><span class="line">    fastcgi_cache_path  &#x2F;data&#x2F;ngx_fcgi_cache    levels&#x3D;2:2  keys_zone&#x3D;ngx_fcgi_cache:10m   inactive&#x3D;1d;    #指定Nginx缓存放置路径</span><br><span class="line">    </span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        location ~ .*\.(php|php5)?$ &#123;</span><br><span class="line">                root   html;</span><br><span class="line">        fastcgi_pass 127.0.0.1:9000;</span><br><span class="line">        fastcgi_index index.php;</span><br><span class="line">        include fastcgi.conf;</span><br><span class="line">        fastcgi_cache ngx_fcgi_cache;     #开启fcgi缓存并起名叫ngx_fcgi_cache,很重要，有效降低CPU负载，并且防止502错误发生。</span><br><span class="line">        fastcgi_cache_valid 200 302 1h; #指定应答代码的缓存时间，1h&#x3D;1小时</span><br><span class="line">        fastcgi_cache_valid 301 1d;     #1d&#x3D;1天</span><br><span class="line">        fastcgi_cache_valid any 1m;     #and 1m：将其他应答缓存1分钟</span><br><span class="line">        fastcgi_cache_min_uses 1;       #待缓存内容至少要被用户请求过1次</span><br><span class="line">        fastcgi_cache_use_stale error timeout invalid_header http_500;  #当遇到error，timeout，或者返回码500时，启用过期缓存返回用户（返回过期也比返回错误强）</span><br><span class="line">#       fastcgi_cache_key   http:&#x2F;&#x2F;$host$request_uri;   </span><br><span class="line">        </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h1 id="Nginx-gzip"><a href="#Nginx-gzip" class="headerlink" title="Nginx gzip"></a>Nginx gzip</h1><p>Nginx gzip压缩模块提供了压缩文件内容的功能，用户请求的内容在发送到用户客户端之前，Nginx服务器会根据一些具体的策略实施压缩，以节约网站出口带宽，同时加快数据传输效率，来提升用户访问体验。</p>
<p>Nginx gzip压缩的优点<br>1.提升网站用户体验：发送给用户的内容小了，用户访问单位大小的页面就加快了，用户体验提升了，网站口碑就好了。<br>2.节约网站带宽成本：数据是压缩传输的，因此节省了网站的带宽流量成本，不过压缩时会稍微消耗一些CPU资源，这个一般可以忽略。</p>
<p>对应的压缩参数说明如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#压缩配置</span><br><span class="line">gzip on;</span><br><span class="line">#&lt;&#x3D;&#x3D;开启gzip压缩功能</span><br><span class="line"></span><br><span class="line">gzip_min_length 1k;</span><br><span class="line">#&lt;&#x3D;&#x3D;设置允许压缩的页面最小字节数，页面字节数从header头的Content-Length中获取。默认值0，表示不管页面多大都进行压缩。建议设置成大于1K，如果小于1K可能会越压越大。</span><br><span class="line"></span><br><span class="line">gzip_buffers 4 16K;</span><br><span class="line">#&lt;&#x3D;&#x3D;压缩缓冲区大小。表示申请4个单位为16K的内存作为压缩结果流缓冲，默认值是申请与原始数据大小相同的内存空间来存储gzip压缩结果。</span><br><span class="line"></span><br><span class="line">gzip_http_version 1.1;</span><br><span class="line">#&lt;&#x3D;&#x3D;压缩版本（默认1.1，前端为squid2.5时使用1.0），用于设置识别HTTP协议版本，默认是1.1，目前大部分浏览器已经支持GZIP解压，使用默认即可。</span><br><span class="line"></span><br><span class="line">gzip_comp_level 2;</span><br><span class="line">#&lt;&#x3D;&#x3D;压缩比率。用来指定gzip压缩比，1压缩比最小，处理速度最快；9压缩比最大，传输速度快，但处理最慢，也比较消耗CPU资源。</span><br><span class="line"></span><br><span class="line">gzip_types text&#x2F;plain text&#x2F;css application&#x2F;x-javascript application&#x2F;xml application&#x2F;javascript application&#x2F;javascript  image&#x2F;jpeg image&#x2F;jpg image&#x2F;png;  </span><br><span class="line">#&lt;&#x3D;&#x3D;用来指定压缩的类型，“text&#x2F;html”类型总是会被压缩，这个就是HTTP原理部分讲的媒体类型。压缩类型可以在..&#x2F;conf&#x2F;mime.types文件中查看</span><br><span class="line"></span><br><span class="line">gzip_vary on;</span><br><span class="line">#&lt;&#x3D;&#x3D;vary header支持。该选项可以让前端的缓存服务器缓存经过gzip压缩的页面，例如用Squid缓存经过Nginx压缩的数据。</span><br></pre></td></tr></table></figure>

<h1 id="Nginx-Expires-缓存"><a href="#Nginx-Expires-缓存" class="headerlink" title="Nginx Expires 缓存"></a>Nginx Expires 缓存</h1><p>简单说，Nginx expires的功能就是为用户访问的网站内容设定一个过期时间，当用户第一次访问这些内容时，会把这些内容存储在用户浏览器本地，这样用户第二次及以后继续访问该网站时，浏览器会检查加载已经缓存在用户浏览器本地的内容，就不会去服务器请求了，当缓存的内容过期了会向源服务器发送请求，检查缓存内容是否被修改。</p>
<p>更深入的理解：<br>expires的功能就是允许通过Nginx配置文件控制HTTP的“Expires”和“Cache-Control”响应头部内容，告诉客户端浏览器是否缓存和缓存多久以内访问的内容。这个expires模块控制Nginx服务器应答时的expires头内容和Cache-Control头的max-age指令。缓存的有效期可以设置为相对于源文件的最后修改时刻或客户端的访问时刻。<br>这些HTTP头向客户端表明了额内容的有效性和持久性。如果客户端本地有内容缓存，则内容就可以从缓存而不是从服务器中读取，然后客户端会检查缓存中的副本，看其是否过期或失效，以决定是否重新从服务器获得内容更新。</p>
<h2 id="Nginx-expires配置详解"><a href="#Nginx-expires配置详解" class="headerlink" title="Nginx expires配置详解"></a>Nginx expires配置详解</h2><h3 id="根据文件扩展名进行判断，添加expires功能范例"><a href="#根据文件扩展名进行判断，添加expires功能范例" class="headerlink" title="根据文件扩展名进行判断，添加expires功能范例"></a>根据文件扩展名进行判断，添加expires功能范例</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location ~ .*\.(gif|jpg|jpeg|png|bmp|swf)$</span><br><span class="line">&#123;</span><br><span class="line">    expires 3650d;</span><br><span class="line">&#125;</span><br><span class="line">#该范例的意思是当用户访问网站URL结尾的文件扩展名为上述指定类型的图片时，设置缓存3650天，即10年。</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location ~ .*\.(js|css)$</span><br><span class="line"> &#123;</span><br><span class="line">    expires 30d;</span><br><span class="line"> &#125;</span><br><span class="line">#该范例的意思是当用户访问网站URL结尾的文件扩展名为js，css类型的元素时，设置缓存30天，即1个月。</span><br></pre></td></tr></table></figure>

<h3 id="根据URL中的路径（目录）进行判断，添加expires功能范例"><a href="#根据URL中的路径（目录）进行判断，添加expires功能范例" class="headerlink" title="根据URL中的路径（目录）进行判断，添加expires功能范例"></a>根据URL中的路径（目录）进行判断，添加expires功能范例</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location ~ ^&#x2F;(images|javascript|js|css|flash|media|static)&#x2F;</span><br><span class="line">&#123;</span><br><span class="line">    expires 360d;</span><br><span class="line">&#125;</span><br><span class="line">#该范例的意思是当用过户访问网站URL中包含上述路径（例：images，js，css，这些在服务器端是程序目录）时，把访问的内容设置缓存360天，即1年。</span><br></pre></td></tr></table></figure>

<p><strong>注意事项：expires只能在web节点上的nginx的配置文件里生效，如果配置在nginx方向代理的话，expires是不生效的</strong></p>
<h1 id="Nginx-日志文件安全"><a href="#Nginx-日志文件安全" class="headerlink" title="Nginx 日志文件安全"></a>Nginx 日志文件安全</h1><h2 id="不记录特定的访问日志"><a href="#不记录特定的访问日志" class="headerlink" title="不记录特定的访问日志"></a>不记录特定的访问日志</h2><p>在实际工作中，对于负载均衡器健康节点检查或某些特定文件（比如图片，JS，CSS）的日志，一般不需要记录下来，因为在统计PV时是按照页面计算的，而且日志写入太频繁会消耗大量磁盘I/O，降低服务的性能。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location ~ .*\.(js|jpg|JPG|jpeg|JPEG|css|bmp|gif|GIF)$ &#123;</span><br><span class="line">    access_log  off;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="访问日志的权限设置"><a href="#访问日志的权限设置" class="headerlink" title="访问日志的权限设置"></a>访问日志的权限设置</h2><p>假如日志目录为/app/logs,则授权方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chown -R root.root &#x2F;app&#x2F;logs</span><br><span class="line">chmod -R 600 &#x2F;app&#x2F;logs</span><br></pre></td></tr></table></figure>

<p>不需要在日志目录上给Nginx用户读或写许可，但很多网友都没注意这个问题，他们把该权限直接给了Nginx或Apache用户，这就成为了安全隐患。</p>
<h1 id="Nginx-访问控制"><a href="#Nginx-访问控制" class="headerlink" title="Nginx 访问控制"></a>Nginx 访问控制</h1><h2 id="根据扩展名限制程序和文件访问"><a href="#根据扩展名限制程序和文件访问" class="headerlink" title="根据扩展名限制程序和文件访问"></a>根据扩展名限制程序和文件访问</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location ~ ^&#x2F;images&#x2F;.*\.(php|php5|sh|pl|py)$ &#123;</span><br><span class="line">    deny all;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~ ^&#x2F;static&#x2F;.*\.(php|php5|sh|pl|py)$	&#123;</span><br><span class="line">    deny all;</span><br><span class="line">&#125;</span><br><span class="line">location ~* ^&#x2F;data&#x2F;(attachment|avatar)&#x2F;.*\.(php|php5)$ &#123;</span><br><span class="line">    deny all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对上述目录的限制必须写在Nginx处理PHP服务配置的前面，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location ~ .*\.(php|php5)$</span><br><span class="line">&#123;</span><br><span class="line">    fastcgi_pass 127.0.0.1:9000;</span><br><span class="line">    fastcgi_index index.php;</span><br><span class="line">    include fcgi.conf;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Nginx下配置禁止访问.txt和.doc文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location ~* \.(txt|doc)$</span><br><span class="line">&#123;</span><br><span class="line">    if (-f $request_filename)</span><br><span class="line">    &#123;</span><br><span class="line">        root &#x2F;data&#x2F;www&#x2F;www;</span><br><span class="line">        #rewrite ...可以重定向到某个URL</span><br><span class="line">        break；</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">location ~* \.(txt|doc)$</span><br><span class="line">&#123;</span><br><span class="line">    root &#x2F;data&#x2F;www&#x2F;www;</span><br><span class="line">    deny all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="禁止访问指定的目录"><a href="#禁止访问指定的目录" class="headerlink" title="禁止访问指定的目录"></a>禁止访问指定的目录</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#禁止访问单个目录的命令如下：</span><br><span class="line">location ~ ^&#x2F;static  &#123;</span><br><span class="line">    deny all;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#禁止访问多个目录的命令如下：</span><br><span class="line">location ~ ^&#x2F;(static|js)  &#123;</span><br><span class="line">    deny all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#禁止访问目录并返回指定的HTTP状态码</span><br><span class="line">server  &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name www.youngboy.com youngboy.com;</span><br><span class="line">    root &#x2F;data&#x2F;www&#x2F;www;</span><br><span class="line">    index index.html index.htm;</span><br><span class="line">    access_log logs&#x2F;www_access.log commonlog;</span><br><span class="line">    location &#x2F;admin&#x2F; &#123;return 404;&#125;</span><br><span class="line">    location &#x2F;tmplates&#x2F; &#123;return 403;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Nginx-限制来源IP访问"><a href="#Nginx-限制来源IP访问" class="headerlink" title="Nginx 限制来源IP访问"></a>Nginx 限制来源IP访问</h1><p>使用ngx_http_access_module限制网站来源IP访问</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 禁止某目录让外界访问，但允许某IP访问该目录，且支持PHP解析，命令如下：</span><br><span class="line">location ~ ^&#x2F;yunjisuan&#x2F; &#123;</span><br><span class="line">    allow 202.111.12.211;</span><br><span class="line">    deny all;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location ~ .*\.(php|php5)$ &#123;</span><br><span class="line">    fastcgi_pass 127.0.0.1:9000;</span><br><span class="line">    fastcgi_index   index.php;</span><br><span class="line">    include         fastcgi.conf;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 限制指定IP或IP段访问</span><br><span class="line">location &#x2F; &#123;</span><br><span class="line">    deny  192.168.1.1;</span><br><span class="line">    allow 192.168.1.0&#x2F;24;</span><br><span class="line">    allow 10.1.1.0&#x2F;16;</span><br><span class="line">    deny  all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Nginx做反向代理的时候可以限制客户端IP</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#方法一：使用if来控制，命令如下：</span><br><span class="line">if ($remote_addr &#x3D; 10.0.0.7) &#123;</span><br><span class="line">    return 403;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if ($remote_addr &#x3D; 218.247.17.130) &#123;</span><br><span class="line">    set $allow_access_root &#39;ture&#39;;     </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#方法二：利用deny和allow只允许IP访问，命令如下：</span><br><span class="line">location &#x2F; &#123;</span><br><span class="line">    root html&#x2F;blog;</span><br><span class="line">    index index.php index.html index.htm;</span><br><span class="line">    allow 10.0.0.7;</span><br><span class="line">    deny all;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#方法三：只拒绝某些IP访问，命令如下：</span><br><span class="line">location &#x2F; &#123;</span><br><span class="line">    root html&#x2F;blog;</span><br><span class="line">    index index.php index.html index.htm;</span><br><span class="line">    deny 10.0.0.7;</span><br><span class="line">    allow all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意事项：<br>1.deny一定要加一个IP，否则会直接跳转到403，不再往下执行了，如果403默认页是在同一域名下，会造成死循环访问。<br>2.对于allow的IP段，从允许访问的段位从小到大排列，如127.0.0.0/24的下面才能是10.10.0.0/16，其中：<br>1）24表示子网掩码：255.255.255.0<br>2）16表示子网掩码：255.255.0.0<br>3）8表示子网掩码：255.0.0.0<br>3.以deny all:结尾，表示除了上面允许的，其他的都禁止。如：<br>1）deny 192.168.1.1;<br>2）allow 127.0.0.0/24;<br>3）allow 192.168.0.0/16;<br>4）allow 10.10.0.0/16;<br>5）deny all;</p>
<h1 id="Nginx-禁止非法域名解析"><a href="#Nginx-禁止非法域名解析" class="headerlink" title="Nginx 禁止非法域名解析"></a>Nginx 禁止非法域名解析</h1><p>Nginx如何防止用户IP访问网站（恶意域名解析，也相当于是直接IP访问企业网站）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 方法一</span><br><span class="line"># 让使用IP访问网站的用户，或者恶意解析域名的用过户，收到501错误</span><br><span class="line"># 直接用新的server标签</span><br><span class="line">server &#123;</span><br><span class="line">    listen 80 default_server;</span><br><span class="line">    server_name _;</span><br><span class="line">    return 501;</span><br><span class="line">&#125;</span><br><span class="line"># 说明：直接报501错误，从用户体验上不是很好</span><br><span class="line"></span><br><span class="line"># 方法二</span><br><span class="line"># 通过301跳转到主页</span><br><span class="line"># 当输入IP地址访问的时候会自动跳转到域名</span><br><span class="line">server &#123;</span><br><span class="line">    listen 80 default_server;</span><br><span class="line">    server_name _;</span><br><span class="line">    rewrite ^(.*) http:&#x2F;&#x2F;xxx.com&#x2F;$1 permanent;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 方法三</span><br><span class="line"># 发现某域名恶意解析到公司的服务器IP，在server标签里添加以下代码即可，若有多个server则要多处添加</span><br><span class="line">if ($host !~ ^xxx.com$) &#123;</span><br><span class="line">        rewrite ^(.*) http:&#x2F;&#x2F;xxx.com&#x2F;$1 permanent;</span><br><span class="line">                &#125;</span><br><span class="line"># 说明：代码含义为如果header信息的host主机名字非xxx.com，就301跳转到xxx.com</span><br></pre></td></tr></table></figure>

<h1 id="Nginx-防爬虫"><a href="#Nginx-防爬虫" class="headerlink" title="Nginx 防爬虫"></a>Nginx 防爬虫</h1><p>我们可以根据客户端的user-agents信息，轻松地阻止指定的爬虫爬取我们的网站。下面来看几个案例。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 阻止下载协议代理</span><br><span class="line"># Block download agents##</span><br><span class="line">if ($http_user_agent ~* LWP:Simple|BBBike|wget) &#123;</span><br><span class="line">    return 403;</span><br><span class="line">&#125;</span><br><span class="line">#如果用户匹配了if后面的客户端（例如wget），就返回403.</span><br><span class="line">#这里根据$http_user_agent获取客户端agent，然后判断是否允许或返回指定错误码。</span><br><span class="line"></span><br><span class="line"># 添加内容防止N多爬虫代理访问网站</span><br><span class="line">#这些爬虫代理使用“|”分隔，具体要处理的爬虫可以根据需求增加或减少，添加的内容如下：</span><br><span class="line">if ($http_user_agent ~* &quot;qihoobot|Baiduspider|Googlebot-Modile|Googlebot-Image|Mediapartners-Google|Adsbot-Google|Yahoo! SSlurp  China|YoudaoBot|Sosospider|Sogou spider|Sogou web spider|MSNBot&quot;)&#123;</span><br><span class="line">    return 403;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 测试禁止不同的浏览器软件访问</span><br><span class="line">if ($http_user_agent ~* &quot;Firefox|MSIE&quot;) &#123;</span><br><span class="line">    rewrite ^(.*) http:&#x2F;&#x2F;xxx.com&#x2F;$1 permanent;</span><br><span class="line">&#125;</span><br><span class="line">#如果浏览器为Firefox或IE，就会跳转到http:&#x2F;&#x2F;xxx.com</span><br></pre></td></tr></table></figure>

<h1 id="Nginx-限制HTTP请求方法"><a href="#Nginx-限制HTTP请求方法" class="headerlink" title="Nginx 限制HTTP请求方法"></a>Nginx 限制HTTP请求方法</h1><p>最常用的HTTP方法为GET，POST，我们可以通过Nginx限制HTTP请求的方法来达到提升服务器安全的目的，例如，让HTTP只能使用GET，HEAD和POST方法的配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#Only allow these request methods</span><br><span class="line">if ($request_method !~ ^(GET|HEAD|POST)$) &#123;</span><br><span class="line">    return 501;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#命令解释:</span><br><span class="line">($request_method !~ ^(GET|HEAD|POST)$)  #&lt;&#x3D;&#x3D;如果匹配的http请求方法不是get|head|post就拒绝掉</span><br></pre></td></tr></table></figure>

<p>当上传服务器上传数据到存储服务器时，用户上传写入的目录就不得不给Nginx对应的用户相关权限，这样一旦程序有漏洞，木马就有可能被上传到服务器挂载的对应存储服务器的目录里，虽然我们也做了禁止PHP，SH，PL，PY等扩展名的解析限制，但还是会遗漏一些想不到的可执行文件。对于这样情况，该怎么办呢？事实上，还可以通过限制上传服务器的Web服务（可以具体到文件）使用GET方法，防止用户通过上传服务器访问存储内容，让访问存储渠道只能从静态或图片服务器入口进入。</p>
<p>例如，在上传服务器上限制HTTP的GET方法的配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#Only deny GET request methods ##</span><br><span class="line">if ($request_method ~* ^(GET)$) &#123;</span><br><span class="line">    return 501;</span><br><span class="line">&#125;</span><br><span class="line">#还可以加一层location，更具体地限制文件名</span><br></pre></td></tr></table></figure>

<h1 id="Nginx-连接限制"><a href="#Nginx-连接限制" class="headerlink" title="Nginx 连接限制"></a>Nginx 连接限制</h1><p>连接频率限制 limit_conn_module<br>请求频率限制 limit_req_module</p>
<h2 id="Nginx连接限制配置"><a href="#Nginx连接限制配置" class="headerlink" title="Nginx连接限制配置"></a>Nginx连接限制配置</h2><p>具体配置如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http &#123;  #http段配置连接限制, 同⼀时刻只允许⼀个客户端IP连接 </span><br><span class="line">limit_conn_zone $binary_remote_addr zone&#x3D;conn_zone:10m;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">location &#x2F; &#123;</span><br><span class="line">#同⼀时刻只允许⼀个客户端IP连接 </span><br><span class="line">limit_conn conn_zone 1; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#压⼒测试 yum install -y httpd-tools ab -n 50 -c 20 http:&#x2F;&#x2F;127.0.0.1&#x2F;index.html</span><br></pre></td></tr></table></figure>

<h2 id="Nginx-请求限制配置"><a href="#Nginx-请求限制配置" class="headerlink" title="Nginx 请求限制配置"></a>Nginx 请求限制配置</h2><p>具体配置如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http &#123;  ##http段配置请求限制, rate限制速率，限制⼀秒钟最多⼀个IP请求 </span><br><span class="line">limit_req_zone $binary_remote_addr zone&#x3D;req_zone:10m rate&#x3D;1r&#x2F;s;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">location &#x2F; &#123; </span><br><span class="line">#1r&#x2F;s只接收⼀个请求,其余请求拒绝处理并返回错误码给客户端</span><br><span class="line"></span><br><span class="line">limit_req zone&#x3D;req_zone; </span><br><span class="line">#请求超过1r&#x2F;s,剩下的将被延迟处理,请求数超过burst定义的数量, 多余的请求返回503</span><br><span class="line"></span><br><span class="line">#limit_req zone&#x3D;req_zone burst&#x3D;3 nodelay; &#125;</span><br><span class="line"></span><br><span class="line">#压⼒测试 yum install -y httpd-tools ab -n 50 -c 20 http:&#x2F;&#x2F;127.0.0.1&#x2F;index.html</span><br></pre></td></tr></table></figure>







]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos7 Oracle11.2.0.4安装步骤</title>
    <url>/posts/b62c4623.html</url>
    <content><![CDATA[<h1 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h1><p>在开始安装前先要检查下相应的依赖包，这里要注意的是如果是在X64的系统上安装记得需要安装i686即X64系统上的X86的包,这点很重要,如果漏了Oracle是无法正常的安装的，这里pdksh没有安装没有关系，在这里已经安装了ksh就可以了</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# yum -y install binutils compat-libstdc++-33 elfutils-libelf elfutils-libelf-devel glibc glibc-common glibc-devel gcc gcc-c++ libaio-devel libaio libgcc libstdc++ libstdc++-devel make sysstat unixODBC unixODBC-devel pdksh ksh libaio.i686 glibc.i686 compat-libstdc++-33.i686 libaio-devel.i686 libgcc.i686 libstdc++.i686 unixODBC.i686 unixODBC-devel.i686 compat-libcap1</span><br></pre></td></tr></table></figure>
<h2 id="创建oracle用户、组"><a href="#创建oracle用户、组" class="headerlink" title="创建oracle用户、组"></a>创建oracle用户、组</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# groupadd oinstall </span><br><span class="line">[root@localhost ~]# groupadd dba </span><br><span class="line">[root@localhost ~]# useradd -g oinstall -G dba oracle </span><br><span class="line">[root@localhost ~]# echo oracle:123456 |chpasswd</span><br></pre></td></tr></table></figure>

<h2 id="配置系统的limits的配置文件"><a href="#配置系统的limits的配置文件" class="headerlink" title="配置系统的limits的配置文件"></a>配置系统的limits的配置文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# cat &gt;&gt; &#x2F;etc&#x2F;security&#x2F;limits.conf &lt;&lt;EOF</span><br><span class="line">oracle soft nproc 2047</span><br><span class="line">oracle hard nproc 16384</span><br><span class="line">oracle soft nofile 1024</span><br><span class="line">oracle hard nofile 65536</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="修改内核变量配置文件"><a href="#修改内核变量配置文件" class="headerlink" title="修改内核变量配置文件"></a>修改内核变量配置文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# cat &gt;&gt; &#x2F;etc&#x2F;sysctl.conf &lt;&lt;EOF</span><br><span class="line">kernel.shmmni &#x3D; 4096</span><br><span class="line">kernel.shmall &#x3D; 2097152</span><br><span class="line">kernel.shmmax &#x3D; 2147483648</span><br><span class="line">kernel.sem &#x3D; 250 32000 100 128</span><br><span class="line">fs.file-max &#x3D; 6815744</span><br><span class="line">net.ipv4.ip_local_port_range &#x3D; 9000 65500</span><br><span class="line">net.core.rmem_default &#x3D; 262144</span><br><span class="line">net.core.rmem_max &#x3D; 4194304</span><br><span class="line">net.core.wmem_default&#x3D; 262144</span><br><span class="line">net.core.wmem_max &#x3D; 1048576</span><br><span class="line">fs.aio-max-nr &#x3D; 1048576</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# sysctl -p</span><br></pre></td></tr></table></figure>

<h2 id="配置Oracle用户的环境变量配置文件"><a href="#配置Oracle用户的环境变量配置文件" class="headerlink" title="配置Oracle用户的环境变量配置文件"></a>配置Oracle用户的环境变量配置文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# su - oracle </span><br><span class="line">[oracle@localhost ~]$ cat &gt;&gt; &#x2F;home&#x2F;oracle&#x2F;.bash_profile &lt;&lt;EOF</span><br><span class="line"></span><br><span class="line">export ORACLE_BASE&#x3D;&#x2F;app&#x2F;oracle11g</span><br><span class="line">export ORACLE_HOME&#x3D;&#x2F;app&#x2F;oracle11g&#x2F;product&#x2F;11.2.0&#x2F;dbhome_1</span><br><span class="line">export ORACLE_SID&#x3D;orcl</span><br><span class="line">export PATH&#x3D;$PATH:$ORACLE_HOME&#x2F;bin</span><br><span class="line">#NLS_LANG&#x3D;AMERICAN_CHINA.ZHS16GBK</span><br><span class="line">#export ORACLE_UNQNAME&#x3D;orcl</span><br><span class="line">#export LD_LIBRARY_PATH&#x3D;$ORACLE_HOME&#x2F;lib:&#x2F;usr&#x2F;lib</span><br><span class="line">#export CLASSPATH&#x3D;$ORACLE_HOME&#x2F;jlib:$ORACLE_HOME&#x2F;rdbms&#x2F;jlib</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">以下配置需要另外安装rlwrap</span><br><span class="line">alias sqlplus&#x3D;&#39;rlwrap sqlplus&#39;</span><br><span class="line">alias rman&#x3D;&#39;rlwrap rman&#39;</span><br></pre></td></tr></table></figure>

<h2 id="安装Oracle"><a href="#安装Oracle" class="headerlink" title="安装Oracle"></a>安装Oracle</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# mkdir -p &#x2F;app&#x2F;&#123;oracle11g,oraInventory&#125;</span><br><span class="line">[root@localhost ~]# chown -R oracle:oinstall &#x2F;app </span><br><span class="line">[root@localhost ~]#su - oracle</span><br><span class="line">[oracle@localhost database]$ .&#x2F;runInstaller</span><br><span class="line"></span><br><span class="line">如启动安装界面失败</span><br><span class="line">切换root账号</span><br><span class="line">执行命令</span><br><span class="line">[root@localhost ~] xhost +</span><br></pre></td></tr></table></figure>

<p><strong>如swap不足</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@oracle ~]#cd &#x2F;tmp &amp;&amp;  dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;swap bs&#x3D;1M count&#x3D;2048</span><br><span class="line">[root@oracle tmp]# mkswap &#x2F;tmp&#x2F;swap -f</span><br><span class="line">[root@oracle tmp]# swapon &#x2F;tmp&#x2F;swap</span><br></pre></td></tr></table></figure>
<p><strong>安装完成后</strong><br>管理界面是ip:1158/em </p>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx根据指定IP转发链接</title>
    <url>/posts/4f490c84.html</url>
    <content><![CDATA[<p><strong>需求</strong><br>因项目变更，外网官网跳转到维护界面，指定ip地址能够正常访问</p>
<a id="more"></a>

<p><strong>第一种情形：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#可以作为nginx的停服更新使用，仅允许172.16.2.80访问,其他IP都rewrite到停服页面</span><br><span class="line"></span><br><span class="line">set $flag 0;</span><br><span class="line"></span><br><span class="line">                if ( $remote_addr ~* ^172\.16\.2\.80 ) &#123;</span><br><span class="line">                        set $flag &quot;$&#123;flag&#125;1&quot;;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                if ($flag !&#x3D; &quot;01&quot;) &#123;</span><br><span class="line">                   return 301 https:&#x2F;&#x2F;172.16.2.20&#x2F;weihu.html;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 如前段非nginx，可使用http_x_forwarded_for</span><br></pre></td></tr></table></figure>

<p><strong>第二种情形</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#访问某个php应用的时候我只想让内部的某个IP访问，其他的IP都转到另一个PHP上</span><br><span class="line">访问test.php，且IP不等172.16.2.80的跳转到weihu.php:</span><br><span class="line">set $test &#39;&#39;;</span><br><span class="line">if ( $request_uri ~* &#x2F;img&#x2F;test.php ) &#123;</span><br><span class="line">        set $test P;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if ( $http_x_forwarded_for !~* ^172\.16\.2\.80.* ) &#123;</span><br><span class="line">        set $test &quot;$&#123;test&#125;C&quot;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#当条件符合 访问test.php并且 ip不是172.16.2.80的转发到weihu.php</span><br><span class="line">if ( $test &#x3D; PC ) &#123; </span><br><span class="line">	   rewrite ^(.*)$ &#x2F;img&#x2F;weihu.php permanent;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>rewrite</tag>
      </tags>
  </entry>
  <entry>
    <title>ORA-00980同义词转换不再有效</title>
    <url>/posts/e366de51.html</url>
    <content><![CDATA[<p><strong>问题：</strong><br>ORA-00980: 同义词转换不再有效</p>
<p><strong>解决方法：</strong><br>使用拥有dba权限的账号sys的登录。<br>执行查找所有失效的同义词，生成删除脚本如下：</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--复制需要删除的同义词</span><br><span class="line">select &#39;drop &#39;   </span><br><span class="line">       || decode(s.owner,</span><br><span class="line">             &#39;PUBLIC&#39;,           </span><br><span class="line">             &#39;public synonym &#39;, </span><br><span class="line">             &#39;synonym &#39; || s.owner || &#39;.&#39;)</span><br><span class="line">       || s.synonym_name</span><br><span class="line">       || &#39;;&#39; as &quot;Dropping invalid synonyms:&quot;</span><br><span class="line">  from dba_synonyms s</span><br><span class="line"> where table_owner not in (&#39;SYSTEM&#39;, &#39;SYS&#39;)    </span><br><span class="line">   and db_link is null  </span><br><span class="line">   and not exists</span><br><span class="line"> (select null    </span><br><span class="line">          from dba_objects o</span><br><span class="line">         where s.table_owner &#x3D; o.owner     </span><br><span class="line">           and s.table_name &#x3D; o.object_name);</span><br></pre></td></tr></table></figure>

<p>处理办法删除重建：<br>SQL&gt;Drop public synonym T_INVEST_ZS;<br>SQL&gt;Create public synonym T_INVEST_ZS; for 用户名.T_INVEST_ZS;</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--复制需要创建的同义词</span><br><span class="line">select &#39;create &#39;   </span><br><span class="line">       || decode(s.owner,</span><br><span class="line">             &#39;PUBLIC&#39;,           </span><br><span class="line">             &#39;public synonym &#39;, </span><br><span class="line">             &#39;synonym &#39; </span><br><span class="line">       || s.owner || &#39;.&#39;)</span><br><span class="line">       || s.synonym_name</span><br><span class="line">       || &#39; for&#39;</span><br><span class="line">       || &#39; 用户名&#39; </span><br><span class="line">       || &#39;.&#39;</span><br><span class="line">       || s.synonym_name</span><br><span class="line">       || &#39;;&#39; as &quot;Dropping invalid synonyms:&quot;</span><br><span class="line">  from dba_synonyms s</span><br><span class="line"> where table_owner not in (&#39;SYSTEM&#39;, &#39;SYS&#39;)    </span><br><span class="line">   and db_link is null  </span><br><span class="line">   and not exists</span><br><span class="line"> (select null    </span><br><span class="line">          from dba_objects o</span><br><span class="line">         where s.table_owner &#x3D; o.owner     </span><br><span class="line">           and s.table_name &#x3D; o.object_name);</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title>Oracle 11g2 DataGuard简单部署</title>
    <url>/posts/17302f63.html</url>
    <content><![CDATA[<p>环境：<br>centos 7<br>redhat 6.4<br>oracle 11.2.0.4</p>
<a id="more"></a>
<p>172.16.2.31 primary<br>172.16.2.32 standby<br>172.16.2.19 testdb</p>
<h1 id="修改主库静态监听"><a href="#修改主库静态监听" class="headerlink" title="修改主库静态监听"></a>修改主库静态监听</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim listener.ora</span><br><span class="line"></span><br><span class="line">LISTENER &#x3D;</span><br><span class="line">  (DESCRIPTION_LIST &#x3D;</span><br><span class="line">    (DESCRIPTION &#x3D;</span><br><span class="line">      (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; primary)(PORT &#x3D; 1521))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">SID_LIST_LISTENER&#x3D;</span><br><span class="line">  (SID_LIST&#x3D;</span><br><span class="line">    (SID_DESC&#x3D;</span><br><span class="line">       (GLOBAL_DBNAME&#x3D;primary)</span><br><span class="line">       (SID_NAME&#x3D;orcl)</span><br><span class="line">     )</span><br><span class="line">  )</span><br><span class="line">ADR_BASE_LISTENER &#x3D; &#x2F;app&#x2F;oracle11g</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim tnsnames.ora</span><br><span class="line"></span><br><span class="line">PRIMARY &#x3D;</span><br><span class="line">  (DESCRIPTION &#x3D;</span><br><span class="line">    (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; primary)(PORT &#x3D; 1521))</span><br><span class="line">    (CONNECT_DATA &#x3D;</span><br><span class="line">      (SERVER &#x3D; DEDICATED)</span><br><span class="line">      (SERVICE_NAME &#x3D; primary)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">STANDBY &#x3D;</span><br><span class="line">  (DESCRIPTION &#x3D;</span><br><span class="line">    (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; standby)(PORT &#x3D; 1521))</span><br><span class="line">    (CONNECT_DATA &#x3D;</span><br><span class="line">      (SERVER &#x3D; DEDICATED)</span><br><span class="line">      (SERVICE_NAME &#x3D; standby)</span><br><span class="line">    )</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>
<h1 id="修改主库初始化参数文件"><a href="#修改主库初始化参数文件" class="headerlink" title="修改主库初始化参数文件"></a>修改主库初始化参数文件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">startup mount;</span><br><span class="line">alter database archivelog;</span><br><span class="line">alter database force logging;</span><br><span class="line">alter database open;</span><br><span class="line">alter system set db_unique_name&#x3D;primary scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_config &#x3D; &#39;DG_CONFIG&#x3D;(primary,standby)&#39; scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_dest_1 &#x3D; &#39;LOCATION&#x3D;&#x2F;app&#x2F;oracle11g&#x2F;archivelog valid_for&#x3D;(all_logfiles,all_roles) db_unique_name&#x3D;primary&#39; scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_dest_2 &#x3D; &#39;SERVICE&#x3D;standby lgwr sync valid_for&#x3D;(online_logfiles,primary_role) db_unique_name&#x3D;standby&#39; scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_dest_state_1 &#x3D; ENABLE;</span><br><span class="line">alter system set log_archive_dest_state_2 &#x3D; ENABLE;</span><br><span class="line">alter system set fal_server &#x3D; standby scope&#x3D;spfile;</span><br><span class="line">alter system set fal_client &#x3D; primary scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_max_processes&#x3D;5 scope&#x3D;spfile;</span><br><span class="line">alter system set standby_file_management&#x3D;AUTO scope&#x3D;spfile;</span><br><span class="line">create pfile from spfile;</span><br></pre></td></tr></table></figure>

<h1 id="在主库pfile参数文件和密码文件，并且拷贝到备库相应位置"><a href="#在主库pfile参数文件和密码文件，并且拷贝到备库相应位置" class="headerlink" title="在主库pfile参数文件和密码文件，并且拷贝到备库相应位置"></a>在主库pfile参数文件和密码文件，并且拷贝到备库相应位置</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd $ORACLE_HOME&#x2F;dbs</span><br><span class="line">scp initorcl.ora orapworcl oracle@172.16.2.32:&#x2F;$ORACLE_HOME&#x2F;dbs</span><br></pre></td></tr></table></figure>

<h1 id="修改备库静态监听"><a href="#修改备库静态监听" class="headerlink" title="修改备库静态监听"></a>修改备库静态监听</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim listener.ora</span><br><span class="line"></span><br><span class="line">LISTENER &#x3D;</span><br><span class="line">  (DESCRIPTION_LIST &#x3D;</span><br><span class="line">    (DESCRIPTION &#x3D;</span><br><span class="line">      (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; standby)(PORT &#x3D; 1521))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">SID_LIST_LISTENER&#x3D;</span><br><span class="line">  (SID_LIST&#x3D;</span><br><span class="line">    (SID_DESC&#x3D;</span><br><span class="line">       (GLOBAL_DBNAME&#x3D;standby)</span><br><span class="line">       (SID_NAME&#x3D;orcl)</span><br><span class="line">     )</span><br><span class="line">  )</span><br><span class="line">ADR_BASE_LISTENER &#x3D; &#x2F;app&#x2F;oracle11g</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim tnsnames.ora</span><br><span class="line"></span><br><span class="line">PRIMARY &#x3D;</span><br><span class="line">  (DESCRIPTION &#x3D;</span><br><span class="line">    (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; primary)(PORT &#x3D; 1521))</span><br><span class="line">    (CONNECT_DATA &#x3D;</span><br><span class="line">      (SERVER &#x3D; DEDICATED)</span><br><span class="line">      (SERVICE_NAME &#x3D; primary)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">STANDBY &#x3D;</span><br><span class="line">  (DESCRIPTION &#x3D;</span><br><span class="line">    (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; standby)(PORT &#x3D; 1521))</span><br><span class="line">    (CONNECT_DATA &#x3D;</span><br><span class="line">      (SERVER &#x3D; DEDICATED)</span><br><span class="line">      (SERVICE_NAME &#x3D; standby)</span><br><span class="line">    )</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>
<h1 id="修改备库初始化参数文件"><a href="#修改备库初始化参数文件" class="headerlink" title="修改备库初始化参数文件"></a>修改备库初始化参数文件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;app&#x2F;oracle11g&#x2F;fast_recovery_area&#x2F;orcl</span><br><span class="line">mkdir -p &#x2F;app&#x2F;oracle11g&#x2F;admin&#x2F;orcl&#x2F;adump</span><br><span class="line">mkdir -p &#x2F;app&#x2F;oracle11g&#x2F;&#123;standbylog,archivelog&#125;</span><br><span class="line">mkdir -p &#x2F;app&#x2F;oracle11g&#x2F;oradata&#x2F;orcl</span><br><span class="line"></span><br><span class="line">startup nomount</span><br><span class="line">create spfile from pfile&#x3D;&#39;&#x2F;app&#x2F;oracle11g&#x2F;product&#x2F;11.2.0&#x2F;dbhome_1&#x2F;dbs&#x2F;initorcl.ora&#39;;</span><br><span class="line">shutdown abort;</span><br><span class="line">alter system set db_unique_name&#x3D;standby scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_config&#x3D;&#39;DG_CONFIG&#x3D;(primary,standby)&#39; scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_dest_1 &#x3D; &#39;LOCATION&#x3D;&#x2F;app&#x2F;oracle11g&#x2F;archivelog valid_for&#x3D;(all_logfiles,all_roles) db_unique_name&#x3D;standby&#39; scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_dest_2 &#x3D; &#39;SERVICE&#x3D;primary lgwr sync valid_for&#x3D;(online_logfiles,primary_role) db_unique_name&#x3D;primary&#39; scope&#x3D;spfile;</span><br><span class="line">alter system set fal_server&#x3D;primary scope&#x3D;spfile;</span><br><span class="line">alter system set fal_client&#x3D;standby scope&#x3D;spfile;</span><br><span class="line">shutdown abort;</span><br><span class="line">startup nomount;</span><br></pre></td></tr></table></figure>
<h1 id="测试主库备库是否tnsping正常"><a href="#测试主库备库是否tnsping正常" class="headerlink" title="测试主库备库是否tnsping正常"></a>测试主库备库是否tnsping正常</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqlplus sys&#x2F;123456@primary as sysdba</span><br><span class="line">sqlplus sys&#x2F;123456@standby as sysdba </span><br><span class="line">tnsping standby</span><br><span class="line">tnsping primary</span><br></pre></td></tr></table></figure>

<h1 id="在主库通过Rman-Duplicate创建备库"><a href="#在主库通过Rman-Duplicate创建备库" class="headerlink" title="在主库通过Rman Duplicate创建备库"></a>在主库通过Rman Duplicate创建备库</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rman target sys&#x2F;123456@primary auxiliary sys&#x2F;123456@standby nocatalog</span><br><span class="line">duplicate target database for standby from active database nofilenamecheck;</span><br></pre></td></tr></table></figure>

<h1 id="在主库和备库添加standby日志"><a href="#在主库和备库添加standby日志" class="headerlink" title="在主库和备库添加standby日志"></a>在主库和备库添加standby日志</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter database add standby logfile group 4 &#39;&#x2F;app&#x2F;oracle11g&#x2F;standbylog&#x2F;standby_redo04.log&#39; size 50m;</span><br><span class="line">alter database add standby logfile group 5 &#39;&#x2F;app&#x2F;oracle11g&#x2F;standbylog&#x2F;standby_redo05.log&#39; size 50m;</span><br><span class="line">alter database add standby logfile group 6 &#39;&#x2F;app&#x2F;oracle11g&#x2F;standbylog&#x2F;standby_redo06.log&#39; size 50m;</span><br><span class="line">alter database add standby logfile group 7 &#39;&#x2F;app&#x2F;oracle11g&#x2F;standbylog&#x2F;standby_redo07.log&#39; size 50m;</span><br></pre></td></tr></table></figure>

<h1 id="在备库开启实时日志应用"><a href="#在备库开启实时日志应用" class="headerlink" title="在备库开启实时日志应用"></a>在备库开启实时日志应用</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SQL&gt;recover managed standby database using current logfile disconnect from session;</span><br></pre></td></tr></table></figure>

<p><strong>取消备库实时日志</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter database recover managed standby database cancel;</span><br></pre></td></tr></table></figure>

<p><strong>查看备库状态</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select group#,thread#,sequence#,archived,status from v$standby_log</span><br><span class="line">select open_mode,database_role,db_unique_name from v$database;</span><br></pre></td></tr></table></figure>
<p><strong>查看standby启动进程状态</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select process,client_process,sequence#,status from v$managed_standby;</span><br></pre></td></tr></table></figure>
<p><strong>查看数据库保护模式</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select database_role,protection_mode,protection_level,open_mode from v$database;</span><br></pre></td></tr></table></figure>



<h1 id="新增DG"><a href="#新增DG" class="headerlink" title="新增DG"></a>新增DG</h1><h1 id="在主库pfile参数文件和密码文件，并且拷贝到备库相应位置-1"><a href="#在主库pfile参数文件和密码文件，并且拷贝到备库相应位置-1" class="headerlink" title="在主库pfile参数文件和密码文件，并且拷贝到备库相应位置"></a>在主库pfile参数文件和密码文件，并且拷贝到备库相应位置</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd $ORACLE_HOME&#x2F;dbs</span><br><span class="line">scp initorcl.ora orapworcl oracle@172.16.2.32:&#x2F;$ORACLE_HOME&#x2F;dbs</span><br></pre></td></tr></table></figure>

<h2 id="修改备新库初始化参数文件"><a href="#修改备新库初始化参数文件" class="headerlink" title="修改备新库初始化参数文件"></a>修改备新库初始化参数文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;app&#x2F;oracle11g&#x2F;fast_recovery_area&#x2F;orcl</span><br><span class="line">mkdir -p &#x2F;app&#x2F;oracle11g&#x2F;admin&#x2F;orcl&#x2F;adump</span><br><span class="line">mkdir -p &#x2F;app&#x2F;oracle11g&#x2F;&#123;standbylog,archivelog&#125;</span><br><span class="line">mkdir -p &#x2F;app&#x2F;oracle11g&#x2F;oradata&#x2F;orcl</span><br><span class="line"></span><br><span class="line">startup nomount</span><br><span class="line">create spfile from pfile&#x3D;&#39;&#x2F;app&#x2F;oracle11g&#x2F;product&#x2F;11.2.0&#x2F;dbhome_1&#x2F;dbs&#x2F;initorcl.ora&#39;;</span><br><span class="line">alter system set db_unique_name&#x3D;testdb scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_config&#x3D;&#39;DG_CONFIG&#x3D;(primary,standby,testdb)&#39; scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_dest_1 &#x3D; &#39;LOCATION&#x3D;&#x2F;app&#x2F;oracle11g&#x2F;archivelog valid_for&#x3D;(all_logfiles,all_roles) db_unique_name&#x3D;testdb&#39; scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_dest_2 &#x3D; &#39;SERVICE&#x3D;primary lgwr sync valid_for&#x3D;(online_logfiles,primary_role) db_unique_name&#x3D;primary&#39; scope&#x3D;spfile;</span><br><span class="line">alter system set log_archive_dest_3 &#x3D; &#39;SERVICE&#x3D;testdb lgwr sync valid_for&#x3D;(online_logfiles,primary_role) db_unique_name&#x3D;testdb&#39; scope&#x3D;both;</span><br><span class="line">alter system set log_archive_dest_state_3 &#x3D; ENABLE;</span><br><span class="line">alter system set fal_server&#x3D;primary,standby scope&#x3D;spfile;</span><br><span class="line">alter system set fal_client&#x3D;testdb scope&#x3D;spfile;</span><br><span class="line">shutdown abort;</span><br><span class="line">startup nomount;</span><br></pre></td></tr></table></figure>

<h2 id="修改新备库静态监听"><a href="#修改新备库静态监听" class="headerlink" title="修改新备库静态监听"></a>修改新备库静态监听</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim listener.ora</span><br><span class="line"></span><br><span class="line">LISTENER &#x3D;</span><br><span class="line">  (DESCRIPTION_LIST &#x3D;</span><br><span class="line">    (DESCRIPTION &#x3D;</span><br><span class="line">      (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; testdb)(PORT &#x3D; 1521))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">SID_LIST_LISTENER&#x3D;</span><br><span class="line">  (SID_LIST&#x3D;</span><br><span class="line">    (SID_DESC&#x3D;</span><br><span class="line">       (GLOBAL_DBNAME&#x3D;testdb)</span><br><span class="line">       (SID_NAME&#x3D;orcl)</span><br><span class="line">     )</span><br><span class="line">  )</span><br><span class="line">ADR_BASE_LISTENER &#x3D; &#x2F;app&#x2F;oracle11g</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim tnsnames.ora</span><br><span class="line"></span><br><span class="line">PRIMARY &#x3D;</span><br><span class="line">  (DESCRIPTION &#x3D;</span><br><span class="line">    (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; primary)(PORT &#x3D; 1521))</span><br><span class="line">    (CONNECT_DATA &#x3D;</span><br><span class="line">      (SERVER &#x3D; DEDICATED)</span><br><span class="line">      (SERVICE_NAME &#x3D; primary)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">STANDBY &#x3D;</span><br><span class="line">  (DESCRIPTION &#x3D;</span><br><span class="line">    (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; standby)(PORT &#x3D; 1521))</span><br><span class="line">    (CONNECT_DATA &#x3D;</span><br><span class="line">      (SERVER &#x3D; DEDICATED)</span><br><span class="line">      (SERVICE_NAME &#x3D; standby)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">TESTDB &#x3D;</span><br><span class="line">  (DESCRIPTION &#x3D;</span><br><span class="line">    (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; testdb)(PORT &#x3D; 1521))</span><br><span class="line">    (CONNECT_DATA &#x3D;</span><br><span class="line">      (SERVER &#x3D; DEDICATED)</span><br><span class="line">      (SERVICE_NAME &#x3D; testdb)</span><br><span class="line">    )</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>

<h2 id="主库修改相关参数"><a href="#主库修改相关参数" class="headerlink" title="主库修改相关参数"></a>主库修改相关参数</h2><h3 id="修改主库参数文件"><a href="#修改主库参数文件" class="headerlink" title="修改主库参数文件"></a>修改主库参数文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter system set log_archive_config&#x3D;&#39;DG_CONFIG&#x3D;(primary,standby,testdb)&#39; scope&#x3D;both;</span><br><span class="line">alter system set log_archive_dest_3 &#x3D; &#39;SERVICE&#x3D;testdb lgwr sync valid_for&#x3D;(online_logfiles,primary_role) db_unique_name&#x3D;testdb&#39; scope&#x3D;both;</span><br><span class="line">alter system set log_archive_dest_state_3 &#x3D; ENABLE;</span><br><span class="line">alter system set fal_server&#x3D;standby,testdb scope&#x3D;both;</span><br></pre></td></tr></table></figure>
<h3 id="修改主库tnsnames-ora文件-新增如下"><a href="#修改主库tnsnames-ora文件-新增如下" class="headerlink" title="修改主库tnsnames.ora文件,新增如下"></a>修改主库tnsnames.ora文件,新增如下</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TESTDB &#x3D;</span><br><span class="line">  (DESCRIPTION &#x3D;</span><br><span class="line">    (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; testdb)(PORT &#x3D; 1521))</span><br><span class="line">    (CONNECT_DATA &#x3D;</span><br><span class="line">      (SERVER &#x3D; DEDICATED)</span><br><span class="line">      (SERVICE_NAME &#x3D; testdb)</span><br><span class="line">    )</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>

<h2 id="测试主库、新备库是否tnsping正常"><a href="#测试主库、新备库是否tnsping正常" class="headerlink" title="测试主库、新备库是否tnsping正常"></a>测试主库、新备库是否tnsping正常</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqlplus sys&#x2F;123456@primary as sysdba</span><br><span class="line">sqlplus sys&#x2F;123456@testdb as sysdba </span><br><span class="line">tnsping primary</span><br><span class="line">tnsping testdb</span><br></pre></td></tr></table></figure>
<h2 id="在主库通过Rman-Duplicate创建新备库"><a href="#在主库通过Rman-Duplicate创建新备库" class="headerlink" title="在主库通过Rman Duplicate创建新备库"></a>在主库通过Rman Duplicate创建新备库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rman target sys&#x2F;123456@primary auxiliary sys&#x2F;123456@testdb nocatalog</span><br><span class="line">duplicate target database for standby from active database nofilenamecheck;</span><br></pre></td></tr></table></figure>

<h2 id="在新备库开启实时日志应用"><a href="#在新备库开启实时日志应用" class="headerlink" title="在新备库开启实时日志应用"></a>在新备库开启实时日志应用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter database open;</span><br><span class="line">recover managed standby database using current logfile disconnect from session;</span><br><span class="line">alter database recover managed standby database cancel;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title>视频语音实时可视化部署记录</title>
    <url>/posts/c4140ae8.html</url>
    <content><![CDATA[<p>  公司项目所需要安装的一些软件，以作记录，方便后期新增环境操作。</p>
<ol>
<li><p>SRS定位是运营级的互联网直播服务器集群，追求更好的概念完整性和最简单实现的代码。<a href="https://github.com/ossrs/srs">https://github.com/ossrs/srs</a></p>
<a id="more"></a></li>
<li><p>FFmpeg是一个自由软件，可以运行音频和视频多种格式的录影、转换、流功能，包含了libavcodec ─这是一个用于多个项目中音频和视频的解码器库，以及libavformat——一个音频与视频格式转换库。<a href="https://www.ffmpeg.org/">https://www.ffmpeg.org/</a></p>
</li>
<li><p>ImageMagick：免费的创建、编辑、合成图片的软件。它可以读取、转换、写入多种格式的图片。图片切割、颜色替换、各种效果的应用，图片的旋转、组合，文本，直线，多边形，椭圆，曲线，附加到图片伸展旋转。<a href="http://www.imagemagick.org/download/">http://www.imagemagick.org/download/</a></p>
</li>
<li><p>GraphicsMagick号称图像处理领域的瑞士军刀。 短小精悍的代码却提供了一个鲁棒、高效的工具和库集合,来处理图像的读取、写入和操作,支持超过88中图像格式,包括重要的DPX、GIF、JPEG、JPEG-2000、PNG、PDF、PNM和TIFF。GraphicsMagick是从 ImageMagick 5.5.2 分支出来的,但是现在他变得更稳定和优秀,GM更小更容易安装、GM更有效率、GM的手册非常丰富GraphicsMagick的命令与ImageMagick基本是一样的。<a href="http://www.graphicsmagick.org/">http://www.graphicsmagick.org/</a></p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.裁切</span><br><span class="line">gm convert -crop 100x100+20+50 xdr.png xdr1.png </span><br><span class="line">100x100:指要裁切图片的大小;这里不要用*,要用字母X</span><br><span class="line"></span><br><span class="line">20+50:裁切的坐标,xy</span><br><span class="line"></span><br><span class="line">xdr.png:要裁切图片的名字;</span><br><span class="line"></span><br><span class="line">xdr1.png:裁切后图片的名字;</span><br><span class="line"></span><br><span class="line">2.格式转换</span><br><span class="line"></span><br><span class="line">gm convert a.bmp a.jpg </span><br><span class="line">由a.bmp图片转换为a.jpg.</span><br><span class="line"></span><br><span class="line">3.缩略图(640x480)</span><br><span class="line"></span><br><span class="line">gm convert a.jpg -geometry 640x480^ -gravity center -extent 640x480 b.jpg </span><br><span class="line">由a.jpg转换为640x480的b.jpg</span><br><span class="line"></span><br><span class="line">4.追加文字水印</span><br><span class="line"></span><br><span class="line">gm convert a.jpg -font Aricl -fill red -pointsize 33 -draw &quot;text 1600,50 &#39;wenzi&#39;&quot; b.jpg </span><br><span class="line">a.jpg图片用 Aricl字体 红色 大小33 文字从1600 50 坐标开始 内容为wnezi 转换为b.jpg</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>jpegsrc：jpegsrc是用软件实现JPEG图像编码、解码、转码。<a href="https://fossies.org/linux/misc/jpegsrc.v9c.tar.gz/">https://fossies.org/linux/misc/jpegsrc.v9c.tar.gz/</a></li>
</ol>
<h1 id="jdk"><a href="#jdk" class="headerlink" title="jdk"></a>jdk</h1><p>下载地址：<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p>
<p>添加环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#注意jdk版本可能不一致，需修改</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;jdk1.8.0_181&#x2F;</span><br><span class="line">export JRE_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;jdk1.8.0_181&#x2F;jre&#x2F;</span><br><span class="line">export PATH&#x3D;$JAVA_HOME&#x2F;bin:$JAVA_HOME&#x2F;jre&#x2F;bin:$PATH</span><br><span class="line">export CLASSPATH&#x3D;$JAVA_HOME&#x2F;lib:$JAVA_HOME&#x2F;jre&#x2F;lib</span><br></pre></td></tr></table></figure>

<h1 id="ffmpeg"><a href="#ffmpeg" class="headerlink" title="ffmpeg"></a>ffmpeg</h1><p>可参考这里：<a href="https://www.ywthings.com/2018/10/23/ffmpeg-install-static-release.html">https://www.ywthings.com/2018/10/23/ffmpeg-install-static-release.html</a><br>下载地址：<a href="https://www.johnvansickle.com/ffmpeg/">https://www.johnvansickle.com/ffmpeg/</a></p>
<p>添加环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export PATH&#x3D;&quot;&#x2F;usr&#x2F;local&#x2F;ffmpeg&#x2F;bin:$PATH&quot;</span><br></pre></td></tr></table></figure>

<p>测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># amr转换mp3</span><br><span class="line">ffmpeg -i 111.amr test.mp3</span><br></pre></td></tr></table></figure>


<h1 id="srs"><a href="#srs" class="headerlink" title="srs"></a>srs</h1><p>下载地址：<a href="https://github.com/ossrs/srs">https://github.com/ossrs/srs</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#安装</span><br><span class="line">cd</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;ossrs&#x2F;srs</span><br><span class="line">cd srs&#x2F;trunk</span><br><span class="line">.&#x2F;configure  --full &amp;&amp; make</span><br><span class="line"></span><br><span class="line">#复制</span><br><span class="line">mkdir &#x2F;usr&#x2F;local&#x2F;srs</span><br><span class="line">cp -a &#x2F;root&#x2F;srs&#x2F;trunk&#x2F;&#123;conf,etc,objs&#125; &#x2F;usr&#x2F;local&#x2F;srs&#x2F;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#修改启动配置文件路径</span><br><span class="line">ln -s &#x2F;usr&#x2F;local&#x2F;srs&#x2F;etc&#x2F;init.d&#x2F;srs &#x2F;etc&#x2F;init.d&#x2F;srs</span><br><span class="line">vim &#x2F;etc&#x2F;init.d&#x2F;srs</span><br><span class="line">ROOT&#x3D;&quot;&#x2F;usr&#x2F;local&#x2F;srs&#x2F;&quot;</span><br></pre></td></tr></table></figure>

<p>可能出现编译错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#问题1：</span><br><span class="line">Found no assembler Minimum version is nasm-2.13 If you really want to compile without asm, configure with --disable-asm. </span><br><span class="line">解决：</span><br><span class="line">rpm -ivh https:&#x2F;&#x2F;www.nasm.us&#x2F;pub&#x2F;nasm&#x2F;releasebuilds&#x2F;2.14&#x2F;linux&#x2F;nasm-2.14-0.fc27.x86_64.rpm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">问题2：</span><br><span class="line">ERROR: speex not found using pkg-config</span><br><span class="line"></span><br><span class="line">If you think configure made a mistake, make sure you are using the latest</span><br><span class="line">version from Git.  If the latest version fails, report the problem to the</span><br><span class="line">ffmpeg-user@ffmpeg.org mailing list or IRC #ffmpeg on irc.freenode.net.</span><br><span class="line">solve the problem.</span><br><span class="line">解决：</span><br><span class="line">yum install -y speex speex-devel</span><br><span class="line"></span><br><span class="line">问题3：</span><br><span class="line">ERROR: bzlib requested but not found</span><br><span class="line">解决：</span><br><span class="line">yum install -y bzip2 bzip2-devel</span><br></pre></td></tr></table></figure>



<p>srs配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">listen              1935;</span><br><span class="line">max_connections     1000;</span><br><span class="line">srs_log_tank        file;</span><br><span class="line">srs_log_file        .&#x2F;objs&#x2F;srs.log;</span><br><span class="line">http_api &#123;</span><br><span class="line">    enabled         on;</span><br><span class="line">    listen          1985;</span><br><span class="line">&#125;</span><br><span class="line">http_server &#123;</span><br><span class="line">    enabled         on;</span><br><span class="line">    listen          8080;</span><br><span class="line">    dir             .&#x2F;objs&#x2F;nginx&#x2F;html;</span><br><span class="line">&#125;</span><br><span class="line">stats &#123;</span><br><span class="line">    network         0;</span><br><span class="line">    disk            sda sdb xvda xvdb;</span><br><span class="line">&#125;</span><br><span class="line">vhost __defaultVhost__ &#123;</span><br><span class="line">    http_remux &#123;</span><br><span class="line">        enabled     on;</span><br><span class="line">        mount       [vhost]&#x2F;[app]&#x2F;[stream].flv;</span><br><span class="line">        hstrs       on;</span><br><span class="line">    &#125;</span><br><span class="line">#	mix_correct		on;</span><br><span class="line">        http_hooks &#123;</span><br><span class="line">                enabled         on;</span><br><span class="line">                on_publish      http:&#x2F;&#x2F;10.10.0.10:1241&#x2F;callbackOnPublish;</span><br><span class="line">                on_unpublish    http:&#x2F;&#x2F;10.10.0.10:1241&#x2F;callbackOnUnpublish;</span><br><span class="line">                on_play         http:&#x2F;&#x2F;10.10.0.10:1241&#x2F;callbackOnPlay;</span><br><span class="line">                on_stop         http:&#x2F;&#x2F;10.10.0.10:1241&#x2F;callbackOnStop;</span><br><span class="line">                on_dvr          http:&#x2F;&#x2F;10.10.0.10:1241&#x2F;callbackOnDvr;</span><br><span class="line">        &#125;</span><br><span class="line">	dvr &#123;</span><br><span class="line">        	enabled         on;</span><br><span class="line">        	dvr_path        &#x2F;mnt&#x2F;mov&#x2F;[stream]-[timestamp].flv;</span><br><span class="line">        	dvr_plan        session;</span><br><span class="line">         	dvr_duration    30;</span><br><span class="line">         	dvr_wait_keyframe       on;</span><br><span class="line">         	time_jitter             full;</span><br><span class="line">	&#125;</span><br><span class="line">	hls &#123;</span><br><span class="line">        	enabled         on;</span><br><span class="line">        	hls_path        &#x2F;mnt&#x2F;mov;</span><br><span class="line">        	hls_m3u8_file   [stream].m3u8;</span><br><span class="line">        	hls_ts_file     [stream]-[seq].ts;</span><br><span class="line">        	hls_fragment    10;</span><br><span class="line">		hls_window      60000;</span><br><span class="line">#		hls_on_error	disconnect;</span><br><span class="line">	&#125;        </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vhost bandcheck.srs.com &#123;</span><br><span class="line">    enabled         on;</span><br><span class="line">    chunk_size      65000;</span><br><span class="line">    bandcheck &#123;</span><br><span class="line">        enabled         on;</span><br><span class="line">        key             &quot;35c9b402c12a7246868752e2878f7e0e&quot;;</span><br><span class="line">        interval        30;</span><br><span class="line">        limit_kbps      4000;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="jpegsrc-可不安装"><a href="#jpegsrc-可不安装" class="headerlink" title="jpegsrc(可不安装)"></a>jpegsrc(可不安装)</h1><p>下载地址：<a href="http://www.ijg.org/files/jpegsrc.v9c.tar.gz">http://www.ijg.org/files/jpegsrc.v9c.tar.gz</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#编译安装</span><br><span class="line"></span><br><span class="line">tar xf jpegsrc.v9c.tar.gz</span><br><span class="line">cd jpeg-9c</span><br><span class="line">.&#x2F;configure</span><br><span class="line">make libdir&#x3D;&#x2F;usr&#x2F;lib64</span><br><span class="line">make libdir&#x3D;&#x2F;usr&#x2F;lib64 install</span><br></pre></td></tr></table></figure>

<h1 id="ImageMagick-可不安装"><a href="#ImageMagick-可不安装" class="headerlink" title="ImageMagick(可不安装)"></a>ImageMagick(可不安装)</h1><p>下载地址：<a href="https://imagemagick.org/download/ImageMagick.tar.gz">https://imagemagick.org/download/ImageMagick.tar.gz</a></p>
<p>yum安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install php-pear php-devel gcc </span><br><span class="line"></span><br><span class="line">yum install ImageMagick ImageMagick-devel ImageMagick-perl</span><br><span class="line"></span><br><span class="line">convert --version</span><br></pre></td></tr></table></figure>

<p>源码编译安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#编译环境</span><br><span class="line">yum groupinstall &#39;Development Tools&#39;</span><br><span class="line">yum -y install bzip2-devel freetype-devel libjpeg-devel libpng-devel libtiff-devel giflib-devel zlib-devel ghostscript-devel djvulibre-devel libwmf-devel jasper-devel libtool-ltdl-devel libX11-devel libXext-devel libXt-devel lcms-devel libxml2-devel librsvg2-devel OpenEXR-devel php-devel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#编译安装</span><br><span class="line"></span><br><span class="line">wget https:&#x2F;&#x2F;www.imagemagick.org&#x2F;download&#x2F;ImageMagick.tar.gz</span><br><span class="line">tar xvzf ImageMagick.tar.gz</span><br><span class="line">cd ImageMagick*</span><br><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;imagemagick</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">#查看版本号</span><br><span class="line">magick -version</span><br></pre></td></tr></table></figure>

<h1 id="GraphicsMagick"><a href="#GraphicsMagick" class="headerlink" title="GraphicsMagick"></a>GraphicsMagick</h1><p>下载地址：<a href="https://nchc.dl.sourceforge.net/project/graphicsmagick/graphicsmagick/1.3.31/GraphicsMagick-1.3.31.tar.xz">https://nchc.dl.sourceforge.net/project/graphicsmagick/graphicsmagick/1.3.31/GraphicsMagick-1.3.31.tar.xz</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#安装相关依赖</span><br><span class="line">#yum install -y gcc libpng libjpeg libpng-devel libjpeg-devel ghostscript libtiff libtiff-devel freetype freetype-devel</span><br><span class="line"></span><br><span class="line">#编译安装</span><br><span class="line"></span><br><span class="line">tar xf GraphicsMagick-1.3.31.tar.xz</span><br><span class="line">cd GraphicsMagick-1.3.31</span><br><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;gm --with-jpeg&#x3D;yes --with-png&#x3D;yes </span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">#添加环境变量</span><br><span class="line">vim &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line">export GM_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;gm</span><br><span class="line">export PATH&#x3D;$GM_HOME&#x2F;bin:$GM_HOME&#x2F;lib:$PATH</span><br><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;gm&#x2F;bin:$PATH</span><br><span class="line"></span><br><span class="line">#查看是否安装成功（png、jpeg）</span><br><span class="line">gm version</span><br></pre></td></tr></table></figure>

<h1 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h1><p>参考这里：<a href="https://www.ywthings.com/2018/11/24/yum-install-mysql5-6.html">https://www.ywthings.com/2018/11/24/yum-install-mysql5-6.html</a></p>
<h1 id="memcached"><a href="#memcached" class="headerlink" title="memcached"></a>memcached</h1><p>下载地址：<a href="http://www.memcached.org/files/memcached-1.5.12.tar.gz">http://www.memcached.org/files/memcached-1.5.12.tar.gz</a></p>
<p>安装 libevent </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;libevent&#x2F;libevent&#x2F;releases&#x2F;download&#x2F;release-2.1.8-stable&#x2F;libevent-2.1.8-stable.tar.gz </span><br><span class="line">tar -zxvf libevent-2.1.8-stable.tar.gz </span><br><span class="line">cd libevent-2.1.8-stable</span><br><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;libevent </span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure>

<p>安装 Memcached </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;www.memcached.org&#x2F;files&#x2F;memcached-1.5.12.tar.gz</span><br><span class="line">tar -zxvf memcached-1.5.12.tar.gz </span><br><span class="line">cd memcached-1.5.12</span><br><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;memcached --with-libevent&#x3D;&#x2F;usr&#x2F;local&#x2F;libevent&#x2F;</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure>

<p>启动 Memcached</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-d是启动一个守护进程；</span><br><span class="line">-m是分配给Memcache使用的内存数量，单位是MB；</span><br><span class="line">-u是运行Memcache的用户；</span><br><span class="line">-l是监听的服务器IP地址，可以有多个地址；</span><br><span class="line">-p是设置Memcache监听的端口，，最好是1024以上的端口；</span><br><span class="line">-c是最大运行的并发连接数，默认是1024；</span><br><span class="line">-P是设置保存Memcache的pid文件。</span><br><span class="line"></span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;memcached&#x2F;bin&#x2F;memcached -d -m 1024 -u root -l 192.168.1.44 -p 11211 -c 1024 -P &#x2F;tmp&#x2F;memcached.pid</span><br></pre></td></tr></table></figure>

<h1 id="mongodb"><a href="#mongodb" class="headerlink" title="mongodb"></a>mongodb</h1><p>阿里云 天翼云 mongodb 2<br>华为云 腾讯云 mongodb &gt;=3</p>
<h2 id="mongodb2"><a href="#mongodb2" class="headerlink" title="mongodb2"></a>mongodb2</h2><p>mongodb 2 配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dbpath &#x3D; &#x2F;data&#x2F;mongodb&#x2F;db&#x2F;</span><br><span class="line">logpath &#x3D; &#x2F;data&#x2F;mongodb&#x2F;logs&#x2F;mongodb.log</span><br><span class="line">port &#x3D; 5566</span><br><span class="line">fork &#x3D; true</span><br><span class="line">nohttpinterface &#x3D; true</span><br><span class="line">logappend &#x3D; true</span><br><span class="line">auth &#x3D; true</span><br></pre></td></tr></table></figure>
<p>mongodb 2 启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;mongodb.conf</span><br></pre></td></tr></table></figure>

<p>mongodb 2 添加帐号密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo 127.0.0.1:5566&#x2F;admin</span><br><span class="line">&gt;use admin</span><br><span class="line">&gt;db.addUser(&#39;test&#39;,&#39;123456&#39;)</span><br></pre></td></tr></table></figure>

<p><strong>备注：本地连接不上，防火墙关了就可以？</strong></p>
<h2 id="mongodb3"><a href="#mongodb3" class="headerlink" title="mongodb3"></a>mongodb3</h2><p>mongodb 3 配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">port&#x3D;27017</span><br><span class="line">dbpath&#x3D;&#x2F;data&#x2F;mongodb&#x2F;</span><br><span class="line">logpath&#x3D;&#x2F;data&#x2F;mongodb&#x2F;mongod1.log</span><br><span class="line">fork&#x3D;true</span><br><span class="line">bind_ip&#x3D;0.0.0.0   </span><br><span class="line">logappend&#x3D;true</span><br><span class="line">auth&#x3D;true 第一次启动不要添加，添加认证后才启用</span><br></pre></td></tr></table></figure>

<p>mongodb 3 启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;mongodb.conf</span><br></pre></td></tr></table></figure>

<p>mongodb 3 添加帐号密码认证</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo 127.0.0.1:27017&#x2F;admin</span><br><span class="line">&gt;use admin</span><br><span class="line">&gt; db.createUser(&#123; user: &#39;root&#39;, pwd: &#39;test123&#39;, roles: [ &#123; role:&quot;root&quot;, db: &quot;admin&quot; &#125; ] &#125;);</span><br></pre></td></tr></table></figure>

<h1 id="端口"><a href="#端口" class="headerlink" title="端口"></a>端口</h1><p>需开放端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">16400 文件服务</span><br><span class="line">1241 1238 通信服务</span><br><span class="line">1935 1985 srs服务</span><br><span class="line">因srs占用8080，web需更改端口18080 web服务</span><br></pre></td></tr></table></figure>

<h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>1、通信服务需拷贝/lib64/libudx.so<br>2、内核参数，如阿里云tcp负载需添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">net.ipv4.conf.default.rp_filter &#x3D; 0</span><br><span class="line">net.ipv4.conf.all.rp_filter &#x3D; 0</span><br><span class="line">net.ipv4.conf.eth0.rp_filter &#x3D; 0</span><br><span class="line">net.ipv4.conf.eth1.rp_filter &#x3D; 0</span><br></pre></td></tr></table></figure>
<p>3、两个数据库，一个全局数据库修改数据库连接表，另数据库修改流媒体信息表，一个通信信息表<br>4、web服务修改文件服务地址、mongodb地址、数据库地址<br>5、tcp服务修改memcache地址、mongodb地址、数据库地址<br>6、相关目录的创建，可查看文件服务配置，srs配置<br>7、两台以上服务负载需创建共同目录，如NFS来支撑</p>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>ffmpeg</tag>
        <tag>srs</tag>
        <tag>ImageMagick</tag>
        <tag>GraphicsMagick</tag>
        <tag>jpegsrc</tag>
      </tags>
  </entry>
  <entry>
    <title>PicGo+Gitee实现Typora图床</title>
    <url>/posts/2ed66b0a.html</url>
    <content><![CDATA[<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ul>
<li>Typora</li>
<li>PicGo</li>
<li>picgo-plugin-gitee-uploader插件</li>
</ul>
<a id="more"></a>

<h3 id="安装之后打开主界面"><a href="#安装之后打开主界面" class="headerlink" title="安装之后打开主界面"></a>安装之后打开主界面</h3><img src="https://gitee.com/hyman0603/img/raw/master/img/image-20200611100417092.png" alt="image-20200611100417092" style="zoom:50%;" />

<h3 id="选择最底下的插件设置，搜索gitee"><a href="#选择最底下的插件设置，搜索gitee" class="headerlink" title="选择最底下的插件设置，搜索gitee"></a>选择最底下的插件设置，搜索<strong>gitee</strong></h3><img src="https://gitee.com/hyman0603/img/raw/master/img/image-20200611100515697.png" alt="image-20200611100515697" style="zoom:50%;" />

<h3 id="建立gitee（码云）图床库"><a href="#建立gitee（码云）图床库" class="headerlink" title="建立gitee（码云）图床库"></a>建立gitee（码云）图床库</h3><img src="https://gitee.com/hyman0603/img/raw/master/img/image-20200611100744890.png" alt="image-20200611100744890" style="zoom:50%;" />

<h3 id="获取token"><a href="#获取token" class="headerlink" title="获取token"></a>获取token</h3> <img src="https://gitee.com/hyman0603/img/raw/master/img/image-20200611101003289.png" alt="image-20200611101003289" style="zoom: 50%;" />

<h3 id="配置PicGo"><a href="#配置PicGo" class="headerlink" title="配置PicGo"></a>配置PicGo</h3><img src="https://gitee.com/hyman0603/img/raw/master/img/image-20200611101145548.png" alt="image-20200611101145548" style="zoom:50%;" />]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>picgo</tag>
        <tag>gitee</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx上的限流</title>
    <url>/posts/91a613ae.html</url>
    <content><![CDATA[<p>原文链接：<br><a href="https://www.nginx.com/blog/rate-limiting-nginx/">https://www.nginx.com/blog/rate-limiting-nginx/</a></p>
<p>限流（rate limiting）是NGINX众多特性中最有用的，也是经常容易被误解和错误配置的，特性之一。该特性可以限制某个用户在一个给定时间段内能够产生的HTTP请求数。请求可以简单到就是一个对于主页的GET请求或者一个登陆表格的POST请求。</p>
<p>限流也可以用于安全目的上，比如减慢暴力密码破解攻击。通过限制进来的请求速率，并且（结合日志）标记出目标URLs来帮助防范DDoS攻击。一般地说，限流是用在保护上游应用服务器不被在同一时刻的大量用户请求湮没。</p>
<a id="more"></a>
<p>下面介绍NGINX限流的基本用法。</p>
<h1 id="NGINX限流是如何工作的"><a href="#NGINX限流是如何工作的" class="headerlink" title="NGINX限流是如何工作的"></a>NGINX限流是如何工作的</h1><p>NGINX限流使用漏桶算法（leaky bucket algorithm），该算法广泛应用于通信和基于包交换计算机网络中，用来处理当带宽被限制时的突发情况。和一个从上面进水，从下面漏水的桶的原理很相似；如果进水的速率大于漏水的速率，这个桶就会发生溢出。</p>
<p>在请求处理过程中，水代表从客户端来的请求，而桶代表了一个队列，请求在该队列中依据先进先出（FIFO）算法等待被处理。漏的水代表请求离开缓冲区并被服务器处理，溢出代表了请求被丢弃并且永不被服务。</p>
<h1 id="配置基本的限流功能"><a href="#配置基本的限流功能" class="headerlink" title="配置基本的限流功能"></a>配置基本的限流功能</h1><p>有两个主要的指令可以用来配置限流：limit_req_zone和limit_req，例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">limit_req_zone $binary_remote_addr zone&#x3D;mylimit:10m rate&#x3D;10r&#x2F;s;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    location &#x2F;login&#x2F; &#123;</span><br><span class="line">        limit_req zone&#x3D;mylimit;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;my_upstream;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当limit_req 在它出现的环境中启用了限流（在上面的例子中，作用在所有对于/login/的请求上），则limit_req_zone指令定义了限流的参数。</p>
<p>limit_req_zone指令一般定义在http块内部，使得该指令可以在多个环境中使用。该指令有下面三个参数：</p>
<ul>
<li>Key — 在限流应用之前定义了请求的特征。在上面例子中，它是$binary_remote_addr（NGINX变量），该变量代表了某个客户端IP地址的二进制形式。这意味着我们可以将每个特定的IP地址的请求速率限制为第三个参数所定义的值。（使用这个变量的原因是因为它比用string代表客户端IP地址的$remote_addr变量消耗更少的空间。）</li>
</ul>
<ul>
<li><p>Zone — 定义了存储每个IP地址状态和它访问受限请求URL的频率的共享内存区域。将这些信息保存在共享内存中，意味着这些信息能够在NGINX工作进程之间共享。定义有两个部分：由zone=关键字标识的区域名称，以及冒号后面的区域大小。约16000个IP地址的状态信息消耗1M内存大小，因此我们的区域（zone）大概可以存储约160000个地址。<br>当NGINX需要添加新的记录时，如果此时存储耗尽了，最老的记录会被移除。如果释放的存储空间还是无法容纳新的记录，NGINX返回503 (Service Temporarily Unavailable)状态码。此外，为了防止内存被耗尽，每次NGINX创建一个新的记录的同时移除多达两条前60秒内没有被使用的记录。</p>
</li>
<li><p>Rate — 设置最大的请求速率。在上面的例子中，速率不能超过10个请求每秒。NGINX事实上可以在毫秒级别追踪请求，因此这个限制对应了1个请求每100毫秒。因为我们不允许突刺（bursts，短时间内的突发流量，详细见下一部分。），这意味着如果某个请求到达的时间离前一个被允许的请求小于100毫秒，它会被拒绝。</p>
</li>
</ul>
<p>limit_req_zone指令设置限流和共享内存区域的参数，但是该指令实际上并不限制请求速率。为了限制起作用，需要将该限制应用到某个特定的location或server块（block），通过包含一个limit_req指令的方式。在上面的例子中，我们将请求限制在/login/上。</p>
<p>所以现在对于/login/，每个特定的IP地址被限制为10个请求每秒— 或者更准确地说，不能在与前一个请求间隔100毫秒时间内发送请求。</p>
<h1 id="处理流量突刺（Bursts）"><a href="#处理流量突刺（Bursts）" class="headerlink" title="处理流量突刺（Bursts）"></a>处理流量突刺（Bursts）</h1><p>如果在100毫秒内得到2个请求会怎么样？对于第2个请求，NGINX返回503状态码给客户端。这可能不是我们想要的，因为事实上，应用是趋向于突发性的。相反，我们想要缓存任何过多的请求并且及时地服务它们。下面是我们使用limit_req的burst参数来更新配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location &#x2F;login&#x2F; &#123;</span><br><span class="line">      limit_req zone&#x3D;mylimit burst&#x3D;20;</span><br><span class="line"></span><br><span class="line">      proxy_pass http:&#x2F;&#x2F;my_upstream;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>burst参数定义了一个客户端能够产生超出区域（zone）规定的速率的请求数量（在我们示例mylimit区域中，速率限制是10个请求每秒，或1个请求每100毫秒）。一个请求在前一个请求后的100毫秒间隔内达到，该请求会被放入一个队列，并且该队列大小被设置为20.</p>
<p>这意味着如果从某个特定IP地址来的21个请求同时地达到，NGINX立即转发第一个请求到上游的服务器组，并且将剩余的20个请求放入队列中。然后，NGINX每100毫秒转发一个队列中的请求，并且只有当某个新进来的请求使得队列中的请求数目超过了20，则返回503给客户端。</p>
<h1 id="无延迟排队"><a href="#无延迟排队" class="headerlink" title="无延迟排队"></a>无延迟排队</h1><p>带有burst的配置产生平滑的网络流量，但是不实用，因为该配置会使得你的网站表现的很慢。在上面的例子中，队列中第20个数据包等待2秒才能被转发，这时该数据包的响应可能对于客户端已经没有了意义。为了处理这种情况，除了burst参数外，添加nodelay参数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location &#x2F;login&#x2F; &#123;</span><br><span class="line">      limit_req zone&#x3D;mylimit burst&#x3D;20 nodelay;</span><br><span class="line"></span><br><span class="line">      proxy_pass http:&#x2F;&#x2F;my_upstream</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>带有nodelay参数，NGINX仍然会按照burst参数在队列中分配插槽（slot）以及利用已配置的限流，但是不是通过间隔地转发队列中的请求。相反，当某个请求来的太快，只要队列中有可用的空间（slot），NGINX会立即转发它。该插槽（slot）被标记为“已使用”，并且不会被释放给另一个请求，一直到经过适当的时间（在上面的例子中，是100毫秒）。</p>
<p>像之前一样假设有20个插槽的队列是空的，并且来自于给定的IP地址的21个请求同时地到达。NGINX立即转发这21个请求以及将队列中的20个插槽标记为“已使用”，然后每隔100毫秒释放一个插槽。（相反，如果有25个请求，NGINX会立即转发25个中的21个请求，标记20个插槽为“已使用”，并且用503状态拒绝4个请求。）</p>
<p>现在假设在转发第一个请求集合之后的101毫秒，有另外的20个请求同时地到达。队列中只有1个插槽被释放，因此NGINX转发1个请求，并且用503状态拒绝其它的19个请求。相反，如果在这20个新请求到达之前过去了501毫秒，则有5个插槽被释放，因此NGINX立即转发5个请求，并且拒绝其它15个请求。</p>
<p>效果等同于10个请求每秒的限流。如果你想利用请求之间的无限制性间隔的限流，nodelay选项则是非常有用的。</p>
<p>注意：对于大多数的部署，我们推荐在limit_req指令中包含burst和nodelay参数。</p>
<h1 id="高级设置的例子"><a href="#高级设置的例子" class="headerlink" title="高级设置的例子"></a>高级设置的例子</h1><p>通过结合基本的限流和其它的NGINX特性，你可以实现更多的细微的流量限制。</p>
<h2 id="白名单"><a href="#白名单" class="headerlink" title="白名单"></a>白名单</h2><p>下面的例子展示了如何将限流作用在任何一个不在“白名单”中的请求上。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">geo $limit &#123;</span><br><span class="line">        default 1;</span><br><span class="line">        10.0.0.0&#x2F;8 0;</span><br><span class="line">        192.168.0.0&#x2F;24 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">map $limit $limit_key &#123;</span><br><span class="line">        0 &quot;&quot;;</span><br><span class="line">        1 $binary_remote_addr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">limit_req_zone $limit_key zone&#x3D;req_zone:10m rate&#x3D;5r&#x2F;s;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">        location &#x2F; &#123;</span><br><span class="line">                limit_req zone&#x3D;req_zone burst&#x3D;10 nodelay;</span><br><span class="line">                </span><br><span class="line">                # ...</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个例子同时使用了geo和map指令。对于IP地址在白名单中的，geo块分配0值给$limit；其它所有不在白名单中的IP地址，分配1值。然后我们使用一个map去将这些值映射到某个key中，例如：</p>
<ul>
<li>如果$limit是0，$limit_key被设置为空字符串</li>
<li>如果$limit是1，$limit_key被设置为客户端的IP地址的二进制格式</li>
</ul>
<p>这个两个结合起来，对于白名单中的IP地址，$limit_key被设置为空字符串；否则，被设置为客户端的IP地址。当limit_req_zone指令的第一个参数是一个空字符串，限制不起作用，因此白名单的IP地址（在10.0.0.0/8和192.168.0.0/24子网中）没有被限制。其它所有的IP地址都被限制为5个请求每秒。</p>
<p>limit_req指令将限制作用在/定位中，并且允许在没有转发延迟的情况下，转发多达10个数据包。</p>
<h2 id="在一个定位中包含多个limit-req指令"><a href="#在一个定位中包含多个limit-req指令" class="headerlink" title="在一个定位中包含多个limit_req指令"></a>在一个定位中包含多个limit_req指令</h2><p>可以在单个定位（location）中包含多个limit_req指令。匹配给定的请求限制都会被使用，这意味着采用最严格的限制。例如，如果多于一个的指令使用了延迟，最终使用最长的延迟。类似地，如果某个指令使得请求被拒绝，即使其它的指令允许请求通过，最终还是被拒绝。</p>
<p>我们可以在白名单中的IP地址上应用某个限流来扩展之前的例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">      # ...</span><br><span class="line"></span><br><span class="line">      limit_req_zone $limit_key zone&#x3D;req_zone:10m rate&#x3D;5r&#x2F;s;</span><br><span class="line">      limit_req_zone $binary_remote_addr zone&#x3D;req_zone_wl:10m rate&#x3D;15r&#x2F;s;</span><br><span class="line"></span><br><span class="line">      server &#123;</span><br><span class="line">            # ...</span><br><span class="line">            location &#x2F; &#123;</span><br><span class="line">                  limit_req zone&#x3D;req_zone burst&#x3D;10 nodelay;</span><br><span class="line">                  limit_req zone&#x3D;req_zone_wl burst&#x3D;20 nodelay;</span><br><span class="line">                  # ...            </span><br><span class="line">            &#125;</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在白名单上的IP地址不匹配第一个限流（req_zone），但是能匹配第二个（req_zone_wl），因此这些IP地址被限制为15个请求每秒。不在白名单上的IP地址两个限流都能匹配上，因此最严格的那个限流起作用：5个请求每秒。</p>
<h1 id="配置相关的特性"><a href="#配置相关的特性" class="headerlink" title="配置相关的特性"></a>配置相关的特性</h1><h2 id="日志（Logging）"><a href="#日志（Logging）" class="headerlink" title="日志（Logging）"></a>日志（Logging）</h2><p>默认，NGNIX记录由于限流导致的延迟或丢弃的请求的日志，如下面的例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2015&#x2F;06&#x2F;13 04:20:00 [error] 120315#0: *32086 limiting requests, excess: 1.000 by zone &quot;mylimit&quot;, client: 192.168.1.2, server: nginx.com, request: &quot;GET &#x2F; HTTP&#x2F;1.0&quot;, host: &quot;nginx.com&quot;</span><br></pre></td></tr></table></figure>

<p>该日志记录包含的字段：</p>
<ul>
<li>limiting requests — 日志条目记录了某个限流的标志</li>
<li>excess — 超过这个请求代表的配置的速率的每毫秒请求数目</li>
<li>zone — 定义了启用了限流的区域</li>
<li>client — 产生请求的客户端IP地址</li>
<li>server — 服务器的IP地址或主机名</li>
<li>request — 客户端产生的实际的HTTP请求</li>
<li>host — HTTP头部主机名的值</li>
</ul>
<p>默认，NGINX日志在error级别拒绝请求，如上面例子中的[error]所示。（它在低一个级别上记录延迟的请求，因此默认是info。）用limit_req_log_level指令来改变日志级别。下面我们设置在warn级别上记录被拒绝的请求的日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location &#x2F;login&#x2F; &#123;</span><br><span class="line">      limit_req zone&#x3D;mylimit burst&#x3D;20 nodelay;</span><br><span class="line">      limit_req_log_level warn;</span><br><span class="line"></span><br><span class="line">      proxy_pass http:&#x2F;&#x2F;my_upstream;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="发送给客户端的错误码"><a href="#发送给客户端的错误码" class="headerlink" title="发送给客户端的错误码"></a>发送给客户端的错误码</h2><p>默认，当某个客户端超过它的限流，NGINX用503（Service Temporarily Unavailable）状态码来响应。使用limit_req_status指令设置一个不同的状态码（在下面的例子是444）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location &#x2F;login&#x2F; &#123;</span><br><span class="line">      limit_req zone&#x3D;mylimit burst&#x3D;20 nodelay;</span><br><span class="line">      limit_req_status 444;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="拒绝对特定位置的所有请求"><a href="#拒绝对特定位置的所有请求" class="headerlink" title="拒绝对特定位置的所有请求"></a>拒绝对特定位置的所有请求</h2><p>如果你想拒绝对于某个特定URL的所有请求，而不是仅仅的限制它们，可以为这个URL配置一个location块，并且在其中包含deny all指令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location &#x2F;foo.php &#123;</span><br><span class="line">      deny all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus 监控MySQL</title>
    <url>/posts/21401b21.html</url>
    <content><![CDATA[<h1 id="mysql-exporter部署"><a href="#mysql-exporter部署" class="headerlink" title="mysql_exporter部署"></a>mysql_exporter部署</h1><p>下载mysql_exporter并解压</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;mysqld_exporter&#x2F;releases</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 将mysql_exporter二进制文件拷贝至&#x2F;usr&#x2F;local&#x2F;bin</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<h1 id="授权用户给exporter、修改mysql配置文件"><a href="#授权用户给exporter、修改mysql配置文件" class="headerlink" title="授权用户给exporter、修改mysql配置文件"></a>授权用户给exporter、修改mysql配置文件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; CREATE USER &#39;exporter&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123456&#39; WITH MAX_USER_CONNECTIONS 5;</span><br><span class="line">&gt; GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO &#39;exporter&#39;@&#39;localhost&#39;;</span><br><span class="line"></span><br><span class="line">$ vim &#x2F;etc&#x2F;my.cnf</span><br><span class="line">[client]</span><br><span class="line">user&#x3D;exporter</span><br><span class="line">password&#x3D;123456</span><br></pre></td></tr></table></figure>

<h1 id="启动exporter客户端"><a href="#启动exporter客户端" class="headerlink" title="启动exporter客户端"></a>启动exporter客户端</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mysqld_exporter --config.my-cnf&#x3D;&#x2F;etc&#x2F;my.cnf</span><br><span class="line"></span><br><span class="line">常用参数：</span><br><span class="line">&#x2F;&#x2F; 选择采集innodb</span><br><span class="line">--collect.info_schema.innodb_cmp</span><br><span class="line">&#x2F;&#x2F; innodb存储引擎状态</span><br><span class="line">--collect.engine_innodb_status</span><br><span class="line">&#x2F;&#x2F; 指定配置文件</span><br><span class="line">--config.my-cnf&#x3D;&quot;&#x2F;etc&#x2F;my.cnf&quot;</span><br></pre></td></tr></table></figure>

<h1 id="添加system系统服务"><a href="#添加system系统服务" class="headerlink" title="添加system系统服务"></a>添加system系统服务</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;mysql_exporter.service</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;Prometheus</span><br><span class="line">Wants&#x3D;network-online.target</span><br><span class="line">After&#x3D;network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User&#x3D;root</span><br><span class="line">Group&#x3D;root</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;mysqld_exporter \</span><br><span class="line">--config.my-cnf&#x3D;&#x2F;etc&#x2F;my.cnf</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure>

<h1 id="启动添加的system服务"><a href="#启动添加的system服务" class="headerlink" title="启动添加的system服务"></a>启动添加的system服务</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ systemctl daemon-reload</span><br><span class="line">$ systemctl start mysql_exporter.service</span><br><span class="line">$ systemctl enable mysql_exporter.service</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; mysql_export默认端口 - 9104</span><br><span class="line">$ netstat -lntup | grep &quot;9104&quot;</span><br></pre></td></tr></table></figure>
<h1 id="查看捕获mysql数据"><a href="#查看捕获mysql数据" class="headerlink" title="查看捕获mysql数据"></a>查看捕获mysql数据</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:9104&#x2F;metrics</span><br></pre></td></tr></table></figure>

<h1 id="prometheus配置加入mysql节点"><a href="#prometheus配置加入mysql节点" class="headerlink" title="prometheus配置加入mysql节点"></a>prometheus配置加入mysql节点</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ vim &#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;prometheus.yml</span><br><span class="line">  - job_name: &#39;mysql&#39;</span><br><span class="line">    scrape_interval: 5s</span><br><span class="line">    # 静态添加node</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#39;192.168.1.61:9104&#39;]</span><br></pre></td></tr></table></figure>

<h1 id="Granfana导入MySQL监控图表"><a href="#Granfana导入MySQL监控图表" class="headerlink" title="Granfana导入MySQL监控图表"></a>Granfana导入MySQL监控图表</h1><p>去grafana dashboard下载对应的图表或者直接在grafana导入图表输入ID下载<br>图表下载地址：<a href="https://grafana.com/grafana/dashboards/7362">https://grafana.com/grafana/dashboards/7362</a><br>图表ID：11796</p>
<h1 id="mysql报警规则"><a href="#mysql报警规则" class="headerlink" title="mysql报警规则"></a>mysql报警规则</h1><h2 id="配置alertmanager报警-添加prometheus配置"><a href="#配置alertmanager报警-添加prometheus配置" class="headerlink" title="配置alertmanager报警,添加prometheus配置"></a>配置alertmanager报警,添加prometheus配置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rule_files:</span><br><span class="line">  ...</span><br><span class="line">  - &quot;&#x2F;data&#x2F;etc&#x2F;mysql*.rules&quot;</span><br></pre></td></tr></table></figure>

<h2 id="配置mysql报警规则"><a href="#配置mysql报警规则" class="headerlink" title="配置mysql报警规则"></a>配置mysql报警规则</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">groups:</span><br><span class="line">- name: MySQLStatsAlert</span><br><span class="line">  rules:</span><br><span class="line">  - alert: MySQL is down</span><br><span class="line">    expr: mysql_up &#x3D;&#x3D; 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; MySQL is down&quot;</span><br><span class="line">      description: &quot;MySQL database is down. This requires immediate action!&quot;</span><br><span class="line">  - alert: open files high</span><br><span class="line">    expr: mysql_global_status_innodb_num_open_files &gt; (mysql_global_variables_open_files_limit) * 0.75</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; open files high&quot;</span><br><span class="line">      description: &quot;Open files is high. Please consider increasing open_files_limit.&quot;</span><br><span class="line">  - alert: Read buffer size is bigger than max. allowed packet size</span><br><span class="line">    expr: mysql_global_variables_read_buffer_size &gt; mysql_global_variables_slave_max_allowed_packet </span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Read buffer size is bigger than max. allowed packet size&quot;</span><br><span class="line">      description: &quot;Read buffer size (read_buffer_size) is bigger than max. allowed packet size (max_allowed_packet).This can break your replication.&quot;</span><br><span class="line">  - alert: Sort buffer possibly missconfigured</span><br><span class="line">    expr: mysql_global_variables_innodb_sort_buffer_size &lt;256*1024 or mysql_global_variables_read_buffer_size &gt; 4*1024*1024 </span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Sort buffer possibly missconfigured&quot;</span><br><span class="line">      description: &quot;Sort buffer size is either too big or too small. A good value for sort_buffer_size is between 256k and 4M.&quot;</span><br><span class="line">  - alert: Thread stack size is too small</span><br><span class="line">    expr: mysql_global_variables_thread_stack &lt;196608</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Thread stack size is too small&quot;</span><br><span class="line">      description: &quot;Thread stack size is too small. This can cause problems when you use Stored Language constructs for example. A typical is 256k for thread_stack_size.&quot;</span><br><span class="line">  - alert: Used more than 80% of max connections limited </span><br><span class="line">    expr: mysql_global_status_max_used_connections &gt; mysql_global_variables_max_connections * 0.8</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Used more than 80% of max connections limited&quot;</span><br><span class="line">      description: &quot;Used more than 80% of max connections limited&quot;</span><br><span class="line">  - alert: InnoDB Force Recovery is enabled</span><br><span class="line">    expr: mysql_global_variables_innodb_force_recovery !&#x3D; 0 </span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Force Recovery is enabled&quot;</span><br><span class="line">      description: &quot;InnoDB Force Recovery is enabled. This mode should be used for data recovery purposes only. It prohibits writing to the data.&quot;</span><br><span class="line">  - alert: InnoDB Log File size is too small</span><br><span class="line">    expr: mysql_global_variables_innodb_log_file_size &lt; 16777216 </span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Log File size is too small&quot;</span><br><span class="line">      description: &quot;The InnoDB Log File size is possibly too small. Choosing a small InnoDB Log File size can have significant performance impacts.&quot;</span><br><span class="line">  - alert: InnoDB Flush Log at Transaction Commit</span><br><span class="line">    expr: mysql_global_variables_innodb_flush_log_at_trx_commit !&#x3D; 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Flush Log at Transaction Commit&quot;</span><br><span class="line">      description: &quot;InnoDB Flush Log at Transaction Commit is set to a values !&#x3D; 1. This can lead to a loss of commited transactions in case of a power failure.&quot;</span><br><span class="line">  - alert: Table definition cache too small</span><br><span class="line">    expr: mysql_global_status_open_table_definitions &gt; mysql_global_variables_table_definition_cache</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Table definition cache too small&quot;</span><br><span class="line">      description: &quot;Your Table Definition Cache is possibly too small. If it is much too small this can have significant performance impacts!&quot;</span><br><span class="line">  - alert: Table open cache too small</span><br><span class="line">    expr: mysql_global_status_open_tables &gt;mysql_global_variables_table_open_cache * 99&#x2F;100</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Table open cache too small&quot;</span><br><span class="line">      description: &quot;Your Table Open Cache is possibly too small (old name Table Cache). If it is much too small this can have significant performance impacts!&quot;</span><br><span class="line">  - alert: Thread stack size is possibly too small</span><br><span class="line">    expr: mysql_global_variables_thread_stack &lt; 262144</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Thread stack size is possibly too small&quot;</span><br><span class="line">      description: &quot;Thread stack size is possibly too small. This can cause problems when you use Stored Language constructs for example. A typical is 256k for thread_stack_size.&quot;</span><br><span class="line">  - alert: InnoDB Buffer Pool Instances is too small</span><br><span class="line">    expr: mysql_global_variables_innodb_buffer_pool_instances &#x3D;&#x3D; 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Buffer Pool Instances is too small&quot;</span><br><span class="line">      description: &quot;If you are using MySQL 5.5 and higher you should use several InnoDB Buffer Pool Instances for performance reasons. Some rules are: InnoDB Buffer Pool Instance should be at least 1 Gbyte in size. InnoDB Buffer Pool Instances you can set equal to the number of cores of your machine.&quot;</span><br><span class="line">  - alert: InnoDB Plugin is enabled</span><br><span class="line">    expr: mysql_global_variables_ignore_builtin_innodb &#x3D;&#x3D; 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Plugin is enabled&quot;</span><br><span class="line">      description: &quot;InnoDB Plugin is enabled&quot;</span><br><span class="line">  - alert: Binary Log is disabled</span><br><span class="line">    expr: mysql_global_variables_log_bin !&#x3D; 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binary Log is disabled&quot;</span><br><span class="line">      description: &quot;Binary Log is disabled. This prohibits you to do Point in Time Recovery (PiTR).&quot;</span><br><span class="line">  - alert: Binlog Cache size too small</span><br><span class="line">    expr: mysql_global_variables_binlog_cache_size &lt; 1048576</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Cache size too small&quot;</span><br><span class="line">      description: &quot;Binlog Cache size is possibly to small. A value of 1 Mbyte or higher is OK.&quot;</span><br><span class="line">  - alert: Binlog Statement Cache size too small</span><br><span class="line">    expr: mysql_global_variables_binlog_stmt_cache_size &lt;1048576 and mysql_global_variables_binlog_stmt_cache_size &gt; 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Statement Cache size too small&quot;</span><br><span class="line">      description: &quot;Binlog Statement Cache size is possibly to small. A value of 1 Mbyte or higher is typically OK.&quot;</span><br><span class="line">  - alert: Binlog Transaction Cache size too small</span><br><span class="line">    expr: mysql_global_variables_binlog_cache_size  &lt;1048576</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Transaction Cache size too small&quot;</span><br><span class="line">      description: &quot;Binlog Transaction Cache size is possibly to small. A value of 1 Mbyte or higher is typically OK.&quot;</span><br><span class="line">  - alert: Sync Binlog is enabled</span><br><span class="line">    expr: mysql_global_variables_sync_binlog &#x3D;&#x3D; 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Sync Binlog is enabled&quot;</span><br><span class="line">      description: &quot;Sync Binlog is enabled. This leads to higher data security but on the cost of write performance.&quot;</span><br><span class="line">  - alert: IO thread stopped</span><br><span class="line">    expr: mysql_slave_status_slave_io_running !&#x3D; 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; IO thread stopped&quot;</span><br><span class="line">      description: &quot;IO thread has stopped. This is usually because it cannot connect to the Master any more.&quot;</span><br><span class="line">  - alert: SQL thread stopped </span><br><span class="line">    expr: mysql_slave_status_slave_sql_running &#x3D;&#x3D; 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; SQL thread stopped&quot;</span><br><span class="line">      description: &quot;SQL thread has stopped. This is usually because it cannot apply a SQL statement received from the master.&quot;</span><br><span class="line">  - alert: SQL thread stopped</span><br><span class="line">    expr: mysql_slave_status_slave_sql_running !&#x3D; 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Sync Binlog is enabled&quot;</span><br><span class="line">      description: &quot;SQL thread has stopped. This is usually because it cannot apply a SQL statement received from the master.&quot;</span><br><span class="line">  - alert: Slave lagging behind Master</span><br><span class="line">    expr: rate(mysql_slave_status_seconds_behind_master[1m]) &gt;30 </span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning </span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Slave lagging behind Master&quot;</span><br><span class="line">      description: &quot;Slave is lagging behind Master. Please check if Slave threads are running and if there are some performance issues!&quot;</span><br><span class="line">  - alert: Slave is NOT read only(Please ignore this warning indicator.)</span><br><span class="line">    expr: mysql_global_variables_read_only !&#x3D; 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Slave is NOT read only&quot;</span><br><span class="line">      description: &quot;Slave is NOT set to read only. You can accidentally manipulate data on the slave and get inconsistencies...&quot;</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>Prometheus</category>
      </categories>
      <tags>
        <tag>Prometheus</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>记录MongoDB3.4分片的一些配置</title>
    <url>/posts/3737b178.html</url>
    <content><![CDATA[<h2 id="MongoDB介绍"><a href="#MongoDB介绍" class="headerlink" title="MongoDB介绍"></a>MongoDB介绍</h2><p>MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。在高负载的情况下，添加更多的节点，可以保证服务器性能。MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。</p>
<a id="more"></a>
<h2 id="MongoDB特点"><a href="#MongoDB特点" class="headerlink" title="MongoDB特点"></a>MongoDB特点</h2><ul>
<li>MongoDB的提供了一个面向文档存储，操作起来比较简单和容易。</li>
<li>你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=”Ning”,Address=”Beijing”)来实现更快的排序。</li>
<li>你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。</li>
<li>如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。</li>
<li>Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。</li>
<li>MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。</li>
<li>Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。</li>
<li>Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。</li>
<li>Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。</li>
<li>GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。</li>
<li>MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。</li>
<li>MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。</li>
</ul>
<h2 id="MongoDB安装"><a href="#MongoDB安装" class="headerlink" title="MongoDB安装"></a>MongoDB安装</h2><h3 id="yum安装"><a href="#yum安装" class="headerlink" title="yum安装"></a>yum安装</h3><p>官网：<a href="https://docs.mongodb.com/master/tutorial/install-mongodb-on-red-hat/">https://docs.mongodb.com/master/tutorial/install-mongodb-on-red-hat/</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# vim &#x2F;etc&#x2F;yum.repos.d&#x2F;mongodb-org-3.4.repo</span><br><span class="line">[mongodb-org-3.4]</span><br><span class="line">name&#x3D;MongoDB Repository</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;repo.mongodb.org&#x2F;yum&#x2F;redhat&#x2F;$releasever&#x2F;mongodb-org&#x2F;3.4&#x2F;x86_64&#x2F;</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;www.mongodb.org&#x2F;static&#x2F;pgp&#x2F;server-3.4.asc</span><br><span class="line"></span><br><span class="line">[root@object1 ~]#sudo yum install -y mongodb-org</span><br></pre></td></tr></table></figure>

<h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>官网：<a href="https://www.mongodb.com/download-center?jmp=nav#community">https://www.mongodb.com/download-center?jmp=nav#community</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]#curl -O https:&#x2F;&#x2F;fastdl.mongodb.org&#x2F;linux&#x2F;mongodb-linux-x86_64-rhel70-3.4.4.tgz</span><br><span class="line">[root@object1 ~]#tar -zxvf mongodb-linux-x86_64-rhel70-3.4.4.tgz </span><br><span class="line">[root@object1 ~]# mv mongodb-linux-x86_64-rhel70-3.4.4 &#x2F;usr&#x2F;local&#x2F;mongodb</span><br><span class="line"></span><br><span class="line">#把安装目录添加到系统环境中</span><br><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure>

<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>官网：<a href="https://docs.mongodb.com/manual/administration/configuration/">https://docs.mongodb.com/manual/administration/configuration/</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#使用YAML配置也可以使用原ini配置</span><br><span class="line">#基本设置</span><br><span class="line">processManagement:</span><br><span class="line">   fork: true</span><br><span class="line">   pidFilePath: &#x2F;home&#x2F;mongodb&#x2F;mongodb.pid</span><br><span class="line">storage:</span><br><span class="line">   dbPath: &#x2F;home&#x2F;mongodb&#x2F;data</span><br><span class="line">systemLog:</span><br><span class="line">   destination: file</span><br><span class="line">   path: &quot;&#x2F;home&#x2F;mongodb&#x2F;log&#x2F;mongod.log&quot;</span><br><span class="line">   logAppend: true</span><br><span class="line">storage:</span><br><span class="line">   journal:</span><br><span class="line">      enabled: true</span><br><span class="line"></span><br><span class="line">#安全</span><br><span class="line">security:</span><br><span class="line">   authorization: enabled</span><br><span class="line">net:</span><br><span class="line">   bindIp: 192.168.1.226</span><br><span class="line">   port: 27017</span><br><span class="line"></span><br><span class="line">#副本集</span><br><span class="line">replication:</span><br><span class="line">   replSetName: set0</span><br><span class="line">#副本集安全</span><br><span class="line">security:</span><br><span class="line">   keyFile: &#x2F;home&#x2F;mongodb&#x2F;keyfile</span><br></pre></td></tr></table></figure>

<p><strong>分片配置</strong><br>分片服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">processManagement:</span><br><span class="line">   fork: true</span><br><span class="line">   pidFilePath: &#x2F;home&#x2F;mongodb&#x2F;mongodb.pid</span><br><span class="line">storage:</span><br><span class="line">   dbPath: &#x2F;home&#x2F;mongodb&#x2F;data</span><br><span class="line">systemLog:</span><br><span class="line">   destination: file</span><br><span class="line">   path: &quot;&#x2F;home&#x2F;mongodb&#x2F;log&#x2F;mongod.log&quot;</span><br><span class="line">   logAppend: true</span><br><span class="line">storage:</span><br><span class="line">   journal:</span><br><span class="line">      enabled: true</span><br><span class="line">net:</span><br><span class="line">   bindIp: 192.168.1.226</span><br><span class="line">   port: 27017</span><br><span class="line">sharding:</span><br><span class="line">   clusterRole: shardsvr</span><br><span class="line">replication:</span><br><span class="line">   replSetName: shardA</span><br></pre></td></tr></table></figure>

<p>配置服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">processManagement:</span><br><span class="line">   fork: true</span><br><span class="line">   pidFilePath: &#x2F;home&#x2F;mongodb&#x2F;mongodb.pid</span><br><span class="line">storage:</span><br><span class="line">   dbPath: &#x2F;home&#x2F;mongodb&#x2F;data</span><br><span class="line">systemLog:</span><br><span class="line">   destination: file</span><br><span class="line">   path: &quot;&#x2F;home&#x2F;mongodb&#x2F;log&#x2F;mongod.log&quot;</span><br><span class="line">   logAppend: true</span><br><span class="line">storage:</span><br><span class="line">   journal:</span><br><span class="line">      enabled: true</span><br><span class="line">sharding:</span><br><span class="line">    clusterRole: configsvr</span><br><span class="line">net:</span><br><span class="line">    bindIp: 192.168.1.226</span><br><span class="line">    port: 27001</span><br><span class="line">replication:</span><br><span class="line">    replSetName: csRS</span><br></pre></td></tr></table></figure>
<p>路由服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">processManagement:</span><br><span class="line">   fork: true</span><br><span class="line">   pidFilePath: &#x2F;home&#x2F;mongodb&#x2F;mongos.pid</span><br><span class="line">#storage:</span><br><span class="line">#   dbPath: &#x2F;home&#x2F;mongodb&#x2F;data</span><br><span class="line">systemLog:</span><br><span class="line">   destination: file</span><br><span class="line">   path: &quot;&#x2F;home&#x2F;mongodb&#x2F;log&#x2F;mongos.log&quot;</span><br><span class="line">   logAppend: true</span><br><span class="line">#storage:</span><br><span class="line">#   journal:</span><br><span class="line">#      enabled: true</span><br><span class="line">net:</span><br><span class="line">    bindIp: 192.168.1.226</span><br><span class="line">    port: 28001</span><br><span class="line">sharding:</span><br><span class="line">   configDB: csRS&#x2F;192.168.1.226:27001,192.168.1.226:27002,192.168.1.226:27003</span><br></pre></td></tr></table></figure>

<h3 id="启动mongodb"><a href="#启动mongodb" class="headerlink" title="启动mongodb"></a>启动mongodb</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f 分片服务器配置文件</span><br><span class="line">[root@object1 ~]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f 配置服务器配置文件</span><br><span class="line">[root@object1 ~]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongos -f 路由服务器配置文件</span><br></pre></td></tr></table></figure>

<h3 id="初始化mongodb"><a href="#初始化mongodb" class="headerlink" title="初始化mongodb"></a>初始化mongodb</h3><p>分片服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --port 27017</span><br><span class="line">&gt;use admin</span><br><span class="line">&gt;config &#x3D; &#123;_id:&quot;shardA&quot;,members:[</span><br><span class="line">&#123;_id:0, host:&quot;192.168.1.226:27017&quot;&#125;,</span><br><span class="line">&#123;_id:1, host:&quot;192.168.1.226:27018&quot;&#125;,</span><br><span class="line">&#123;_id:2, host:&quot;192.168.1.226:27019&quot;&#125;</span><br><span class="line">]&#125;</span><br><span class="line">&gt;rs.initiate(config)</span><br></pre></td></tr></table></figure>

<p>配置服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 ~]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --port 27001</span><br><span class="line">&gt;use admin</span><br><span class="line">&gt;config &#x3D; &#123;_id:&quot;csRS&quot;,configsvr:true,members:[</span><br><span class="line">&#123;_id:0, host:&quot;192.168.1.226:27001&quot;&#125;,</span><br><span class="line">&#123;_id:1, host:&quot;192.168.1.226:27002&quot;&#125;,</span><br><span class="line">&#123;_id:2, host:&quot;192.168.1.226:27003&quot;&#125;</span><br><span class="line">]&#125;</span><br><span class="line">&gt;rs.initiate(config)</span><br></pre></td></tr></table></figure>

<h3 id="启动分片"><a href="#启动分片" class="headerlink" title="启动分片"></a>启动分片</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@object1 mongodb]# mongo --port 28001</span><br><span class="line">mongos&gt; use admin</span><br><span class="line">mongos&gt; db.runCommand( &#123; addShard: &quot;shardA&#x2F;192.168.1.226:27017,192.168.1.226:27018,192.168.1.226:27019&quot;&#125; )</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title>Redhat6不能使用yum安装软件的解决</title>
    <url>/posts/9c285129.html</url>
    <content><![CDATA[<p>RHEL的YUM源需要注册用户才能更新使用，由于CentOS和RHEL基本没有区别，并且CentOS已经被REHL收购。所以将RHEL的YUM源替换为CentOS即可。</p>
<a id="more"></a>
<p>新安装的redhat使用yum时会出现如下如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost~]# yum repolist</span><br><span class="line">Loaded plugins: product-id, refresh-packagekit, security, subscription-manager</span><br><span class="line">This system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.</span><br></pre></td></tr></table></figure>

<h1 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h1><p>执行此脚本即可：(只对于Redhat6测试，具体方法后面)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -s https:&#x2F;&#x2F;www.hyman.shop&#x2F;sh&#x2F;redhat6-yum.sh |bash</span><br></pre></td></tr></table></figure>



<h2 id="清除原有RHEL的YUM及相关软件包。"><a href="#清除原有RHEL的YUM及相关软件包。" class="headerlink" title="清除原有RHEL的YUM及相关软件包。"></a>清除原有RHEL的YUM及相关软件包。</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost~]# rpm -qa | grep yum | xargs rpm -e --nodeps</span><br><span class="line">[root@localhost~]# rpm -qa |grep python-urlgrabber|xargs rpm -e --nodeps</span><br></pre></td></tr></table></figure>
<h2 id="下载centos6的相关软件包。"><a href="#下载centos6的相关软件包。" class="headerlink" title="下载centos6的相关软件包。"></a>下载centos6的相关软件包。</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost~]# wget http:&#x2F;&#x2F;mirrors.163.com&#x2F;centos&#x2F;6&#x2F;os&#x2F;x86_64&#x2F;Packages&#x2F;yum-3.2.29-81.el6.centos.noarch.rpm</span><br><span class="line">[root@localhost~]# wget http:&#x2F;&#x2F;mirrors.163.com&#x2F;centos&#x2F;6&#x2F;os&#x2F;x86_64&#x2F;Packages&#x2F;yum-metadata-parser-1.1.2-16.el6.x86_64.rpm</span><br><span class="line">[root@localhost~]# wget http:&#x2F;&#x2F;mirrors.163.com&#x2F;centos&#x2F;6&#x2F;os&#x2F;x86_64&#x2F;Packages&#x2F;yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm</span><br><span class="line">[root@localhost~]# wget http:&#x2F;&#x2F;mirrors.163.com&#x2F;centos&#x2F;6&#x2F;os&#x2F;x86_64&#x2F;Packages&#x2F;python-iniparse-0.3.1-2.1.el6.noarch.rpm</span><br><span class="line">[root@localhost~]# wget http:&#x2F;&#x2F;mirrors.163.com&#x2F;centos&#x2F;6&#x2F;os&#x2F;x86_64&#x2F;Packages&#x2F;python-urlgrabber-3.9.1-11.el6.noarch.rpm</span><br></pre></td></tr></table></figure>
<h2 id="安装软件包。"><a href="#安装软件包。" class="headerlink" title="安装软件包。"></a>安装软件包。</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost~]# rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm</span><br><span class="line">[root@localhost~]# rpm -ivh python-urlgrabber-3.9.1-11.el6.noarch.rpm</span><br><span class="line">[root@localhost~]# rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm</span><br><span class="line">[root@localhost~]# rpm -ivh yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm yum-3.2.29-81.el6.centos.noarch.rpm</span><br><span class="line">*注释yum-plugin-fastestmirror和yum-3.2.29要一起安装。</span><br></pre></td></tr></table></figure>
<h2 id="编辑文件"><a href="#编辑文件" class="headerlink" title="编辑文件"></a>编辑文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;mirrors.163.com&#x2F;.help&#x2F;CentOS6-Base-163.repo</span><br><span class="line">[root@localhost~]# vim &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo</span><br><span class="line"></span><br><span class="line">把$releasever替换成现有系统的版cd本号(6)</span><br><span class="line">vim替换命令</span><br><span class="line">：%s&#x2F;$releasever&#x2F;6&#x2F;g</span><br><span class="line">退出保存</span><br></pre></td></tr></table></figure>
<h2 id="清理yum缓存"><a href="#清理yum缓存" class="headerlink" title="清理yum缓存"></a>清理yum缓存</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost~]# yum clean all</span><br></pre></td></tr></table></figure>
<h2 id="将服务器软件包信息缓存至本地，提高搜索安装效率"><a href="#将服务器软件包信息缓存至本地，提高搜索安装效率" class="headerlink" title="将服务器软件包信息缓存至本地，提高搜索安装效率"></a>将服务器软件包信息缓存至本地，提高搜索安装效率</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost~]# yum makecache</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>yum</tag>
        <tag>redhat</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis学习笔记--Redis数据过期策略详解</title>
    <url>/posts/1a55ce53.html</url>
    <content><![CDATA[<h1 id="设置过期时间"><a href="#设置过期时间" class="headerlink" title="设置过期时间"></a>设置过期时间</h1><p>Redis对存储值的过期处理实际上是针对该值的键（key）处理的，即时间的设置也是设置key的有效时间。Expires字典保存了所有键的过期时间，Expires也被称为过期字段。</p>
<a id="more"></a>
<ul>
<li>expire key time(以秒为单位)–这是最常用的方式</li>
<li>setex(String key, int seconds, String value)–字符串独有的方式</li>
</ul>
<p>注：<br>　　1、除了字符串自己独有设置过期时间的方法外，其他方法都需要依靠expire方法来设置时间<br>　　2、如果没有设置时间，那缓存就是永不过期<br>　　3、如果设置了过期时间，之后又想让缓存永不过期，使用persist key</p>
<h2 id="常用方式"><a href="#常用方式" class="headerlink" title="常用方式"></a>常用方式</h2><p>一般主要包括4种处理过期方，其中expire都是以秒为单位，pexpire都是以毫秒为单位的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 EXPIRE key seconds　　&#x2F;&#x2F;将key的生存时间设置为ttl秒</span><br><span class="line">2 PEXPIRE key milliseconds　　&#x2F;&#x2F;将key的生成时间设置为ttl毫秒</span><br><span class="line">3 EXPIREAT key timestamp　　&#x2F;&#x2F;将key的过期时间设置为timestamp所代表的的秒数的时间戳</span><br><span class="line">4 PEXPIREAT key milliseconds-timestamp　　&#x2F;&#x2F;将key的过期时间设置为timestamp所代表的的毫秒数的时间戳</span><br></pre></td></tr></table></figure>

<p>备注：</p>
<p>1、timestamp为unix时间戳（例如：timestamp=1499788800 表示将在2017.07.12过期）<br>2、2两种方式是设置一个过期的时间段，就是咱们处理验证码最常用的策略，设置三分钟或五分钟后失效，把分钟数转换成秒或毫秒存储到Redis中。<br>3、4两种方式是指定一个过期的时间 ，比如优惠券的过期时间是某年某月某日，只是单位不一样。</p>
<p>下面我们就以EXPIREAT为例子简单讲解下用法。</p>
<h3 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h3><p>一个整数值1或0，如下：</p>
<ul>
<li>如果成功地为该键设置了超时时间，返回 1</li>
<li>如果键不存在或无法设置超时时间，返回 0</li>
</ul>
<h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><p>以下是以Redis的EXPIREAT命令的基本语法。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis 127.0.0.1:6379&gt; Expireat KEY_NAME TIME_IN_UNIX_TIMESTAMP</span><br></pre></td></tr></table></figure>

<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>首先，在Redis中创建一个键：akey，并在akey中设置一些值。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis 127.0.0.1:6379&gt; SET akey redis</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>现在，为设置创建的键设置超时时间为60 秒。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; SET akey redis</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; EXPIREAT akey 1393840000</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; EXISTS akey</span><br><span class="line">(integer) 0</span><br><span class="line">127.0.0.1:6379&gt; SET akey redis</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; EXPIREAT akey 1493840000</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; EXISTS akey</span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure>

<h2 id="字符串独有方式"><a href="#字符串独有方式" class="headerlink" title="字符串独有方式"></a>字符串独有方式</h2><p>对字符串特殊处理的方式为SETEX命令，SETEX命令为指定的 key 设置值及其过期时间。如果 key 已经存在， SETEX 命令将会替换旧的值。</p>
<h3 id="返回值-1"><a href="#返回值-1" class="headerlink" title="返回值"></a>返回值</h3><p>设置成功时返回 OK 。</p>
<h3 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h3><p>Redis Setex 命令基本语法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis 127.0.0.1:6379&gt; SETEX KEY_NAME TIMEOUT VALUE</span><br></pre></td></tr></table></figure>

<h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis 127.0.0.1:6379&gt; SETEX mykey 60 redis</span><br><span class="line">OK</span><br><span class="line">redis 127.0.0.1:6379&gt; TTL mykey</span><br><span class="line">60</span><br><span class="line">redis 127.0.0.1:6379&gt; GET mykey</span><br><span class="line">&quot;redis&quot;</span><br></pre></td></tr></table></figure>

<h1 id="3种过期策略"><a href="#3种过期策略" class="headerlink" title="3种过期策略"></a>3种过期策略</h1><h2 id="定时删除"><a href="#定时删除" class="headerlink" title="定时删除"></a>定时删除</h2><h3 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h3><p>在设置key的过期时间的同时，为该key创建一个定时器，让定时器在key的过期时间来临时，对key进行删除</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>保证内存被尽快释放</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li><p>若过期key很多，删除这些key会占用很多的CPU时间，在CPU时间紧张的情况下，CPU不能把所有的时间用来做要紧的事儿，还需要去花时间删除这些key</p>
</li>
<li><p>定时器的创建耗时，若为每一个设置过期时间的key创建一个定时器（将会有大量的定时器产生），性能影响严重<br>没人用</p>
</li>
</ul>
<h2 id="惰性删除"><a href="#惰性删除" class="headerlink" title="惰性删除"></a>惰性删除</h2><h3 id="含义-1"><a href="#含义-1" class="headerlink" title="含义"></a>含义</h3><p>key过期的时候不删除，每次从数据库获取key的时候去检查是否过期，若过期，则删除，返回null。</p>
<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><p>删除操作只发生在从数据库取出key的时候发生，而且只删除当前key，所以对CPU时间的占用是比较少的，而且此时的删除是已经到了非做不可的地步（如果此时还不删除的话，我们就会获取到了已经过期的key了）</p>
<h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><p>若大量的key在超出超时时间后，很久一段时间内，都没有被获取过，那么可能发生内存泄露（无用的垃圾占用了大量的内存）</p>
<h2 id="定期删除"><a href="#定期删除" class="headerlink" title="定期删除"></a>定期删除</h2><h3 id="含义-2"><a href="#含义-2" class="headerlink" title="含义"></a>含义</h3><p>每隔一段时间执行一次删除(在redis.conf配置文件设置hz，1s刷新的频率)过期key操作</p>
<h3 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h3><ul>
<li>通过限制删除操作的时长和频率，来减少删除操作对CPU时间的占用–处理”定时删除”的缺点</li>
<li>定期删除过期key–处理”惰性删除”的缺点</li>
</ul>
<h3 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>在内存友好方面，不如”定时删除”</li>
<li>在CPU时间友好方面，不如”惰性删除”</li>
</ul>
<h3 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h3><ul>
<li><p>合理设置删除操作的执行时长（每次删除执行多长时间）和执行频率（每隔多长时间做一次删除）（这个要根据服务器运行情况来定了）</p>
</li>
<li><p>定时删除和定期删除为主动删除：Redis会定期主动淘汰一批已过去的key</p>
</li>
<li><p>惰性删除为被动删除：用到的时候才会去检验key是不是已过期，过期就删除</p>
</li>
<li><p>惰性删除为redis服务器内置策略</p>
</li>
<li><p>定期删除可以通过：</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一、配置redis.conf 的hz选项，默认为10 （即1秒执行10次，100ms一次，值越大说明刷新频率越快，最Redis性能损耗也越大） </span><br><span class="line">第二、配置redis.conf的maxmemory最大值，当已用内存超过maxmemory限定时，就会触发主动清理策略</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">上边所说的数据库指的是内存数据库，默认情况下每一台redis服务器有16个数据库（关于数据库的设置，看下边代码），默认使用0号数据库，所有的操作都是对0号数据库的操作</span><br><span class="line"># 设置数据库数量。默认为16个库，默认使用DB 0，可以使用&quot;select 1&quot;来选择一号数据库</span><br><span class="line"># 注意：由于默认使用0号数据库，那么我们所做的所有的缓存操作都存在0号数据库上，</span><br><span class="line"># 当你在1号数据库上去查找的时候，就查不到之前set过得缓存</span><br><span class="line"># 若想将0号数据库上的缓存移动到1号数据库，可以使用&quot;move key 1&quot;</span><br><span class="line">databases 16</span><br></pre></td></tr></table></figure>

<p>memcached只是用了惰性删除，而Redis同时使用了惰性删除与定期删除，这也是二者的一个不同点（可以看做是redis优于memcached的一点）<br>对于惰性删除而言，并不是只有获取key的时候才会检查key是否过期，在某些设置key的方法上也会检查（eg.setnx key2 value2：该方法类似于memcached的add方法，如果设置的key2已经存在，那么该方法返回false，什么都不做；如果设置的key2不存在，那么该方法设置缓存key2-value2。假设调用此方法的时候，发现redis中已经存在了key2，但是该key2已经过期了，如果此时不执行删除操作的话，setnx方法将会直接返回false，也就是说此时并没有重新设置key2-value2成功，所以对于一定要在setnx执行之前，对key2进行过期检查）</p>
<h1 id="Redis采用的过期策略"><a href="#Redis采用的过期策略" class="headerlink" title="Redis采用的过期策略"></a>Redis采用的过期策略</h1><h2 id="惰性删除-定期删除"><a href="#惰性删除-定期删除" class="headerlink" title="惰性删除+定期删除"></a>惰性删除+定期删除</h2><h3 id="惰性删除流程"><a href="#惰性删除流程" class="headerlink" title="惰性删除流程"></a>惰性删除流程</h3><ul>
<li>在进行get或setnx等操作时，先检查key是否过期，</li>
<li>若过期，删除key，然后执行相应操作；</li>
<li>若没过期，直接执行相应操作</li>
</ul>
<h3 id="定期删除流程（简单而言，对指定个数个库的每一个库随机删除小于等于指定个数个过期key）"><a href="#定期删除流程（简单而言，对指定个数个库的每一个库随机删除小于等于指定个数个过期key）" class="headerlink" title="定期删除流程（简单而言，对指定个数个库的每一个库随机删除小于等于指定个数个过期key）"></a>定期删除流程（简单而言，对指定个数个库的每一个库随机删除小于等于指定个数个过期key）</h3><ul>
<li>遍历每个数据库（就是redis.conf中配置的”database”数量，默认为16）</li>
<li>检查当前库中的指定个数个key（默认是每个库检查20个key，注意相当于该循环执行20次，循环体时下边的描述）</li>
<li>如果当前库中没有一个key设置了过期时间，直接执行下一个库的遍历</li>
<li>随机获取一个设置了过期时间的key，检查该key是否过期，如果过期，删除key</li>
<li>判断定期删除操作是否已经达到指定时长，若已经达到，直接退出定期删除。</li>
</ul>
<h1 id="RDB对过期key的处理"><a href="#RDB对过期key的处理" class="headerlink" title="RDB对过期key的处理"></a>RDB对过期key的处理</h1><p>过期key对RDB没有任何影响</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">从内存数据库持久化数据到RDB文件</span><br><span class="line">持久化key之前，会检查是否过期，过期的key不进入RDB文件</span><br><span class="line">从RDB文件恢复数据到内存数据库</span><br><span class="line">数据载入数据库之前，会对key先进行过期检查，如果过期，不导入数据库（主库情况）</span><br></pre></td></tr></table></figure>

<h1 id="AOF对过期key的处理"><a href="#AOF对过期key的处理" class="headerlink" title="AOF对过期key的处理"></a>AOF对过期key的处理</h1><p>过期key对AOF没有任何影响</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">从内存数据库持久化数据到AOF文件：</span><br><span class="line">当key过期后，还没有被删除，此时进行执行持久化操作（该key是不会进入aof文件的，因为没有发生修改命令）</span><br><span class="line">当key过期后，在发生删除操作时，程序会向aof文件追加一条del命令（在将来的以aof文件恢复数据的时候该过期的键就会被删掉）</span><br><span class="line">AOF重写</span><br><span class="line">重写时，会先判断key是否过期，已过期的key不会重写到aof文件</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis学习笔记--Redis配置文件redis.conf参数配置详解</title>
    <url>/posts/d0443e91.html</url>
    <content><![CDATA[<p>Redis配置文件redis.conf参数配置详解</p>
<a id="more"></a> 
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">########################################## 常规 ##########################################</span><br><span class="line"></span><br><span class="line">daemonize no</span><br><span class="line"># Redis默认是不作为守护进程来运行的。你可以把这个设置为&quot;yes&quot;让它作为守护进程来运行。</span><br><span class="line"># 注意，当作为守护进程的时候，Redis会把进程ID写到 &#x2F;var&#x2F;run&#x2F;redis.pid</span><br><span class="line"></span><br><span class="line">pidfile &#x2F;var&#x2F;run&#x2F;redis.pid</span><br><span class="line"># 当以守护进程方式运行的时候，Redis会把进程ID默认写到 &#x2F;var&#x2F;run&#x2F;redis.pid。你可以在这里修改路径。</span><br><span class="line"></span><br><span class="line">port 6379</span><br><span class="line"># 接受连接的特定端口，默认是6379。</span><br><span class="line"># 如果端口设置为0，Redis就不会监听TCP套接字。</span><br><span class="line"></span><br><span class="line">bind 127.0.0.1</span><br><span class="line"># 如果你想的话，你可以绑定单一接口；如果这里没单独设置，那么所有接口的连接都会被监听。</span><br><span class="line"></span><br><span class="line">unixsocket &#x2F;tmp&#x2F;redis.sock</span><br><span class="line"># unix指定监听socket,指定用来监听连接的unxi套接字的路径。这个没有默认值，所以如果你不指定的话，Redis就不会通过unix套接字来监听。</span><br><span class="line"></span><br><span class="line">unixsocketperm 755</span><br><span class="line">#当指定监听为socket时，可以指定其权限为755</span><br><span class="line"></span><br><span class="line">timeout 0</span><br><span class="line">#一个客户端空闲多少秒后关闭连接。(0代表禁用，永不关闭)</span><br><span class="line"></span><br><span class="line">loglevel verbose</span><br><span class="line"># 设置服务器调试等级。</span><br><span class="line"># 可能值：</span><br><span class="line"># debug （很多信息，对开发&#x2F;测试有用）</span><br><span class="line"># verbose （很多精简的有用信息，但是不像debug等级那么多）</span><br><span class="line"># notice （适量的信息，基本上是你生产环境中需要的程度）</span><br><span class="line"># warning （只有很重要&#x2F;严重的信息会记录下来）</span><br><span class="line"></span><br><span class="line">logfile stdout</span><br><span class="line"># 指明日志文件名。也可以使用&quot;stdout&quot;来强制让Redis把日志信息写到标准输出上。</span><br><span class="line"># 注意：如果Redis以守护进程方式运行，而你设置日志显示到标准输出的话，那么日志会发送到 &#x2F;dev&#x2F;null</span><br><span class="line"></span><br><span class="line">syslog-enabled no</span><br><span class="line"># 要使用系统日志记录器很简单，只要设置 &quot;syslog-enabled&quot; 为 &quot;yes&quot; 就可以了。</span><br><span class="line"># 然后根据需要设置其他一些syslog参数就可以了。</span><br><span class="line"></span><br><span class="line">syslog-ident redis</span><br><span class="line"># 指明syslog身份</span><br><span class="line"></span><br><span class="line">syslog-facility local0</span><br><span class="line"># 指明syslog的设备。必须是一个用户或者是 LOCAL0 ~ LOCAL7 之一</span><br><span class="line"></span><br><span class="line">databases 16</span><br><span class="line"># 设置数据库个数。默认数据库是 DB 0，你可以通过SELECT &lt;dbid&gt; WHERE dbid（0～&#39;databases&#39; - 1）来为每个连接使用不同的数据库。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">########################################## 快照 ##########################################</span><br><span class="line"></span><br><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000</span><br><span class="line"># 把数据库存到磁盘上:</span><br><span class="line"># save &lt;seconds&gt; &lt;changes&gt;</span><br><span class="line"># 会在指定秒数和数据变化次数之后把数据库写到磁盘上。</span><br><span class="line">#</span><br><span class="line"># 下面的例子将会进行把数据写入磁盘的操作:</span><br><span class="line"># 900秒（15分钟）之后，且至少1次变更</span><br><span class="line"># 300秒（5分钟）之后，且至少10次变更</span><br><span class="line"># 60秒之后，且至少10000次变更</span><br><span class="line">#</span><br><span class="line"># 注意：你要想不写磁盘的话就把所有 &quot;save&quot; 设置注释掉就行了。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdbcompression yes</span><br><span class="line"></span><br><span class="line"># 当导出到 .rdb 数据库时是否用LZF压缩字符串对象。</span><br><span class="line"># 默认设置为 &quot;yes&quot;，所以几乎总是生效的。</span><br><span class="line"># 如果你想节省CPU的话你可以把这个设置为 &quot;no&quot;，但是如果你有可压缩的key的话，那数据文件就会更大了。</span><br><span class="line"></span><br><span class="line">dbfilename dump.rdb</span><br><span class="line"># 数据库的文件名</span><br><span class="line"></span><br><span class="line">dir .&#x2F;</span><br><span class="line"># 工作目录</span><br><span class="line"># 数据库会写到这个目录下，文件名就是上面的 &quot;dbfilename&quot; 的值。</span><br><span class="line"># 累加文件也放这里。</span><br><span class="line"># 注意你这里指定的必须是目录，不是文件名。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">########################################## 同步 ##########################################</span><br><span class="line"></span><br><span class="line">slaveof &lt;masterip&gt; &lt;masterport&gt;</span><br><span class="line"># 主从同步。通过 slaveof 配置来实现Redis实例的备份。</span><br><span class="line"># 注意，这里是本地从远端复制数据。也就是说，本地可以有不同的数据库文件、绑定不同的IP、监听不同的端口。</span><br><span class="line"></span><br><span class="line">masterauth &lt;master-password&gt;</span><br><span class="line"># 如果master设置了密码（通过下面的 &quot;requirepass&quot; 选项来配置），那么slave在开始同步之前必须进行身份验证，否则它的同步请求会被拒绝。</span><br><span class="line"></span><br><span class="line">slave-serve-stale-data yes</span><br><span class="line"># 当一个slave失去和master的连接，或者同步正在进行中，slave的行为有两种可能：</span><br><span class="line">#</span><br><span class="line"># 1) 如果 slave-serve-stale-data 设置为 &quot;yes&quot; (默认值)，slave会继续响应客户端请求，可能是正常数据，也可能是还没获得值的空数据。</span><br><span class="line"># 2) 如果 slave-serve-stale-data 设置为 &quot;no&quot;，slave会回复&quot;正在从master同步（SYNC with master in progress）&quot;来处理各种请求，除了 INFO 和 SLAVEOF 命令。</span><br><span class="line"></span><br><span class="line">repl-ping-slave-period 10</span><br><span class="line"># slave根据指定的时间间隔向服务器发送ping请求。</span><br><span class="line"># 时间间隔可以通过 repl_ping_slave_period 来设置。</span><br><span class="line"># 默认10。</span><br><span class="line"></span><br><span class="line">repl-timeout 60</span><br><span class="line"># 下面的选项设置了大块数据I&#x2F;O、向master请求数据和ping响应的过期时间。</span><br><span class="line"># 默认值60秒。</span><br><span class="line"># 一个很重要的事情是：确保这个值比 repl-ping-slave-period 大，否则master和slave之间的传输过期时间比预想的要短。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">########################################## 安全 ##########################################</span><br><span class="line"></span><br><span class="line">requirepass foobared</span><br><span class="line"># 要求客户端在处理任何命令时都要验证身份和密码。</span><br><span class="line"># 这在你信不过来访者时很有用。</span><br><span class="line"># 为了向后兼容的话，这段应该注释掉。而且大多数人不需要身份验证（例如：它们运行在自己的服务器上。）</span><br><span class="line"># 警告：因为Redis太快了，所以居心不良的人可以每秒尝试150k的密码来试图破解密码。</span><br><span class="line"># 这意味着你需要一个高强度的密码，否则破解太容易了。</span><br><span class="line"></span><br><span class="line">rename-command CONFIG &quot;&quot;</span><br><span class="line"># 命令重命名</span><br><span class="line"># 在共享环境下，可以为危险命令改变名字。比如，你可以为 CONFIG 改个其他不太容易猜到的名字，这样你自己仍然可以使用，而别人却没法做坏事了。</span><br><span class="line"># 例如:</span><br><span class="line"># rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52</span><br><span class="line"># 甚至也可以通过给命令赋值一个空字符串来完全禁用这条命令：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">################################### 限制 ####################################</span><br><span class="line"></span><br><span class="line">maxclients 128</span><br><span class="line"># 设置最多同时连接客户端数量。</span><br><span class="line"># 默认没有限制，这个关系到Redis进程能够打开的文件描述符数量。</span><br><span class="line"># 特殊值&quot;0&quot;表示没有限制。</span><br><span class="line"># 一旦达到这个限制，Redis会关闭所有新连接并发送错误&quot;达到最大用户数上限（max number of clients reached）&quot;</span><br><span class="line"></span><br><span class="line">maxmemory &lt;bytes&gt;</span><br><span class="line"># 不要用比设置的上限更多的内存。一旦内存使用达到上限，Redis会根据选定的回收策略（参见：maxmemmory-policy）删除key。</span><br><span class="line"># 如果因为删除策略问题Redis无法删除key，或者策略设置为 &quot;noeviction&quot;，Redis会回复需要更多内存的错误信息给命令。</span><br><span class="line"># 例如，SET,LPUSH等等。但是会继续合理响应只读命令，比如：GET。</span><br><span class="line"># 在使用Redis作为LRU缓存，或者为实例设置了硬性内存限制的时候（使用 &quot;noeviction&quot; 策略）的时候，这个选项还是满有用的。</span><br><span class="line"># 警告：当一堆slave连上达到内存上限的实例的时候，响应slave需要的输出缓存所需内存不计算在使用内存当中。</span><br><span class="line"># 这样当请求一个删除掉的key的时候就不会触发网络问题／重新同步的事件，然后slave就会收到一堆删除指令，直到数据库空了为止。</span><br><span class="line"># 简而言之，如果你有slave连上一个master的话，那建议你把master内存限制设小点儿，确保有足够的系统内存用作输出缓存。</span><br><span class="line"># （如果策略设置为&quot;noeviction&quot;的话就不无所谓了）</span><br><span class="line"></span><br><span class="line">maxmemory-policy volatile-lru</span><br><span class="line"># 内存策略：如果达到内存限制了，Redis如何删除key。你可以在下面五个策略里面选：</span><br><span class="line">#</span><br><span class="line"># volatile-lru -&gt; 根据LRU算法生成的过期时间来删除。</span><br><span class="line"># allkeys-lru -&gt; 根据LRU算法删除任何key。</span><br><span class="line"># volatile-random -&gt; 根据过期设置来随机删除key。</span><br><span class="line"># allkeys-&gt;random -&gt; 无差别随机删。</span><br><span class="line"># volatile-ttl -&gt; 根据最近过期时间来删除（辅以TTL）</span><br><span class="line"># noeviction -&gt; 谁也不删，直接在写操作时返回错误。</span><br><span class="line">#</span><br><span class="line"># 注意：对所有策略来说，如果Redis找不到合适的可以删除的key都会在写操作时返回一个错误。</span><br><span class="line">#</span><br><span class="line"># 这里涉及的命令：set setnx setex append</span><br><span class="line"># incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd</span><br><span class="line"># sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby</span><br><span class="line"># zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby</span><br><span class="line"># getset mset msetnx exec sort</span><br><span class="line">#</span><br><span class="line"># 默认值如下：</span><br><span class="line"></span><br><span class="line">maxmemory-samples 3</span><br><span class="line"># LRU和最小TTL算法的实现都不是很精确，但是很接近（为了省内存），所以你可以用样例做测试。</span><br><span class="line"># 例如：默认Redis会检查三个key然后取最旧的那个，你可以通过下面的配置项来设置样本的个数。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">############################## 纯累加模式 ###############################</span><br><span class="line"></span><br><span class="line">appendonly no</span><br><span class="line"># 默认情况下，Redis是异步的把数据导出到磁盘上。这种情况下，当Redis挂掉的时候，最新的数据就丢了。</span><br><span class="line"># 如果不希望丢掉任何一条数据的话就该用纯累加模式：一旦开启这个模式，Redis会把每次写入的数据在接收后都写入 appendonly.aof 文件。</span><br><span class="line"># 每次启动时Redis都会把这个文件的数据读入内存里。</span><br><span class="line">#</span><br><span class="line"># 注意，异步导出的数据库文件和纯累加文件可以并存（你得把上面所有&quot;save&quot;设置都注释掉，关掉导出机制）。</span><br><span class="line"># 如果纯累加模式开启了，那么Redis会在启动时载入日志文件而忽略导出的 dump.rdb 文件。</span><br><span class="line">#</span><br><span class="line"># 重要：查看 BGREWRITEAOF 来了解当累加日志文件太大了之后，怎么在后台重新处理这个日志文件。</span><br><span class="line"></span><br><span class="line">appendfilename appendonly.aof</span><br><span class="line"># 纯累加文件名字（默认：&quot;appendonly.aof&quot;）</span><br><span class="line"></span><br><span class="line">appendfsync always</span><br><span class="line">appendfsync everysec</span><br><span class="line">appendfsync no</span><br><span class="line"># fsync() 请求操作系统马上把数据写到磁盘上，不要再等了。</span><br><span class="line"># 有些操作系统会真的把数据马上刷到磁盘上；有些则要磨蹭一下，但是会尽快去做。</span><br><span class="line"># Redis支持三种不同的模式：</span><br><span class="line"># no：不要立刻刷，只有在操作系统需要刷的时候再刷。比较快。</span><br><span class="line"># always：每次写操作都立刻写入到aof文件。慢，但是最安全。</span><br><span class="line"># everysec：每秒写一次。折衷方案。</span><br><span class="line"># 默认的 &quot;everysec&quot; 通常来说能在速度和数据安全性之间取得比较好的平衡。</span><br><span class="line"># 如果你真的理解了这个意味着什么，那么设置&quot;no&quot;可以获得更好的性能表现（如果丢数据的话，则只能拿到一个不是很新的快照）；</span><br><span class="line"># 或者相反的，你选择 &quot;always&quot; 来牺牲速度确保数据安全、完整。</span><br><span class="line"># 如果拿不准，就用 &quot;everysec&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">no-appendfsync-on-rewrite no</span><br><span class="line"># 如果AOF的同步策略设置成 &quot;always&quot; 或者 &quot;everysec&quot;，那么后台的存储进程（后台存储或写入AOF日志）会产生很多磁盘I&#x2F;O开销。</span><br><span class="line"># 某些Linux的配置下会使Redis因为 fsync() 而阻塞很久。</span><br><span class="line"># 注意，目前对这个情况还没有完美修正，甚至不同线程的 fsync() 会阻塞我们的 write(2) 请求。</span><br><span class="line">#</span><br><span class="line"># 为了缓解这个问题，可以用下面这个选项。它可以在 BGSAVE 或 BGREWRITEAOF 处理时阻止 fsync()。</span><br><span class="line">#</span><br><span class="line"># 这就意味着如果有子进程在进行保存操作，那么Redis就处于&quot;不可同步&quot;的状态。</span><br><span class="line"># 这实际上是说，在最差的情况下可能会丢掉30秒钟的日志数据。（默认Linux设定）</span><br><span class="line">#</span><br><span class="line"># 如果你有延迟的问题那就把这个设为 &quot;yes&quot;，否则就保持 &quot;no&quot;，这是保存持久数据的最安全的方式。</span><br><span class="line"></span><br><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br><span class="line"># 自动重写AOF文件</span><br><span class="line"># 如果AOF日志文件大到指定百分比，Redis能够通过 BGREWRITEAOF 自动重写AOF日志文件。</span><br><span class="line">#</span><br><span class="line"># 工作原理：Redis记住上次重写时AOF日志的大小（或者重启后没有写操作的话，那就直接用此时的AOF文件），</span><br><span class="line"># 基准尺寸和当前尺寸做比较。如果当前尺寸超过指定比例，就会触发重写操作。</span><br><span class="line">#</span><br><span class="line"># 你还需要指定被重写日志的最小尺寸，这样避免了达到约定百分比但尺寸仍然很小的情况还要重写。</span><br><span class="line"># 指定百分比为0会禁用AOF自动重写特性。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">################################## 慢查询日志 ###################################</span><br><span class="line"></span><br><span class="line">slowlog-log-slower-than 10000</span><br><span class="line"># Redis慢查询日志可以记录超过指定时间的查询。运行时间不包括各种I&#x2F;O时间。</span><br><span class="line"># 例如：连接客户端，发送响应数据等。只计算命令运行的实际时间（这是唯一一种命令运行线程阻塞而无法同时为其他请求服务的场景）</span><br><span class="line">#</span><br><span class="line"># 你可以为慢查询日志配置两个参数：一个是超标时间，单位为微妙，记录超过个时间的命令。</span><br><span class="line"># 另一个是慢查询日志长度。当一个新的命令被写进日志的时候，最老的那个记录会被删掉。</span><br><span class="line">#</span><br><span class="line"># 下面的时间单位是微秒，所以1000000就是1秒。注意，负数时间会禁用慢查询日志，而0则会强制记录所有命令。</span><br><span class="line"></span><br><span class="line">slowlog-max-len 128</span><br><span class="line"># 这个长度没有限制。只要有足够的内存就行。你可以通过 SLOWLOG RESET 来释放内存。（译者注：日志居然是在内存里的Orz）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">################################ 虚拟内存 ###############################</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 警告！虚拟内存在Redis 2.4是反对的。</span><br><span class="line">### 非常不鼓励使用虚拟内存！！</span><br><span class="line"></span><br><span class="line">vm-enabled no</span><br><span class="line">vm-enabled yes</span><br><span class="line"># 虚拟内存可以使Redis在内存不够的情况下仍然可以将所有数据序列保存在内存里。</span><br><span class="line"># 为了做到这一点，高频key会调到内存里，而低频key会转到交换文件里，就像操作系统使用内存页一样。</span><br><span class="line">#</span><br><span class="line"># 要使用虚拟内存，只要把 &quot;vm-enabled&quot; 设置为 &quot;yes&quot;，并根据需要设置下面三个虚拟内存参数就可以了。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">vm-swap-file &#x2F;tmp&#x2F;redis.swap</span><br><span class="line"># 这是交换文件的路径。估计你猜到了，交换文件不能在多个Redis实例之间共享，所以确保每个Redis实例使用一个独立交换文件。</span><br><span class="line"># 最好的保存交换文件（被随机访问）的介质是固态硬盘（SSD）。</span><br><span class="line"># *** 警告 *** 如果你使用共享主机，那么默认的交换文件放到 &#x2F;tmp 下是不安全的。</span><br><span class="line"># 创建一个Redis用户可写的目录，并配置Redis在这里创建交换文件。</span><br><span class="line"></span><br><span class="line">vm-max-memory 0</span><br><span class="line"># &quot;vm-max-memory&quot; 配置虚拟内存可用的最大内存容量。</span><br><span class="line"># 如果交换文件还有空间的话，所有超标部分都会放到交换文件里。</span><br><span class="line">#</span><br><span class="line"># &quot;vm-max-memory&quot; 设置为0表示系统会用掉所有可用内存。</span><br><span class="line"># 这默认值不咋地，只是把你能用的内存全用掉了，留点余量会更好。</span><br><span class="line"># 例如，设置为剩余内存的60%-80%</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">vm-page-size 32</span><br><span class="line"># Redis交换文件是分成多个数据页的。</span><br><span class="line"># 一个可存储对象可以被保存在多个连续页里，但是一个数据页无法被多个对象共享。</span><br><span class="line"># 所以，如果你的数据页太大，那么小对象就会浪费掉很多空间。</span><br><span class="line"># 如果数据页太小，那用于存储的交换空间就会更少（假定你设置相同的数据页数量）</span><br><span class="line">#</span><br><span class="line"># 如果你使用很多小对象，建议分页尺寸为64或32个字节。</span><br><span class="line"># 如果你使用很多大对象，那就用大一些的尺寸。</span><br><span class="line"># 如果不确定，那就用默认值 :)</span><br><span class="line"></span><br><span class="line">vm-pages 134217728</span><br><span class="line"># 交换文件里数据页总数。</span><br><span class="line"># 根据内存中分页表（已用&#x2F;未用的数据页分布情况），磁盘上每8个数据页会消耗内存里1个字节。</span><br><span class="line">#</span><br><span class="line"># 交换区容量 &#x3D; vm-page-size * vm-pages</span><br><span class="line">#</span><br><span class="line"># 根据默认的32字节的数据页尺寸和134217728的数据页数来算，Redis的数据页文件会占4GB，而内存里的分页表会消耗16MB内存。</span><br><span class="line">#</span><br><span class="line"># 为你的应验程序设置最小且够用的数字比较好，下面这个默认值在大多数情况下都是偏大的。</span><br><span class="line"></span><br><span class="line">vm-max-threads 4</span><br><span class="line"># 同时可运行的虚拟内存I&#x2F;O线程数。</span><br><span class="line"># 这些线程可以完成从交换文件进行数据读写的操作，也可以处理数据在内存与磁盘间的交互和编码&#x2F;解码处理。</span><br><span class="line"># 多一些线程可以一定程度上提高处理效率，虽然I&#x2F;O操作本身依赖于物理设备的限制，不会因为更多的线程而提高单次读写操作的效率。</span><br><span class="line">#</span><br><span class="line"># 特殊值0会关闭线程级I&#x2F;O，并会开启阻塞虚拟内存机制。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">############################### 高级配置 ###############################</span><br><span class="line"></span><br><span class="line">hash-max-zipmap-entries 512</span><br><span class="line">hash-max-zipmap-value 64</span><br><span class="line"># 当有大量数据时，适合用哈希编码（需要更多的内存），元素数量上限不能超过给定限制。</span><br><span class="line"># 你可以通过下面的选项来设定这些限制：</span><br><span class="line"></span><br><span class="line">list-max-ziplist-entries 512</span><br><span class="line">list-max-ziplist-value 64</span><br><span class="line"># 与哈希相类似，数据元素较少的情况下，可以用另一种方式来编码从而节省大量空间。</span><br><span class="line"># 这种方式只有在符合下面限制的时候才可以用：</span><br><span class="line"></span><br><span class="line">set-max-intset-entries 512</span><br><span class="line"># 还有这样一种特殊编码的情况：数据全是64位无符号整型数字构成的字符串。</span><br><span class="line"># 下面这个配置项就是用来限制这种情况下使用这种编码的最大上限的。</span><br><span class="line"></span><br><span class="line">zset-max-ziplist-entries 128</span><br><span class="line">zset-max-ziplist-value 64</span><br><span class="line"># 与第一、第二种情况相似，有序序列也可以用一种特别的编码方式来处理，可节省大量空间。</span><br><span class="line"># 这种编码只适合长度和元素都符合下面限制的有序序列：</span><br><span class="line"></span><br><span class="line">activerehashing yes</span><br><span class="line"># 哈希刷新，每100个CPU毫秒会拿出1个毫秒来刷新Redis的主哈希表（顶级键值映射表）。</span><br><span class="line"># redis所用的哈希表实现（见dict.c）采用延迟哈希刷新机制：你对一个哈希表操作越多，哈希刷新操作就越频繁；</span><br><span class="line"># 反之，如果服务器非常不活跃那么也就是用点内存保存哈希表而已。</span><br><span class="line">#</span><br><span class="line"># 默认是每秒钟进行10次哈希表刷新，用来刷新字典，然后尽快释放内存。</span><br><span class="line">#</span><br><span class="line"># 建议：</span><br><span class="line"># 如果你对延迟比较在意的话就用 &quot;activerehashing no&quot;，每个请求延迟2毫秒不太好嘛。</span><br><span class="line"># 如果你不太在意延迟而希望尽快释放内存的话就设置 &quot;activerehashing yes&quot;。</span><br><span class="line"></span><br><span class="line">client-output-buffer-limit normal 0 0 0</span><br><span class="line">client-output-buffer-limit slave 256mb 64mb 60</span><br><span class="line">client-output-buffer-limit pubsub 32mb 8mb 60</span><br><span class="line"># 客户端输出缓存限制强迫断开读取速度比较慢的客户端</span><br><span class="line"># 有三种类型的限制</span><br><span class="line"># normal -&gt; 正常的客户端包括监控客户端</span><br><span class="line"># slave -&gt; 从客户端</span><br><span class="line"># pubsub -&gt; 客户端至少订阅了一个频道或者模式</span><br><span class="line"># 客户端输出缓存限制语法如下（时间单位：秒）</span><br><span class="line"># client-output-buffer-limit &lt;类别&gt; &lt;强制限制&gt; &lt;软性限制&gt; &lt;软性时间&gt;</span><br><span class="line"># 达到强制限制缓存大小，立刻断开链接。</span><br><span class="line"># 达到软性限制，仍然会有软性时间大小的链接时间</span><br><span class="line"># 默认正常客户端无限制，只有请求后，异步客户端数据请求速度快于它能读取数据的速度</span><br><span class="line"># 订阅模式和主从客户端又默认限制，因为它们都接受推送。</span><br><span class="line"># 强制限制和软性限制都可以设置为0来禁用这个特性</span><br><span class="line"></span><br><span class="line">hz 10</span><br><span class="line"># 设置Redis后台任务执行频率，比如清除过期键任务。</span><br><span class="line"># 设置范围为1到500，默认为10.越大CPU消耗越大，延迟越小。</span><br><span class="line"># 建议不要超过100</span><br><span class="line"></span><br><span class="line">aof-rewrite-incremental-fsync yes</span><br><span class="line"># 当子进程重写AOF文件，以下选项开启时，AOF文件会每产生32M数据同步一次。</span><br><span class="line"># 这有助于更快写入文件到磁盘避免延迟</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">################################## 包含 ###################################</span><br><span class="line"></span><br><span class="line">include &#x2F;path&#x2F;to&#x2F;local.conf</span><br><span class="line">include &#x2F;path&#x2F;to&#x2F;other.conf</span><br><span class="line"># 包含一个或多个其他配置文件。</span><br><span class="line"># 这在你有标准配置模板但是每个redis服务器又需要个性设置的时候很有用。</span><br><span class="line"># 包含文件特性允许你引人其他配置文件，所以好好利用吧。</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>记录redis的RDB、AOF持久化</title>
    <url>/posts/b50fd2f7.html</url>
    <content><![CDATA[<p>Redis是个支持持久化的内存数据库，redis需要经常将内存中的数据同步到磁盘来保证持久化。</p>
<a id="more"></a>
<h3 id="快照（Snapshotting）默认持久化方式"><a href="#快照（Snapshotting）默认持久化方式" class="headerlink" title="快照（Snapshotting）默认持久化方式"></a>快照（Snapshotting）默认持久化方式</h3><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">snapshotting：       </span><br><span class="line">save 900 1          #900秒内如果超过1个key被修改，则发起快照保存</span><br><span class="line">save 300 10         #300秒内如果超过10个key被修改，则发起快照保存</span><br><span class="line">save 60 10000</span><br></pre></td></tr></table></figure>

<h4 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h4><ol>
<li><p>Redis使用fork函数复制一份当前进程（父进程）的副本（子进程）；</p>
</li>
<li><p>父进程继续接收并处理客户端发来的命令，而子进程开始将内存中的数据写入硬盘中的临时文件；</p>
</li>
<li><p>当子进程写入完所有数据后会用该临时文件替换旧的RDB文件，至此一次快照操作完成。
　　</p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4></li>
</ol>
<ul>
<li><p>RDB是个非常紧凑的文件，保存了redis在某个时间点上的数据集，使得我们可以通过定时备份RDB文件来实现Redis数据库备份和灾难恢复，也可以将其传送到其他的数据中心用于保存。</p>
</li>
<li><p>RDB可以最大化redis的性能，执行RDB持久化时只需要fork一个子进程，并由子进程进行持久化工作，父进程不需要处理任何磁盘I/O操作。</p>
</li>
<li><p>RDB在恢复大数据集时比AOF要快，启动效率要高许多。</p>
</li>
<li><p>RDB文件是经过压缩（可以配置rdbcompression参数以禁用压缩节省CPU占用）的二进制格式，所以占用的空间会小于内存中的数据大小，更加利于传输。</p>
</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li><p>每次快照持久化都是将内存数据完整写入到磁盘一次，并不是增量的只同步增数据。如果数据量大的话，而且写操作比较多，必然会引起大量的磁盘io操作，可能会严重影响性能。</p>
</li>
<li><p>快照方式是在一定间隔时间做一次的，所以如果redis意外down掉的话，就会丢失最后一次快照后的所有修改，有一些数据丢失的风险。</p>
</li>
<li><p>client的save通知redis做一次快照持久化不推荐。save操作是在主线程中保存快照的，由于redis是用一个主线程来处理所有client的请求，这种方式会阻塞所有client请求，所以不推荐使用。</p>
</li>
</ul>
<h3 id="日志追加方式（append-only-file）方式"><a href="#日志追加方式（append-only-file）方式" class="headerlink" title="日志追加方式（append-only file）方式"></a>日志追加方式（append-only file）方式</h3><h4 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aof：</span><br><span class="line">appendonly yes         #启动aof持久化方式有三种修改方式</span><br><span class="line">#appendfsync always    #收到写命令就立即写入到硬盘，效率最慢，但是保证完全持久化</span><br><span class="line">#appendfsync everysec  #每秒种就写入一次硬盘，在性能和持久化方面做了折中</span><br><span class="line">#appendfsync no        #完全依赖操作系统，性能最好，但是持久化没保证，不知道何时持久化</span><br></pre></td></tr></table></figure>

<h4 id="工作原理-1"><a href="#工作原理-1" class="headerlink" title="工作原理"></a>工作原理</h4><ol>
<li><p>redis调用fork ，现在有父子两个进程</p>
</li>
<li><p>子进程根据内存中的数据库快照，往临时文件中写入重建数据库状态的命令</p>
</li>
</ol>
<ol>
<li><p>父进程继续处理client请求，除了把写命令写入到原来的aof文件中。同时把收到的写命令缓存起来。这样就能保证如果子进程重写失败的话并不会出问题。</p>
</li>
<li><p>当子进程把快照内容写入已命令方式写到临时文件中后，子进程发信号通知父进程。然后父进程把缓存的写命令也写入到临时文件。</p>
</li>
<li><p>现在父进程可以使用临时文件替换老的aof文件，并重命名，后面收到的写命令也开始往新的aof文件中追加。</p>
</li>
</ol>
<h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ul>
<li><p>该机制可以带来更高的数据安全性，即数据持久性。</p>
</li>
<li><p>由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。</p>
</li>
<li><p>如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。</p>
</li>
<li><p>AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，也可以通过该文件完成数据的重建。</p>
</li>
</ul>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li><p>对于相同数量的数据集而言，AOF文件通常要大于RDB文件，持久化文件会变的越来越大。</p>
</li>
<li><p>根据同步策略的不同，AOF在运行效率上往往会慢于RDB。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis学习笔记--Redis一些问题详解</title>
    <url>/posts/1e369957.html</url>
    <content><![CDATA[<h1 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h1><h2 id="Redis的同步机制"><a href="#Redis的同步机制" class="headerlink" title="Redis的同步机制"></a>Redis的同步机制</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Redis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将RDB文件全量同步到复制节点，复制节点接受完成后将RDB镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。后续的增量数据通过AOF日志同步即可，有点类似数据库的binlog。</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="Redis有哪些数据结构"><a href="#Redis有哪些数据结构" class="headerlink" title="Redis有哪些数据结构"></a>Redis有哪些数据结构</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">String、Hash、List、Set、SortedSet、HyperLogLog、Geo、Pub&#x2F;Sub、Redis Module，像BloomFilter，RedisSearch，Redis-ML</span><br></pre></td></tr></table></figure>

<h2 id="Redis是怎么持久化的？服务主从数据怎么交互的？"><a href="#Redis是怎么持久化的？服务主从数据怎么交互的？" class="headerlink" title="Redis是怎么持久化的？服务主从数据怎么交互的？"></a>Redis是怎么持久化的？服务主从数据怎么交互的？</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RDB做镜像全量持久化，AOF做增量持久化。因为RDB会耗费较长时间，不够实时，在停机的时候会导致大量丢失数据，所以需要AOF来配合使用。在redis实例重启时，会使用RDB持久化文件重新构建内存，再使用AOF重放近期的操作指令来实现完整恢复重启之前的状态。</span><br><span class="line">这里很好理解，把RDB理解为一整个表全量的数据，AOF理解为每次操作的日志就好了，服务器重启的时候先把表的数据全部搞进去，但是他可能不完整，你再回放一下日志，数据不就完整了嘛。不过Redis本身的机制是 AOF持久化开启且存在AOF文件时，优先加载AOF文件；AOF关闭或者AOF文件不存在时，加载RDB文件；加载AOF&#x2F;RDB文件城后，Redis启动成功； AOF&#x2F;RDB文件存在错误时，Redis启动失败并打印错误信息</span><br></pre></td></tr></table></figure>

<h2 id="如果突然机器掉电会怎样"><a href="#如果突然机器掉电会怎样" class="headerlink" title="如果突然机器掉电会怎样"></a>如果突然机器掉电会怎样</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">取决于AOF日志sync属性的配置，如果不要求性能，在每条写指令时都sync一下磁盘，就不会丢失数据。但是在高性能的要求下每次都sync是不现实的，一般都使用定时sync，比如1s1次，这个时候最多就会丢失1s的数据。</span><br></pre></td></tr></table></figure>

<h2 id="RDB的原理"><a href="#RDB的原理" class="headerlink" title="RDB的原理"></a>RDB的原理</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fork是指redis通过创建子进程来进行RDB操作，cow指的是copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写脏的页面数据会逐渐和子进程分离开来。</span><br></pre></td></tr></table></figure>

<h2 id="为啥Redis那么快"><a href="#为啥Redis那么快" class="headerlink" title="为啥Redis那么快"></a>为啥Redis那么快</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Redis采用的是基于内存的采用的是单进程单线程模型的 KV 数据库，由C语言编写，官方提供的数据是可以达到100000+的QPS（每秒内查询次数）。</span><br><span class="line"></span><br><span class="line">完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。它的，数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)；</span><br><span class="line"></span><br><span class="line">数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的；</span><br><span class="line"></span><br><span class="line">采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；</span><br><span class="line"></span><br><span class="line">使用多路I&#x2F;O复用模型，非阻塞IO；</span><br><span class="line"></span><br><span class="line">使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；</span><br></pre></td></tr></table></figure>

<h2 id="啥是上下文切换"><a href="#啥是上下文切换" class="headerlink" title="啥是上下文切换"></a>啥是上下文切换</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">就好比你看一本英文书，你看到第十页发现有个单词不会读，你加了个书签，然后去查字典，过了一会你又回来继续从书签那里读，ok到目前为止没啥问题。</span><br><span class="line"></span><br><span class="line">如果是你一个人读肯定没啥问题，但是你去查的时候，别的小伙伴好奇你在看啥他就翻了一下你的书，然后溜了，哦豁，你再看的时候就发现书不是你看的那一页了。不知道到这里为止我有没有解释清楚，以及为啥会线程不安全，就是因为你一个人怎么看都没事，但是人多了换来换去的操作一本书数据就乱了。可能我的解释很粗糙，但是道理应该是一样的。</span><br></pre></td></tr></table></figure>

<h2 id="主从之间的数据怎么同步"><a href="#主从之间的数据怎么同步" class="headerlink" title="主从之间的数据怎么同步"></a>主从之间的数据怎么同步</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">启动一台slave 的时候，他会发送一个psync命令给master ，如果是这个slave第一次连接到master，他会触发一个全量复制。master就会启动一个线程，生成RDB快照，还会把新的写请求都缓存在内存中，RDB文件生成后，master会将这个RDB发送给slave的，slave拿到之后做的第一件事情就是写进本地的磁盘，然后加载进内存，然后master会把内存里面缓存的那些新命名都发给slave。</span><br><span class="line"></span><br><span class="line">###数据传输的时候断网了或者服务器挂了怎么办啊？</span><br><span class="line"></span><br><span class="line">传输过程中有什么网络问题啥的，会自动重连的，并且连接之后会把缺少的数据补上的。</span><br></pre></td></tr></table></figure>

<h2 id="内存淘汰机制"><a href="#内存淘汰机制" class="headerlink" title="内存淘汰机制"></a>内存淘汰机制</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">noeviction:返回错误当内存限制达到并且客户端尝试执行会让更多内存被使用的命令（大部分的写入指令，但DEL和几个例外）</span><br><span class="line"></span><br><span class="line">allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。</span><br><span class="line"></span><br><span class="line">volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放。</span><br><span class="line"></span><br><span class="line">allkeys-random: 回收随机的键使得新添加的数据有空间存放。</span><br><span class="line"></span><br><span class="line">volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。</span><br><span class="line"></span><br><span class="line">volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。</span><br></pre></td></tr></table></figure>

<h2 id="Redis-和-Memcached-有啥区别"><a href="#Redis-和-Memcached-有啥区别" class="headerlink" title="Redis 和 Memcached 有啥区别"></a>Redis 和 Memcached 有啥区别</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Memcache</span><br><span class="line">注意后面会把 Memcache 简称为 MC。</span><br><span class="line"></span><br><span class="line">先来看看 MC 的特点：</span><br><span class="line"></span><br><span class="line">- MC 处理请求时使用多线程异步 IO 的方式，可以合理利用 CPU 多核的优势，性能非常优秀；</span><br><span class="line">- MC 功能简单，使用内存存储数据；</span><br><span class="line">- MC 的内存结构以及钙化问题我就不细说了，大家可以查看官网了解下；</span><br><span class="line">- MC 对缓存的数据可以设置失效期，过期后的数据会被清除；</span><br><span class="line">失效的策略采用延迟失效，就是当再次使用数据时检查是否失效；</span><br><span class="line">当容量存满时，会对缓存中的数据进行剔除，剔除时除了会对过期 key 进行清理，还会按 LRU 策略对数据进行剔除。</span><br><span class="line">另外，使用 MC 有一些限制，这些限制在现在的互联网场景下很致命，成为大家选择Redis、MongoDB的重要原因：</span><br><span class="line"></span><br><span class="line">- key 不能超过 250 个字节；</span><br><span class="line">- value 不能超过 1M 字节；</span><br><span class="line">- key 的最大失效时间是 30 天；</span><br><span class="line">- 只支持 K-V 结构，不提供持久化和主从同步功能。</span><br><span class="line">Redis</span><br><span class="line">先简单说一下 Redis 的特点，方便和 MC 比较。</span><br><span class="line"></span><br><span class="line">与 MC 不同的是，Redis 采用单线程模式处理请求。这样做的原因有 2 个：一个是因为采用了非阻塞的异步事件处理机制；另一个是缓存数据都是内存操作 IO 时间不会太长，单线程可以避免线程上下文切换产生的代价。</span><br><span class="line">Redis 支持持久化，所以 Redis 不仅仅可以用作缓存，也可以用作 NoSQL 数据库。</span><br><span class="line">相比 MC，Redis 还有一个非常大的优势，就是除了 K-V 之外，还支持多种数据格式，例如 list、set、sorted set、hash 等。</span><br><span class="line">Redis 提供主从同步机制，以及 Cluster 集群部署能力，能够提供高可用服务。</span><br></pre></td></tr></table></figure>

<h2 id="Redis-的线程模型"><a href="#Redis-的线程模型" class="headerlink" title="Redis 的线程模型"></a>Redis 的线程模型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 Socket，根据 Socket 上的事件来选择对应的事件处理器进行处理。</span><br><span class="line"></span><br><span class="line">文件事件处理器的结构包含 4 个部分：</span><br><span class="line">- 多个 Socket</span><br><span class="line">- IO 多路复用程序</span><br><span class="line">- 文件事件分派器</span><br><span class="line">事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）</span><br><span class="line">多个 Socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 Socket，会将 Socket 产生的事件放入队列中排队，事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理。</span><br></pre></td></tr></table></figure>


<h2 id="扫描大-KEY"><a href="#扫描大-KEY" class="headerlink" title="扫描大 KEY"></a>扫描大 KEY</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;redis-cli --bigkeys -i 0.01</span><br></pre></td></tr></table></figure>

<h2 id="采样服务器指令"><a href="#采样服务器指令" class="headerlink" title="采样服务器指令"></a>采样服务器指令</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis-cli --host 192.168.x.x --port 6379 monitor</span><br></pre></td></tr></table></figure>

<h2 id="诊断服务器时延"><a href="#诊断服务器时延" class="headerlink" title="诊断服务器时延"></a>诊断服务器时延</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis-cli --host 192.168.x.x --port 6379 --latency</span><br><span class="line">时延单位是 ms。redis-cli 还能显示时延的分布情况，而且是图形化输出。</span><br><span class="line">redis-cli --latency-dist</span><br></pre></td></tr></table></figure>

<h2 id="远程-rdb-备份"><a href="#远程-rdb-备份" class="headerlink" title="远程 rdb 备份"></a>远程 rdb 备份</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">远程服务器会执行一次bgsave操作，然后将 rdb 文件传输到客户端</span><br><span class="line">.&#x2F;redis-cli --host 192.168.x.x --port 6379 --rdb .&#x2F;user.rdb</span><br></pre></td></tr></table></figure>

<h2 id="模拟从库"><a href="#模拟从库" class="headerlink" title="模拟从库"></a>模拟从库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;redis-cli --host 192.168.x.x --port 6379 --slave</span><br><span class="line">从库连上主库的第一件事是全量同步，所以看到上面的指令卡顿这很正常，待首次全量同步完成后，就会输出增量的 aof 日志。</span><br></pre></td></tr></table></figure>

<h2 id="aof-导入方式"><a href="#aof-导入方式" class="headerlink" title="aof 导入方式"></a>aof 导入方式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 清空上文目标实例全部数据</span><br><span class="line">redis-cli -h 目标RedisIP -a password flushall</span><br><span class="line"># 源实例开启 aof 功能，将在 dir 目录下生成 appendonly.aof 文件</span><br><span class="line">redis-cli -h 源RedisIP -a password config set appendonly yes</span><br><span class="line"># 将 appendonly.aof 文件放在当前路径下</span><br><span class="line">redis-cli -h 目标RedisIp -a password --pipe &lt; appendonly.aof</span><br><span class="line"># 源实例关闭 aof 功能</span><br><span class="line">redis-cli -h 源RedisIp -a password config set appendonly no</span><br></pre></td></tr></table></figure>

<h2 id="redis-dump-工具"><a href="#redis-dump-工具" class="headerlink" title="redis-dump 工具"></a>redis-dump 工具</h2><p>Redis-Dump 是一个用于 Redis 数据导入 / 导出的工具，是基于 Ruby 实现的,可以方便的进行 redis 的数据备份。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 没安装 ruby 的话，先安装 ruby</span><br><span class="line">brew install ruby</span><br><span class="line"># 移除 gem 自带源</span><br><span class="line">gem sources --remove https:&#x2F;&#x2F;rubygems.org&#x2F; </span><br><span class="line"># 添加淘宝源</span><br><span class="line">gem sources -a https:&#x2F;&#x2F;ruby.taobao.org&#x2F; </span><br><span class="line"># 安装 redis-dump</span><br><span class="line">gem install redis-dump -V</span><br><span class="line"># 替换镜像地址</span><br><span class="line">gem sources --add http:&#x2F;&#x2F;gems.ruby-china.org&#x2F; --remove http:&#x2F;&#x2F;rubygems.org&#x2F;</span><br><span class="line"># 确认镜像地址是否替换成功</span><br><span class="line">gem sources -l</span><br><span class="line"># 替换成功后再安装 redis-dump</span><br><span class="line">gem install redis-dump -V</span><br><span class="line"># redis-dump 导出</span><br><span class="line">redis-dump -u :password@源RedisIp:6379 &gt; 源Redis数据文件.json</span><br><span class="line"># redis-load 导入</span><br><span class="line">cat 源Redis数据文件.json | redis-load -u :password@目标RedisIp:6379</span><br></pre></td></tr></table></figure>


<h2 id="redis清理某个前缀的key"><a href="#redis清理某个前缀的key" class="headerlink" title="redis清理某个前缀的key"></a>redis清理某个前缀的key</h2><p>需要快速的清理掉某种前缀的key出现了脏数据key</p>
<p>安装 rdb 解析工具</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install rdbtools python-lzf   # python2.7 下 一行命令即可完成安装</span><br><span class="line"></span><br><span class="line">rdb -c memory dump-6379.rdb &gt; memory.csv  # 用这个命令将rdb进行分析</span><br></pre></td></tr></table></figure>

<p>过滤出符合条件的key</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk  -F &#39;,&#39; &#39;&#123;print $3 ,  $NF &#125;&#39;  memory.csv &gt; keys.txt    # 过滤出key的名称和过期时间</span><br><span class="line"></span><br><span class="line">egrep key_  keys.txt &gt; &#x2F;root&#x2F;key_.txt     # 将 key_ 前缀的key 过滤出来</span><br><span class="line"></span><br><span class="line">cat &#x2F;root&#x2F;key_.txt | sort -k 2 -r &gt; &#x2F;root&#x2F;sort_keys    # 对key按照日期进行倒序排序</span><br><span class="line"></span><br><span class="line">egrep 2019-09-10 &#x2F;root&#x2F;sort_keys  &gt; &#x2F;root&#x2F;match_keys    # 注意：我这里紧急处理，只过滤出 2019-09-10 过期的key(这是最新的数据，也是目前业务最常访问的key，也就是最需要紧急处理的)</span><br><span class="line"></span><br><span class="line">awk &#39;&#123;print $1&#125;&#39; &#x2F;root&#x2F;match_keys &gt; &#x2F;root&#x2F;filter_keys    # 将最终需要处理的key重定向到一个文件</span><br><span class="line"></span><br><span class="line">mkdir &#x2F;root&#x2F;test&#x2F;</span><br><span class="line"></span><br><span class="line">split -2000 &#x2F;root&#x2F;filter_keys  &#x2F;root&#x2F;test&#x2F;    # 将 filter_keys 这个文件 按照每个2k行切分成多个文件，便于后续并行处理</span><br></pre></td></tr></table></figure>

<p>批量处理下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for i in &#96;ls &#x2F;root&#x2F;test&#x2F;&#96;; do</span><br><span class="line">echo &quot;while read line;do</span><br><span class="line">echo \&quot;del \$line\&quot; | redis-cli -h 127.0.0.1 -p 6379 </span><br><span class="line">done &lt; &#x2F;root&#x2F;test&#x2F;$&#123;i&#125;&quot; &gt; &#x2F;root&#x2F;run_$&#123;i&#125;.sh</span><br><span class="line">chmod +x &#x2F;root&#x2F;run_$&#123;i&#125;.sh </span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>批量执行下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">for i in &#96;ls run*.sh&#96;; do </span><br><span class="line">nohup sh $i &gt; &#x2F;dev&#x2F;null &amp;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h2 id="redis-server-大量key过期不释放空间"><a href="#redis-server-大量key过期不释放空间" class="headerlink" title="redis-server 大量key过期不释放空间"></a>redis-server 大量key过期不释放空间</h2><p>使用 rdb工具 （git地址：<a href="https://github.com/sripathikrishnan/redis-rdb-tools）">https://github.com/sripathikrishnan/redis-rdb-tools）</a> 分析下rdb文件后，发现内存中有很多的key，过期时间早到了，但是实际上还存在。原因： 因为redis的key清理策略是懒惰删除(lazy free)，我们可以尝试调大，这样每秒钟执行的redis的内部cronjob次数将增大，也就可以加快key的淘汰。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、config get hz   看到当前redis-server 默认值是10 </span><br><span class="line">2、config set hz 50    我们这里将hz设置为50，然后观察段时间看看（注意hz的设置值可以以10为步长逐步增加，但是一般不要超过100）</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis学习笔记--Redis 主从复制技术原理</title>
    <url>/posts/1ce6bfed.html</url>
    <content><![CDATA[<h1 id="理论上的主从复制技术设计"><a href="#理论上的主从复制技术设计" class="headerlink" title="理论上的主从复制技术设计"></a>理论上的主从复制技术设计</h1><p>主从复制技术有两个版本，2.8 以前的版本，设计上有缺陷，在 slave 断线后重连依然需要 master 重新发送 RDB 重新进行数据更新，效率非常低。2.8 版本以后做了重新设计，通过引入偏移量同步，相对而言非常的高效，我们这里不去讨论旧版本的设计了，直接看新版本的主从复制技术设计。</p>
<a id="more"></a>
<p>每一个 Redis 启动后，都会认为自己是一个 master 节点，你可以通过以下命令通知它成为 slave 并向 master 同步数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">slaveof [masterip] [masterport]</span><br></pre></td></tr></table></figure>

<p>另一种方式就是在 Redis 启动配置文件中直接指明让它作为一个 slave 节点启动，并在启动后同步 master 节点数据。配置项和命令是一样的。</p>
<p>如果 master 配置了密码连接，那么还需要在 slave 的配置文件中指明 master 的连接密码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">masterauth &lt;password&gt;</span><br></pre></td></tr></table></figure>

<p>除此之外，salve 节点默认是只读的，不允许写入数据，因为如果支持写入数据，那么与 master 就无法保持数据一致性，所以我们一般会把 slave 节点作为读写分离中读服务提供者。当然，你也可以修改是否允许 slave 写入数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">slave-read-only yes&#x2F;no</span><br></pre></td></tr></table></figure>

<p>当然如果你的 master 宕机了，你需要把某个 slave 上线成 master，你可以通过命令取消 slave 的数据同步，成为单独的一个 master：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">slaveof no one</span><br></pre></td></tr></table></figure>

<p>slave 同步 master 的数据主要分为两个大步骤，全量复制和部分复制。当我们执行 slaveof 命令的时候，我们的 slave 会作为一个客户端连接上 master 并向 master 发送 PSYNC 命令。</p>
<p>master 收到命令后，会调用 bgsave fork 一个后台子进程生产 RDB 文件，待合适的时候，在 serverCron 循环的时候发送给 slave节点。</p>
<p>slave 收到 RDB 文件后，丢弃目前内存中所有的数据并阻塞自己，专心做 RDB 读取，数据恢复。</p>
<p>以上就是主从复制的一个全量复制的大概流程，但是一次全量复制并不能永远的保持主从节点数据一致，master 还需要将实时的修改命令同步到从节点才行，这就是部分复制。</p>
<p>在介绍部分复制之前，这里先介绍几个概念。第一个是复制缓冲区（repl_backlog），这是一个 FIFO 的队列，里面存的是最近的一些写命令，大小默认在 1M，复制偏移量（offset），这个偏移量其实是对应复制缓冲区中的字符偏移。复制缓冲区的结构大致是这样的：<br><img src="/images/GVlEvQ.png" width="100%" height="100%"></p>
<p>在主从节点完成第一轮全量复制以后，主从节点之间已经初步实现了数据同步，往后的 master，会将收到的每一条写命令发送给 slave 并 添加到复制缓冲区并根据字节数计算更新自己的偏移量，slave 收到传输过来的命令后也一样更新自己的偏移量。</p>
<p>这样，只要主从节点的偏移量相同就说明主从节点之间的数据是同步的。复制缓冲区大小是固定的，新的写命令进来以后，旧的数据就会出队列。如果某个 slave 断线重连之后，依然向 master 发送 PSYNC 命令并携带自己的偏移量，master 判断该偏移量是否还在缓冲区区间内，如果在则直接将该偏移量往后的所有偏移量对应的命令发送给 slave，无需重新进行全量复制。</p>
<p>这是新版同步复制的一个优化的设计，如果该断线重连的 slave 的偏移量已经不在缓冲区区间内，那么说明 master 可能已经无法找到自上次断线后的完整更新记录了，于是进行全量复制并将最新的偏移量发到 slave，算是完成了新的数据同步。</p>
]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis学习笔记--Redis配置文件Sentinel.conf参数配置详解</title>
    <url>/posts/b94e6d2e.html</url>
    <content><![CDATA[<p>Redis配置文件Sentinel.conf参数配置详解</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.port 26379</span><br><span class="line"></span><br><span class="line">sentinel监听端口，默认是26379，可以修改。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">2.sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;</span><br><span class="line"></span><br><span class="line">告诉sentinel去监听地址为ip:port的一个master，这里的master-name可以自定义，quorum是一个数字，指明当有多少个sentinel认为一个master失效时，master才算真正失效。master-name只能包含英文字母，数字，和“.-_”这三个字符需要注意的是master-ip 要写真实的ip地址而不要用回环地址（127.0.0.1）。</span><br><span class="line"></span><br><span class="line">配置示例：</span><br><span class="line"></span><br><span class="line">sentinel monitor mymaster 192.168.0.5 6379 2</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">3.sentinel auth-pass &lt;master-name&gt; &lt;password&gt;</span><br><span class="line"></span><br><span class="line">设置连接master和slave时的密码，注意的是sentinel不能分别为master和slave设置不同的密码，因此master和slave的密码应该设置相同。</span><br><span class="line"></span><br><span class="line">配置示例：</span><br><span class="line"></span><br><span class="line">sentinel auth-pass mymaster 0123passw0rd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4.sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt; </span><br><span class="line"></span><br><span class="line">这个配置项指定了需要多少失效时间，一个master才会被这个sentinel主观地认为是不可用的。 单位是毫秒，默认为30秒</span><br><span class="line"></span><br><span class="line">配置示例：</span><br><span class="line"></span><br><span class="line">sentinel down-after-milliseconds mymaster 30000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">5.sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt; </span><br><span class="line"></span><br><span class="line">这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行 同步，这个数字越小，完成failover所需的时间就越长，但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。</span><br><span class="line"></span><br><span class="line">配置示例：</span><br><span class="line"></span><br><span class="line">sentinel parallel-syncs mymaster 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">6. sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt;</span><br><span class="line"></span><br><span class="line">failover-timeout 可以用在以下这些方面： </span><br><span class="line"></span><br><span class="line">      1. 同一个sentinel对同一个master两次failover之间的间隔时间。</span><br><span class="line"></span><br><span class="line">      2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。</span><br><span class="line"></span><br><span class="line">      3.当想要取消一个正在进行的failover所需要的时间。  </span><br><span class="line"></span><br><span class="line">      4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了。</span><br><span class="line"></span><br><span class="line">配置示例：</span><br><span class="line"></span><br><span class="line">sentinel failover-timeout mymaster1 20000</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">7.sentinel的notification-script和reconfig-script是用来配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。对于脚本的运行结果有以下规则：</span><br><span class="line"></span><br><span class="line">        若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10</span><br><span class="line"></span><br><span class="line">        若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。</span><br><span class="line"></span><br><span class="line">        如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。</span><br><span class="line"></span><br><span class="line">        一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。</span><br><span class="line"></span><br><span class="line">1).sentinel notification-script &lt;master-name&gt; &lt;script-path&gt; </span><br><span class="line"></span><br><span class="line">通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，一个是事件的描述。如果sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。</span><br><span class="line"></span><br><span class="line">  配置示例：</span><br><span class="line"></span><br><span class="line"> sentinel notification-script mymaster &#x2F;var&#x2F;redis&#x2F;notify.sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2).sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt;</span><br><span class="line"></span><br><span class="line"> 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已经发生改变的信息。以下参数将会在调用脚本时传给脚本:</span><br><span class="line"></span><br><span class="line">       &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt;</span><br><span class="line"></span><br><span class="line">目前&lt;state&gt;总是“failover”, &lt;role&gt;是“leader”或者“observer”中的一个。 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通信的。这个脚本应该是通用的，能被多次调用，不是针对性的。</span><br><span class="line"></span><br><span class="line">   配置示例：</span><br><span class="line"></span><br><span class="line">   sentinel client-reconfig-script mymaster &#x2F;var&#x2F;redis&#x2F;reconfig.sh</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>redis4.0配置文件介绍</title>
    <url>/posts/97722e36.html</url>
    <content><![CDATA[<p>Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">daemonize no</span><br></pre></td></tr></table></figure>
<p>当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pidfile &#x2F;var&#x2F;run&#x2F;redis.pid</span><br></pre></td></tr></table></figure>
<p>指定Redis监听端口，默认端口为6379，作者在自己的一篇博文中解释了为什么选用6379作为默认端口，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">port 6379</span><br></pre></td></tr></table></figure>
<p>绑定的主机地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bind 127.0.0.1</span><br></pre></td></tr></table></figure>
<p>当 客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">timeout 300</span><br></pre></td></tr></table></figure>
<p>指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">loglevel verbose</span><br></pre></td></tr></table></figure>
<p>日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/null</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">logfile stdout</span><br></pre></td></tr></table></figure>
<p>设置数据库的数量，默认数据库为0，可以使用SELECT <dbid>命令在连接上指定数据库id</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">databases 16</span><br></pre></td></tr></table></figure>
<p>指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合</p>
<p>save <seconds> <changes><br>Redis默认配置文件中提供了三个条件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000</span><br></pre></td></tr></table></figure>
<p>分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。</p>
<p>指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rdbcompression yes</span><br></pre></td></tr></table></figure>
<p>指定本地数据库文件名，默认值为dump.rdb</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dbfilename dump.rdb</span><br></pre></td></tr></table></figure>
<p>指定本地数据库存放目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dir .&#x2F;</span><br></pre></td></tr></table></figure>
<p>设置当本机为slave服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">slaveof &lt;masterip&gt; &lt;masterport&gt;</span><br></pre></td></tr></table></figure>
<p>当master服务设置了密码保护时，slave服务连接master的密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">masterauth &lt;master-password&gt;</span><br></pre></td></tr></table></figure>
<p>设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH <password>命令提供密码，默认关闭</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">requirepass foobared</span><br></pre></td></tr></table></figure>
<p>设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">maxclients 128</span><br></pre></td></tr></table></figure>
<p>指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">maxmemory &lt;bytes&gt;</span><br></pre></td></tr></table></figure>
<p>指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">appendonly no</span><br></pre></td></tr></table></figure>
<p>指定更新日志文件名，默认为appendonly.aof</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">appendfilename appendonly.aof</span><br></pre></td></tr></table></figure>
<p>指定更新日志条件，共有3个可选值：<br>no：表示等操作系统进行数据缓存同步到磁盘（快）<br>always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全）<br>everysec：表示每秒同步一次（折衷，默认值）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">appendfsync everysec</span><br></pre></td></tr></table></figure>
<p>指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vm-enabled no</span><br></pre></td></tr></table></figure>
<p>虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vm-swap-file &#x2F;tmp&#x2F;redis.swap</span><br></pre></td></tr></table></figure>
<p>将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vm-max-memory 0</span><br></pre></td></tr></table></figure>
<p>Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不 确定，就使用默认值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vm-page-size 32</span><br></pre></td></tr></table></figure>
<p>设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，在磁盘上每8个pages将消耗1byte的内存。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vm-pages 134217728</span><br></pre></td></tr></table></figure>
<p>设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vm-max-threads 4</span><br></pre></td></tr></table></figure>
<p>设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">glueoutputbuf yes</span><br></pre></td></tr></table></figure>
<p>指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hash-max-zipmap-entries 64</span><br><span class="line">hash-max-zipmap-value 512</span><br></pre></td></tr></table></figure>
<p>指定是否激活重置哈希，默认为开启（后面在介绍Redis的哈希算法时具体介绍）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">activerehashing yes</span><br></pre></td></tr></table></figure>
<p>指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">include &#x2F;path&#x2F;to&#x2F;local.conf</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>定期删除elasticsearch索引记录</title>
    <url>/posts/a395b63e.html</url>
    <content><![CDATA[<p>需要定期清理的索引的后缀日期格式为YYYY.MM.DD，如：nginx-2018.09.01，解决方法通过_cat/indices接口可以获取当前ES全部索引信息，取第三列为索引名。过滤出索引名中带有的日期字符串，然后进行日期比较，早于10天前的日期便可通过日期模糊匹配索引来删除。</p>
<a id="more"></a>

<p>shell脚本如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">###################################</span><br><span class="line">#删除早于20天的ES集群的索引</span><br><span class="line">###################################</span><br><span class="line">function delete_indices() &#123;</span><br><span class="line">    comp_date&#x3D;&#96;date -d &quot;20 day ago&quot; +&quot;%Y-%m-%d&quot;&#96;</span><br><span class="line">    date1&#x3D;&quot;$1 00:00:00&quot;</span><br><span class="line">    date2&#x3D;&quot;$comp_date 00:00:00&quot;</span><br><span class="line"></span><br><span class="line">    t1&#x3D;&#96;date -d &quot;$date1&quot; +%s&#96; </span><br><span class="line">    t2&#x3D;&#96;date -d &quot;$date2&quot; +%s&#96; </span><br><span class="line"></span><br><span class="line">    if [ $t1 -le $t2 ]; then</span><br><span class="line">        echo &quot;$1时间早于$comp_date，进行索引删除&quot;</span><br><span class="line">        #转换一下格式，将类似2018-09-01格式转化为2018.09.01</span><br><span class="line">        format_date&#x3D;&#96;echo $1| sed &#39;s&#x2F;-&#x2F;\.&#x2F;g&#39;&#96;</span><br><span class="line">        curl -XDELETE http:&#x2F;&#x2F;127.0.0.1:9200&#x2F;*$format_date</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">curl -XGET http:&#x2F;&#x2F;127.0.0.1:9200&#x2F;_cat&#x2F;indices | awk -F&quot; &quot; &#39;&#123;print $3&#125;&#39; | awk -F&quot;-&quot; &#39;&#123;print $NF&#125;&#39; | egrep &quot;[0-9]*\.[0-9]*\.[0-9]*&quot; | sort | uniq  | sed &#39;s&#x2F;\.&#x2F;-&#x2F;g&#39; | while read LINE</span><br><span class="line">do</span><br><span class="line">    #调用索引删除函数</span><br><span class="line">    delete_indices $LINE</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">##将脚本配置到定时任务定期执行即可。</span><br></pre></td></tr></table></figure>

<!--更新-->
<p>ES删除XX天之前的索引和数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line"># ES版本为6.3.x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">function delIndex()</span><br><span class="line">&#123;</span><br><span class="line">        #此处输入要删除的索引名字，例如要删除filebeat-xxx索引的日志，此处要输入‘filebeat-’</span><br><span class="line">        index_name&#x3D;$1</span><br><span class="line">        # 此处输入要数据保留的天数，例如要保留最近60天的数据，此处要输入60</span><br><span class="line">        savedays&#x3D;$2</span><br><span class="line">        # $3变量也是天数，例如你要保留60天的数据，此处输入90，就是会删除第60到90天这段时间的数据，60&lt;&#x3D;90</span><br><span class="line">        while [ $savedays -le $3 ]</span><br><span class="line">        do</span><br><span class="line">                # 此处是es内索引的日期格式，有的是2019.11.26，有的是2019-11-26</span><br><span class="line">                format_day&#x3D;&#39;%Y.%m.%d&#39;</span><br><span class="line">                #format_day&#x3D;&#39;%Y-%m-%d&#39;</span><br><span class="line">                </span><br><span class="line">                #此处通过date命令组成索引的时间戳部分，例如2019.11.26或者2019-11-26</span><br><span class="line">                sevendayago&#x3D;&#96;date -d &quot;-$&#123;savedays&#125; day &quot; +$&#123;format_day&#125;&#96;</span><br><span class="line">                </span><br><span class="line">                #此处组成完整的es数据的索引，数据格式filebeat-2019.11.26或者filebeat-2019-11-26</span><br><span class="line">                index&#x3D;$index_name$sevendayago</span><br><span class="line">                #echo $sevendayago</span><br><span class="line">                echo $index</span><br><span class="line">                curl -XDELETE &quot;http:&#x2F;&#x2F;127.0.0.1:9200&#x2F;$&#123;index&#125;&quot;</span><br><span class="line">                #exit 0</span><br><span class="line">                </span><br><span class="line">                #删除完第60天的索引后，开始删除第61天的索引,直到第90天</span><br><span class="line">                savedays&#x3D;&#96;expr $savedays + 1&#96;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 删除索引前，要关闭索引，然后再删除索引</span><br><span class="line"># 逻辑与上面删除的逻辑一致</span><br><span class="line">function closeIndex()</span><br><span class="line">&#123;</span><br><span class="line">        index_name&#x3D;$1</span><br><span class="line">        savedays&#x3D;$2</span><br><span class="line">        while [ $savedays -le $3 ]</span><br><span class="line">        do</span><br><span class="line"></span><br><span class="line">                format_day&#x3D;&#39;%Y.%m.%d&#39;</span><br><span class="line">                #format_day&#x3D;&#39;%Y-%m-%d&#39;</span><br><span class="line">                sevendayago&#x3D;&#96;date -d &quot;-$&#123;savedays&#125; day &quot; +$&#123;format_day&#125;&#96;</span><br><span class="line">                index&#x3D;$index_name$sevendayago</span><br><span class="line">                #echo $sevendayago</span><br><span class="line">                echo $index</span><br><span class="line">                curl -XPOST &quot;http:&#x2F;&#x2F;127.0.0.1:9200&#x2F;$&#123;index&#125;&#x2F;_close?pretty&quot;</span><br><span class="line">                #exit 0</span><br><span class="line">                savedays&#x3D;&#96;expr $savedays + 1&#96;</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 关闭第60天到第90天的索引</span><br><span class="line">closeIndex ‘filebeat-’ 60 90</span><br><span class="line"></span><br><span class="line"># 删除第60天到第90天的索引</span><br><span class="line">deleteIndex ‘filebeat-’ 60 90</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title>shell脚本定期重启tomcat</title>
    <url>/posts/8a0c2be.html</url>
    <content><![CDATA[<p>因tomcat项目运作，占用内存过大，无法自动释放，只能定期重启项目解决。</p>
<a id="more"></a>
<p>shell脚本如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">. &#x2F;etc&#x2F;profile</span><br><span class="line">pid&#x3D;&#96;ps aux | grep tomcat | grep -v grep | grep new_2bulu | awk &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line">dat&#x3D;&#96;date &#39;+%Y-%m-%d %H:%M:%S&#39;&#96;</span><br><span class="line"> </span><br><span class="line">echo $dat</span><br><span class="line">echo $pid</span><br><span class="line"> </span><br><span class="line">if [ -n &quot;$pid&quot; ]</span><br><span class="line">then</span><br><span class="line">&#123;</span><br><span class="line">   echo &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;shutdown&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">   &#x2F;data&#x2F;application&#x2F;nginx_tomcat&#x2F;new_2bulu&#x2F;bin&#x2F;shutdown.sh</span><br><span class="line">   sleep 1</span><br><span class="line">   pid&#x3D;&#96;ps aux | grep tomcat | grep -v grep | grep new_2bulu| awk &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line">   if [ -n &quot;$pid&quot; ]</span><br><span class="line">   then</span><br><span class="line">    &#123;</span><br><span class="line">      sleep 1</span><br><span class="line">      echo &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;kill tomcat&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">      kill -9 $pid</span><br><span class="line">    &#125;</span><br><span class="line">   fi</span><br><span class="line">   sleep 1</span><br><span class="line">   echo &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;startup.sh&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">   &#x2F;data&#x2F;application&#x2F;nginx_tomcat&#x2F;new_2bulu&#x2F;bin&#x2F;startup.sh</span><br><span class="line"> &#125;</span><br><span class="line">else</span><br><span class="line">echo &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;startup.sh&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x2F;data&#x2F;application&#x2F;nginx_tomcat&#x2F;new_2bulu&#x2F;bin&#x2F;startup.sh</span><br><span class="line"> </span><br><span class="line">fi</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title>在kubernetes-1.12.x上搭建EFK日志分析平台</title>
    <url>/posts/1543163e.html</url>
    <content><![CDATA[<p>EFK 插件是kubernetes项目的一个日志解决方案，它包括三个组件：Elasticsearch, Fluentd, Kibana。</p>
<ul>
<li><p>Elasticsearch 是日志存储和日志搜索引擎</p>
</li>
<li><p>Fluentd 负责把k8s集群的日志发送给 Elasticsearch</p>
</li>
<li><p>Kibana 则是可视化界面查看和检索存储在 Elasticsearch 的数据</p>
<a id="more"></a>
<p>官方EFK地址：<a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch</a></p>
</li>
</ul>
<p>由于官方部署方案存在elasticsearch存储模式为emptyDir，国内下载镜像不能成功，部署Fluentd，还需要添加标签</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl label nodes node2 beta.kubernetes.io&#x2F;fluentd-ds-ready&#x3D;true</span><br></pre></td></tr></table></figure>
<p>问题也有可能出现Fluentd镜像不可用等问题，部署起来操作比较繁琐。因此本次搭建EFK采用Elasticsearch、Kibana单独部署，独立于容器，Fluentd采用官方的daemonset部署方式。</p>
<h1 id="部署java"><a href="#部署java" class="headerlink" title="部署java"></a>部署java</h1><p>java 版本需要1.8以上</p>
<p>添加java环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;jdk1.8.0_161&#x2F;</span><br><span class="line">export PATH&#x3D;$JAVA_HOME&#x2F;bin:$JAVA_HOME&#x2F;jre&#x2F;bin:$PATH</span><br><span class="line">export LASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib:$JAVA_HOME&#x2F;jre&#x2F;lib:$CLASSPATH</span><br></pre></td></tr></table></figure>

<h1 id="部署Elasticsearch"><a href="#部署Elasticsearch" class="headerlink" title="部署Elasticsearch"></a>部署Elasticsearch</h1><p>添加仓库源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;yum.repos.d&#x2F;elasticsearch.repo</span><br><span class="line"></span><br><span class="line">[elasticsearch-6.x]</span><br><span class="line">name&#x3D;Elasticsearch repository for 6.x packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;packages&#x2F;6.x&#x2F;yum</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;GPG-KEY-elasticsearch</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">autorefresh&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br></pre></td></tr></table></figure>

<p>安装Elasticsearch并启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install elasticsearch</span><br><span class="line">systemctl start elasticsearch</span><br></pre></td></tr></table></figure>

<p>检查es健康状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@public03 efk]# curl http:&#x2F;&#x2F;172.16.0.17:9200&#x2F;_cat&#x2F;health</span><br><span class="line">1542354774 07:52:54 my-cluster yellow 1 1 381 381 0 0 370 0 - 50.7%</span><br></pre></td></tr></table></figure>

<h1 id="部署Kibana"><a href="#部署Kibana" class="headerlink" title="部署Kibana"></a>部署Kibana</h1><p>添加仓库源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;yum.repos.d&#x2F;kibana.repo</span><br><span class="line"></span><br><span class="line">[kibana-6.x]</span><br><span class="line">name&#x3D;Kibana repository for 6.x packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;packages&#x2F;6.x&#x2F;yum</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;GPG-KEY-elasticsearch</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">autorefresh&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br></pre></td></tr></table></figure>

<p>安装kibana并启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install kibana</span><br><span class="line">systemctl start kibana</span><br></pre></td></tr></table></figure>

<h1 id="部署Fluentd"><a href="#部署Fluentd" class="headerlink" title="部署Fluentd"></a>部署Fluentd</h1><p>官方仓库地址：<a href="https://github.com/fluent/fluentd-kubernetes-daemonset">https://github.com/fluent/fluentd-kubernetes-daemonset</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下载部署文件</span><br><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;fluent&#x2F;fluentd-kubernetes-daemonset&#x2F;master&#x2F;fluentd-daemonset-cloudwatch-rbac.yaml</span><br><span class="line"></span><br><span class="line"># 修改权限，在env添加如下配置</span><br><span class="line">- name: FLUENT_UID</span><br><span class="line">  value: &quot;0&quot;</span><br><span class="line"></span><br><span class="line">修改elasticsearch连接地址</span><br><span class="line">- name:  FLUENT_ELASTICSEARCH_HOST</span><br><span class="line">  value: &quot;172.16.0.17&quot;</span><br></pre></td></tr></table></figure>
<p>如设置环境FLUENT_UID为0，启动会提示权限不足。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2018-11-16 06:48:42 +0000 [error]: unexpected error error_class&#x3D;Errno::EACCES error&#x3D;#&lt;Errno::EACCES: Permission denied @ rb_sysopen - &#x2F;var&#x2F;log&#x2F;fluentd-containers.log.pos&gt;</span><br><span class="line">2018-11-16 06:48:42 +0000 [error]: &#x2F;fluentd&#x2F;vendor&#x2F;bundle&#x2F;ruby&#x2F;2.4.0&#x2F;gems&#x2F;fluentd-0.12.43&#x2F;lib&#x2F;fluent&#x2F;plugin&#x2F;</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">2018-11-16 06:48:42 +0000 [info]: shutting down output type&#x3D;&quot;null&quot; plugin_id&#x3D;&quot;object:2acb0bd67e18&quot;</span><br><span class="line">2018-11-16 06:48:42 +0000 [info]: process finished code&#x3D;0</span><br><span class="line">2018-11-16 06:48:42 +0000 [warn]: process died within 1 second. exit.</span><br></pre></td></tr></table></figure>

<p>部署成功后，打开kibana设置索引，搜索如图所示：<br><img src="/images/20181116160340.png" width="100%" height="100%"></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
        <tag>EFK</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell脚本全量、增量发布项目部署</title>
    <url>/posts/4473e057.html</url>
    <content><![CDATA[<p>1、nginx非plus版本没有健康流量检测<br>2、通过修改nginx后端地址避免流量转发<br>3、如不修改也可采用backup模式</p>
<a id="more"></a>
<p>环境：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">10.25.208.243 tomcat1 nginx </span><br><span class="line">10.66.220.113 tomcat2 脚本</span><br><span class="line"></span><br><span class="line">两个tomcat已做session共享</span><br></pre></td></tr></table></figure>


<p>Shell脚本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">date_name&#x3D;&#96;date +%Y-%m-%d_%H-%M&#96;</span><br><span class="line"></span><br><span class="line">rn()&#123;</span><br><span class="line">        echo &#39;按任意键继续!&#39;</span><br><span class="line">        read -n 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 检测项目是否启动</span><br><span class="line">check_url()&#123;</span><br><span class="line">        if [ &quot;$1&quot; &#x3D;&#x3D; &quot;help&quot; ] || [ &quot;$1&quot; &#x3D;&#x3D; &quot;&quot; ];then</span><br><span class="line">                echo &quot;参数1 参数2 参数3&quot;</span><br><span class="line">                echo &quot;URL 页面内容 轮询检测[r](如:check_url https:&#x2F;&#x2F;www.baidu.com&#x2F; &quot;百度&quot;)&quot;</span><br><span class="line">                exit 0</span><br><span class="line">        fi</span><br><span class="line">        echo &quot;正在检查 [$1] 能否访问!&quot;</span><br><span class="line">        if [ &quot;$3&quot; &#x3D;&#x3D; &quot;r&quot; ];then</span><br><span class="line">                curl -s --connect-timeout 5 -m 8 $1 | grep $2</span><br><span class="line">                while [ $? !&#x3D; &quot;0&quot; ];do</span><br><span class="line">                        sleep 1s</span><br><span class="line">                        echo -n .</span><br><span class="line">                        curl -s --connect-timeout 5 -m 8 $1 | grep $2</span><br><span class="line">                done</span><br><span class="line">                return $?</span><br><span class="line">        else</span><br><span class="line">                curl -s --connect-timeout 5 -m 8 $1 | grep $2</span><br><span class="line">                return $?</span><br><span class="line">        fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 全量更新并检测项目是否启动</span><br><span class="line">nt_demo()&#123;</span><br><span class="line">        dates&#x3D;&#96;date +%Y-%m-%d_%H:%M&#96;</span><br><span class="line">        apppath&#x3D;&quot;&#x2F;data&#x2F;application&#x2F;tomcat-demo&quot;</span><br><span class="line">        cd &#x2F;data&#x2F;project&#x2F;svn&#x2F;nt_demo&#x2F;ns_possecu</span><br><span class="line">        svn up</span><br><span class="line">        echo &#39;SVN更新成功！&#39;</span><br><span class="line">        rn</span><br><span class="line">        ret&#x3D;&#96;mvn -U clean compile war:war -Pdemo&#96;</span><br><span class="line">        echo &quot;$ret&quot;</span><br><span class="line">        echo &quot;$ret&quot; | grep &#39;BUILD SUCCESS&#39;</span><br><span class="line">        if [ $? -ne 0 ];then</span><br><span class="line">                echo &#39;编译失败！&#39;</span><br><span class="line">                exit</span><br><span class="line">        fi</span><br><span class="line">        date&#x3D;&#96;date +%Y%m%d%H%M&#96;</span><br><span class="line">        echo &quot;开始更新demo1&quot;</span><br><span class="line">        echo &quot;调整nginx负载均衡地址&quot;</span><br><span class="line">        ssh -p 50750 root@10.25.208.243 &quot;sed -i &#39;s&#x2F;\&lt;server 10.25.208.243:23080\&gt;&#x2F;#&amp;&#x2F;&#39; &#x2F;usr&#x2F;local&#x2F;nginx-1.14.2&#x2F;conf&#x2F;nginx.conf&quot;</span><br><span class="line">        ssh -p 50750 root@10.25.208.243 &quot;&#x2F;usr&#x2F;local&#x2F;nginx-1.14.2&#x2F;sbin&#x2F;nginx -s reload&quot;</span><br><span class="line">        ssh -p 50750 root@10.25.208.243 &quot;cp -rf $apppath&#x2F;webapps&#x2F;possecu_cs.war $apppath&#x2F;backup&#x2F;possecu_cs.war$date&quot;</span><br><span class="line">        ssh -p 50750 root@10.25.208.243 &quot;rm -rf $apppath&#x2F;webapps&#x2F;possecu_cs.war&quot;</span><br><span class="line">        ssh -p 50750 root@10.25.208.243 &quot;rm -rf $apppath&#x2F;webapps&#x2F;possecu_cs&quot;</span><br><span class="line">        scp -P 50750 .&#x2F;target&#x2F;ns_possecu.war  10.25.208.243:$apppath&#x2F;webapps&#x2F;possecu_cs.war</span><br><span class="line">        ssh -p 50750 root@10.25.208.243 &quot;&#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;r.sh rr&quot;</span><br><span class="line">        echo &quot;########检查demo1是否更新成功########&quot;</span><br><span class="line">        stat&#x3D;&#39;0&#39;</span><br><span class="line">        i&#x3D;&#39;0&#39;</span><br><span class="line">        while [ $stat !&#x3D; &#39;1&#39; ]; do</span><br><span class="line">        code&#x3D;&#96;curl -s http:&#x2F;&#x2F;x.x.x.x:23080&#x2F;possecu_cs&#x2F;login&#x2F;forward_login.htm | grep &quot;&lt;title&gt;&quot; | awk -F &quot;title&gt;&quot; &#39;&#123;print $2&#125;&#39; | head -n 1 | grep 管理系统&#96;</span><br><span class="line">        if [ $? -eq 0 ];then</span><br><span class="line">                echo &#39;demo1启动成功&#39;</span><br><span class="line">                stat&#x3D;&#39;1&#39;</span><br><span class="line">                ssh -p 50750 root@10.25.208.243 &quot;sed -i &#39;&#x2F;\&lt;server 10.25.208.243:23080\&gt;&#x2F;s&#x2F;#&#x2F;&#x2F;&#39; &#x2F;usr&#x2F;local&#x2F;nginx-1.14.2&#x2F;conf&#x2F;nginx.conf&quot;</span><br><span class="line">                ssh -p 50750 root@10.25.208.243 &quot;&#x2F;usr&#x2F;local&#x2F;nginx-1.14.2&#x2F;sbin&#x2F;nginx -s reload&quot;</span><br><span class="line">		else</span><br><span class="line">                sleep 1</span><br><span class="line">                echo -n $i&#39;...&#39;</span><br><span class="line">                i&#x3D;&#96;expr $i + 1&#96;;</span><br><span class="line">                if [ $i &#x3D; &#39;120&#39; ];then</span><br><span class="line">                        i&#x3D;&#39;0&#39;</span><br><span class="line">                        echo &#39;120秒无相应，尝试重新启动tomcat&#39;</span><br><span class="line">                        ssh -p 50750 root@10.25.208.243 &quot;&#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;r.sh rr&quot;</span><br><span class="line">                fi</span><br><span class="line">        fi</span><br><span class="line">        done</span><br><span class="line">        echo &quot;开始更新demo2&quot;</span><br><span class="line">        echo &quot;调整nginx负载均衡地址&quot;</span><br><span class="line">        ssh -p 50750 root@10.25.208.243 &quot;sed -i &#39;s&#x2F;\&lt;server 10.66.220.113:23080\&gt;&#x2F;#&amp;&#x2F;&#39; &#x2F;usr&#x2F;local&#x2F;nginx-1.14.2&#x2F;conf&#x2F;nginx.conf&quot;</span><br><span class="line">        ssh -p 50750 root@10.25.208.243 &quot;&#x2F;usr&#x2F;local&#x2F;nginx-1.14.2&#x2F;sbin&#x2F;nginx -s reload&quot;</span><br><span class="line">        cp -rf $apppath&#x2F;webapps&#x2F;possecu_cs.war $apppath&#x2F;backup&#x2F;possecu_cs.war$date</span><br><span class="line">        rm -rf $apppath&#x2F;webapps&#x2F;possecu_cs.war</span><br><span class="line">        rm -rf $apppath&#x2F;webapps&#x2F;possecu_cs</span><br><span class="line">        cp .&#x2F;target&#x2F;ns_possecu.war $apppath&#x2F;webapps&#x2F;possecu_cs.war</span><br><span class="line">        &#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;r.sh rr</span><br><span class="line">        echo &quot;########检查demo2是否更新成功########&quot;</span><br><span class="line">        stat&#x3D;&#39;0&#39;</span><br><span class="line">        i&#x3D;&#39;0&#39;</span><br><span class="line">        while [ $stat !&#x3D; &#39;1&#39; ]; do</span><br><span class="line">        code&#x3D;&#96;curl -s http:&#x2F;&#x2F;x.x.x.x:23080&#x2F;possecu_cs&#x2F;login&#x2F;forward_login.htm | grep &quot;&lt;title&gt;&quot; | awk -F &quot;title&gt;&quot; &#39;&#123;print $2&#125;&#39; | head -n 1 | grep 管理系统&#96;</span><br><span class="line">        if [ $? -eq 0 ];then</span><br><span class="line">                echo &#39;demo2启动成功&#39;</span><br><span class="line">                stat&#x3D;&#39;1&#39;</span><br><span class="line">                ssh -p 50750 root@10.25.208.243 &quot;sed -i &#39;&#x2F;\&lt;server 10.66.220.113:23080\&gt;&#x2F;s&#x2F;#&#x2F;&#x2F;&#39; &#x2F;usr&#x2F;local&#x2F;nginx-1.14.2&#x2F;conf&#x2F;nginx.conf&quot;</span><br><span class="line">                echo &quot;调整nginx负载均衡地址&quot;</span><br><span class="line">                ssh -p 50750 root@10.25.208.243 &quot;&#x2F;usr&#x2F;local&#x2F;nginx-1.14.2&#x2F;sbin&#x2F;nginx -s reload&quot;</span><br><span class="line">		else</span><br><span class="line">                sleep 1</span><br><span class="line">                echo -n $i&#39;...&#39;</span><br><span class="line">                i&#x3D;&#96;expr $i + 1&#96;;</span><br><span class="line">                if [ $i &#x3D; &#39;120&#39; ];then</span><br><span class="line">                        i&#x3D;&#39;0&#39;</span><br><span class="line">                        echo &#39;120秒无相应，尝试重新启动tomcat&#39;</span><br><span class="line">                        &#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;r.sh rr</span><br><span class="line">                fi</span><br><span class="line">        fi</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 增量更新并根据参数是否重启项目</span><br><span class="line"># 如 deploy zl_nt_demo r  意味着重启，否则不重启</span><br><span class="line"># </span><br><span class="line">#</span><br><span class="line">zl_nt_demo()&#123;</span><br><span class="line">        mvn ntdemo</span><br><span class="line">        for i in &#96;cat &#x2F;data&#x2F;project&#x2F;nthttp.txt | sed &quot;s&#x2F;src\&#x2F;main\&#x2F;java&#x2F;webapp\&#x2F;WEB-INF\&#x2F;classes&#x2F;g&quot; | sed  &quot;s&#x2F;\.java&#x2F;\.class&#x2F;g&quot;&#96;;do</span><br><span class="line">                path&#x3D;&#96;echo $i | awk -F &#39;webapp&#39; &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line">                echo &quot;备份node1 10.25.208.243&quot;</span><br><span class="line">                ssh -p 50750 10.25.208.243 &quot;mv &#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;webapps&#x2F;possecu_cs&#x2F;$path &#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;webapps&#x2F;possecu_cs&#x2F;$path-$date_name&quot;</span><br><span class="line">                echo &quot;备份node2 10.66.220.113&quot;</span><br><span class="line">                mv &#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;webapps&#x2F;possecu_cs&#x2F;$path &#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;webapps&#x2F;possecu_cs&#x2F;$path-$date_name</span><br><span class="line">                echo &quot;复制node1 10.25.208.243&quot;  </span><br><span class="line">                scp -r -P 50750 &#x2F;data&#x2F;project&#x2F;svn&#x2F;nt_demo&#x2F;ns_possecu&#x2F;target&#x2F;ns_possecu&#x2F;$path 10.25.208.243:&#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;webapps&#x2F;possecu_cs&#x2F;$path</span><br><span class="line">                echo &quot;复制node2 10.66.220.113&quot;</span><br><span class="line">                \cp -rf &#x2F;data&#x2F;project&#x2F;svn&#x2F;nt_demo&#x2F;ns_possecu&#x2F;target&#x2F;ns_possecu&#x2F;$path &#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;webapps&#x2F;possecu_cs&#x2F;$path</span><br><span class="line">                echo &quot;输出替换文件node1&quot;</span><br><span class="line">                ssh -p 50750 10.25.208.243 &quot;ls -al &#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;webapps&#x2F;possecu_cs&#x2F;$path&quot;</span><br><span class="line">                echo &quot;输出替换文件node2&quot;</span><br><span class="line">                ls -al &#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;webapps&#x2F;possecu_cs&#x2F;$path</span><br><span class="line">        done</span><br><span class="line">        if [ &quot;$1&quot; &#x3D; &quot;r&quot; ];then</span><br><span class="line">				echo &quot;重启node1&quot;</span><br><span class="line">                ssh -P 50750 10.25.208.243 &quot;&#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;r.sh rr&quot;</span><br><span class="line">				check_url http:&#x2F;&#x2F;10.25.208.243:23080&#x2F;possecu_cs&#x2F;login&#x2F;forward_login.htm &quot;管理系统&quot;</span><br><span class="line">				while [ $? !&#x3D; &quot;0&quot; ];do</span><br><span class="line">                        sleep 1s</span><br><span class="line">                        echo -n &quot;.&quot;</span><br><span class="line">                        check_url http:&#x2F;&#x2F;10.25.208.243:23080&#x2F;possecu_cs&#x2F;login&#x2F;forward_login.htm &quot;管理系统&quot;</span><br><span class="line">                done</span><br><span class="line">				echo &quot;重启node2&quot;</span><br><span class="line">                &#x2F;data&#x2F;application&#x2F;tomcat-demo&#x2F;r.sh rr</span><br><span class="line">				check_url http:&#x2F;&#x2F;127.0.0.1:23080&#x2F;possecu_cs&#x2F;login&#x2F;forward_login.htm &quot;管理系统&quot;</span><br><span class="line">				while [ $? !&#x3D; &quot;0&quot; ];do</span><br><span class="line">                        sleep 1s</span><br><span class="line">                        echo -n &quot;.&quot;</span><br><span class="line">                        check_url http:&#x2F;&#x2F;127.0.0.1:23080&#x2F;possecu_cs&#x2F;login&#x2F;forward_login.htm &quot;管理系统&quot;</span><br><span class="line">                done</span><br><span class="line"></span><br><span class="line">        fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case &quot;$1&quot; in</span><br><span class="line">nt_demo )</span><br><span class="line">  nt_demo</span><br><span class="line">;;</span><br><span class="line"></span><br><span class="line">*)</span><br><span class="line">echo &quot;更新nt_demo&quot;</span><br><span class="line">exit 1</span><br><span class="line">esac</span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>shell脚本</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell脚本项目部署</title>
    <url>/posts/e8a9292a.html</url>
    <content><![CDATA[<p> 当需要更新多台服务器的时候，通过shell循环列表的形式进行服务器代码更新，并能访问正常值才进行另外一台部署，健康监测依托阿里云负载均衡，因此本脚本不考虑。脚本如下：</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">check_url()&#123;</span><br><span class="line">        if [ &quot;$1&quot; &#x3D;&#x3D; &quot;help&quot; ] || [ &quot;$1&quot; &#x3D;&#x3D; &quot;&quot; ];then</span><br><span class="line">                echo &quot;参数1 参数2 参数3&quot;</span><br><span class="line">                echo &quot;URL 页面内容 轮询检测[r](如:check_url http:&#x2F;&#x2F;www.baidu.com &quot;百度&quot;)&quot;</span><br><span class="line">                exit 0</span><br><span class="line">        fi</span><br><span class="line">        echo &quot;正在检查 [$1] 能否访问!&quot;</span><br><span class="line">        if [ &quot;$3&quot; &#x3D;&#x3D; &quot;r&quot; ];then</span><br><span class="line">                curl -s --connect-timeout 5 -m 8 $1 | grep $2</span><br><span class="line">                while [ $? !&#x3D; &quot;0&quot; ];do</span><br><span class="line">                        sleep 1s</span><br><span class="line">                        echo -n .</span><br><span class="line">                        curl -s --connect-timeout 5 -m 8 $1 | grep $2</span><br><span class="line">                done</span><br><span class="line">                return $?</span><br><span class="line">        else</span><br><span class="line">                curl -s --connect-timeout 5 -m 8 $1 | grep $2</span><br><span class="line">                return $?</span><br><span class="line">        fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dates&#x3D;&#96;date +%Y-%m-%d-%H%M&#96;</span><br><span class="line">DATADIR&#x3D;&quot;&#x2F;data&#x2F;application&#x2F;event-service&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cd &#x2F;data&#x2F;jenkins&#x2F;workspace&#x2F;event_dao_new&#x2F;event-bean</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;maven3&#x2F;bin&#x2F;mvn clean install -Dmaven.test.skip&#x3D;true -U</span><br><span class="line"></span><br><span class="line">cd &#x2F;data&#x2F;jenkins&#x2F;workspace&#x2F;event_dao_new&#x2F;event-dao</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;maven3&#x2F;bin&#x2F;mvn clean install -Dmaven.test.skip&#x3D;true -U</span><br><span class="line"></span><br><span class="line">cd &#x2F;data&#x2F;jenkins&#x2F;workspace&#x2F;event_dao_new&#x2F;event-daoservice</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;maven3&#x2F;bin&#x2F;mvn clean compile -U</span><br><span class="line"></span><br><span class="line">echo -e &quot;\033[32m 编译成功 \033[0m&quot;</span><br><span class="line"></span><br><span class="line">echo -e &quot;\033[32m 开始更新event-service \033[0m&quot;</span><br><span class="line"></span><br><span class="line">echo -e &quot;\033[32m 开始打包event-service \033[0m&quot;</span><br><span class="line">cd &#x2F;data&#x2F;jenkins&#x2F;workspace&#x2F;event_dao_new&#x2F;event-daoservice&#x2F;target&#x2F;classes</span><br><span class="line"></span><br><span class="line">rm -rf com.zip</span><br><span class="line">zip -r com.zip com</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">host&#x3D;(&quot;10.80.220.200&quot; &quot;10.31.54.54&quot; &quot;10.80.60.167&quot; &quot;10.80.112.69&quot; &quot;10.31.52.46&quot; &quot;10.241.104.193&quot; &quot;10.80.226.189&quot;)</span><br><span class="line"></span><br><span class="line">for i in $&#123;host[@]&#125;; do</span><br><span class="line"></span><br><span class="line">ssh -p 50750 root@$i &quot;rm $DATADIR&#x2F;classes&#x2F;com.zip&quot;</span><br><span class="line">echo -e &quot;\033[32m 远程临时文件删除完成event-service \033[0m&quot;</span><br><span class="line"></span><br><span class="line">ssh -p 50750 root@$i &quot;cp $DATADIR&#x2F;lib&#x2F;event-bean-1.0-SNAPSHOT.jar $DATADIR&#x2F;lib&#x2F;event-bean-1.0-SNAPSHOT.jar$dates&quot;</span><br><span class="line">ssh -p 50750 root@$i &quot;mv $DATADIR&#x2F;classes&#x2F;com $DATADIR&#x2F;classes&#x2F;com$dates&quot;</span><br><span class="line">echo -e &quot;\033[32m 备份event-bean、com完成event-service \033[0m&quot;</span><br><span class="line"></span><br><span class="line">scp -P 50750 &#x2F;data&#x2F;jenkins&#x2F;workspace&#x2F;event_dao_new&#x2F;event-bean&#x2F;target&#x2F;event-bean-1.0-SNAPSHOT.jar $i:$DATADIR&#x2F;lib&#x2F;</span><br><span class="line">scp -P 50750 &#x2F;data&#x2F;jenkins&#x2F;workspace&#x2F;event_dao_new&#x2F;event-daoservice&#x2F;target&#x2F;classes&#x2F;com.zip $i:$DATADIR&#x2F;classes&#x2F;</span><br><span class="line">echo -e &quot;\033[32m event-bean、com上传完成event-service \033[0m&quot;</span><br><span class="line"></span><br><span class="line">ssh -p 50750 root@$i &quot;unzip $DATADIR&#x2F;classes&#x2F;com.zip -d $DATADIR&#x2F;classes&#x2F;&quot;</span><br><span class="line">echo -e &quot;\033[32m com解压完成event-service \033[0m&quot;</span><br><span class="line"></span><br><span class="line">echo -e &quot;\033[32m 探测event-service67是否启动成功 \033[0m&quot;</span><br><span class="line">ssh -p 50750 root@$i &quot;&#x2F;data&#x2F;application&#x2F;event-service&#x2F;bin&#x2F;event-service.sh restart&quot;</span><br><span class="line">check_url http:&#x2F;&#x2F;$i:7700&#x2F;getServiceVersion &quot;version&quot;</span><br><span class="line">while [ $? !&#x3D; &quot;0&quot; ];do</span><br><span class="line">	sleep 1s</span><br><span class="line">	echo -n &quot;.&quot;</span><br><span class="line">	check_url http:&#x2F;&#x2F;$i:7700&#x2F;getServiceVersion &quot;version&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">curl -s --connect-timeout 5 -m 8 http:&#x2F;&#x2F;$i:7700&#x2F;getServiceVersion</span><br><span class="line"></span><br><span class="line">done</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>shell脚本</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>一些有用的命令汇总</title>
    <url>/posts/8533151a.html</url>
    <content><![CDATA[<p>一些工作上有时会用到的命令，以便需要时快速查看</p>
<a id="more"></a>

<p>Mysql统计所有表数据量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT TABLE_NAME, TABLE_ROWS FROM &#96;information_schema&#96;.&#96;tables&#96;  WHERE &#96;table_schema&#96; &#x3D; &#39;YOUR_DB_NAME&#39;;</span><br></pre></td></tr></table></figure>

<p>postgresql kill指定数据库所有进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select pg_terminate_backend(pid) from  (select pid from pg_stat_activity where datname &#x3D; &#39;2bulugis&#39;) a;</span><br></pre></td></tr></table></figure>

<p>查看linux top完整命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">top -c</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>systemd-logind导致ssh登录缓慢</title>
    <url>/posts/8a4140ba.html</url>
    <content><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>通过SSH登录服务器CentOS7时异常缓慢，发现机上systemd-logind进程导致cpu占满100%的问题，使得并且消耗资源。</p>
<h1 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h1><p>systemd-logind主要功能是为每一个登陆session创建一个systemd角度的cgroup管理对象，更方便对session使用cgroup，也是一个systemd BUG<br>详情1：<a href="https://github.com/systemd/systemd/issues/1961">https://github.com/systemd/systemd/issues/1961</a><br>详情2：<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1439989">https://bugzilla.redhat.com/show_bug.cgi?id=1439989</a></p>
<a id="more"></a>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>清除/清除所有范围文件和已加载的活动已放弃会话：</p>
<p>清理systemd中已放弃的会话：</p>
<p>删除会话文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find &#x2F;run&#x2F;systemd&#x2F;system -name &quot;session-*.scope&quot; -delete</span><br></pre></td></tr></table></figure>

<p>删除会话目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rm -rf &#x2F;run&#x2F;systemd&#x2F;system&#x2F;session*scope*</span><br></pre></td></tr></table></figure>

<p>删除已放弃的会话<br>此命令可能会终止会话范围内的进程，因此请小心使用此命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl | grep &quot;abandoned&quot; | grep -e &quot;-[[:digit:]]&quot; | sed &quot;s&#x2F;\.scope.*&#x2F;.scope&#x2F;&quot; | xargs systemctl stop</span><br></pre></td></tr></table></figure>

<p>脚本形式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;root&#x2F;cleanup_systemd_sessions.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">## cleanup_systemd_sessions.sh </span><br><span class="line"></span><br><span class="line">echo &quot;Stopping abandoned sessions&quot; </span><br><span class="line">systemctl | grep abandoned | grep -e &quot;[[:digit:]]&quot; | sed &quot;s&#x2F;.scope.*&#x2F;.scope&#x2F;&quot; | xargs --no-run-if-empty systemctl stop </span><br><span class="line"></span><br><span class="line">cd &#x2F;run&#x2F;systemd&#x2F;system&#x2F; || exit 1 </span><br><span class="line">for i in session-*.scope; do </span><br><span class="line">! systemctl status &quot;$i&quot; &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 || continue </span><br><span class="line">echo &quot;Removing &#x2F;run&#x2F;systemd&#x2F;system&#x2F;$&#123;i&#125;*&quot; &gt;&amp;2 </span><br><span class="line">rm -rf &quot;$&#123;i&#125;&quot; &quot;$&#123;i&#125;.d&quot; </span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">chmod +x &#x2F;root&#x2F;cleanup_systemd_sessions.sh</span><br><span class="line">0 * * * * &#x2F;root&#x2F;cleanup_systemd_sessions.sh</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>ssh</tag>
        <tag>systemd-logind</tag>
      </tags>
  </entry>
  <entry>
    <title>Tomcat优化</title>
    <url>/posts/c37025e9.html</url>
    <content><![CDATA[<h1 id="Tomcat运行模式"><a href="#Tomcat运行模式" class="headerlink" title="Tomcat运行模式"></a>Tomcat运行模式</h1><p>Tomcat的运行模式有3种。</p>
<a id="more"></a>
<h2 id="bio模式"><a href="#bio模式" class="headerlink" title="bio模式"></a>bio模式</h2><p>bio(blocking I/O)，阻塞式I/O操作，表示Tomcat使用的是传统的Java I/O操作(即java.io包及其子包)，对于每一个请求都要创建一个线程来进行处理，所以开销较大不适合处理高并发的场景。</p>
<h2 id="nio模式"><a href="#nio模式" class="headerlink" title="nio模式"></a>nio模式</h2><p>nio是Java SE 1.4及后续版本提供的一种新的I/O操作方式(即java.nio包及其子包)，基于缓冲区、并能提供非阻塞I/O操作的Java API实现，比传统的I/O处理方式（bio）有更高的并发运行性能。是Tomcat8的默认运行模式。</p>
<h2 id="apr模式"><a href="#apr模式" class="headerlink" title="apr模式"></a>apr模式</h2><p>apr(Apache Portable Runtime/Apache可移植运行库)是Apache HTTP服务器的支持库。从操作系统级别解决异步I/O问题，大幅度提高服务器的并发处理性能，也是Tomcat生产环境运行的首选方式。要tomcat支持apr，必须安装apr和native，这样tomcat才可以利用apache的apr接口，使用操作系统的部分本地操作，从而提升性能。</p>
<h1 id="Tomcat-APR"><a href="#Tomcat-APR" class="headerlink" title="Tomcat APR"></a>Tomcat APR</h1><h2 id="安装APR"><a href="#安装APR" class="headerlink" title="安装APR"></a>安装APR</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yum -y install apr apr-devel gcc gcc-c++ openssl-devel openssl</span><br></pre></td></tr></table></figure>

<h2 id="解压Tomcat-Native包，编译安装"><a href="#解压Tomcat-Native包，编译安装" class="headerlink" title="解压Tomcat Native包，编译安装"></a>解压Tomcat Native包，编译安装</h2><p>此包在apache-tomcat-8.5.37/bin目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># tar zxvf tomcat-native.tar.gz</span><br><span class="line"># cd tomcat-native-1.2.19-src</span><br><span class="line"># .&#x2F;configure --with-apr&#x3D;&#x2F;usr&#x2F; --with-java-home&#x3D;&#x2F;usr&#x2F;local&#x2F;jdk1.8.0_181 --with-ssl</span><br><span class="line"># make &amp;&amp; make install</span><br></pre></td></tr></table></figure>

<p>说明：./configure有两个参数需要注意下，–with-apr=/usr/提供的是查找apr的路径可以更详细的一点，–with-java-home后面跟随的地址要和”echo $JAVA_HOME”输出地址一致。</p>
<h2 id="根据安装完成后的提示操作"><a href="#根据安装完成后的提示操作" class="headerlink" title="根据安装完成后的提示操作"></a>根据安装完成后的提示操作</h2><p>把库添加到/etc/ld.so.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vim &#x2F;etc&#x2F;ld.so.conf</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;apr&#x2F;lib    &#x2F;&#x2F;添加该行</span><br><span class="line"></span><br><span class="line"># ldconfig　　&#x2F;&#x2F;重新加载</span><br><span class="line"># echo &quot;ldconfig&quot; &gt;&gt; &#x2F;etc&#x2F;rc.local　　&#x2F;&#x2F;这是开机自启文件的软链接，需要源文件有执行权限</span><br></pre></td></tr></table></figure>

<p>配置环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;apr&#x2F;lib　　&#x2F;&#x2F;新增一行</span><br></pre></td></tr></table></figure>

<h2 id="配置APR"><a href="#配置APR" class="headerlink" title="配置APR"></a>配置APR</h2><h3 id="修改conf-server-xml里的协议"><a href="#修改conf-server-xml里的协议" class="headerlink" title="修改conf/server.xml里的协议"></a>修改conf/server.xml里的协议</h3><p>修改conf/server.xml里的协议，这个协议是<service>标签里的<Connector>标签的参数protocol，默认协议是HTTP/1.1。我们将协议修改为org.apache.coyote.http11.Http11AprProtocol</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;Connector port&#x3D;&quot;8080&quot; protocol&#x3D;&quot;org.apache.coyote.http11.Http11AprProtocol&quot;</span><br><span class="line">               connectionTimeout&#x3D;&quot;20000&quot;</span><br><span class="line">               redirectPort&#x3D;&quot;8443&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure>

<h3 id="在bin-catalina-sh中引用apr"><a href="#在bin-catalina-sh中引用apr" class="headerlink" title="在bin/catalina.sh中引用apr"></a>在bin/catalina.sh中引用apr</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#  vim &#x2F;usr&#x2F;local&#x2F;tomcat8.5&#x2F;bin&#x2F;catalina.sh</span><br><span class="line"></span><br><span class="line">JAVA_OPTS&#x3D;&quot;$JAVA_OPTS -Djava.protocol.handler.pkgs&#x3D;org.apache.catalina.webresources&quot; &#x2F;&#x2F;这是第261行，在这下面添加一行参数</span><br><span class="line">JAVA_OPTS&#x3D;&quot;$JAVA_OPTS -Djava.library.path&#x3D;&#x2F;usr&#x2F;local&#x2F;apr&#x2F;lib&quot;  &#x2F;&#x2F;只要添加这一行</span><br></pre></td></tr></table></figure>











]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯云服务器CentOS 7 安装桌面版</title>
    <url>/posts/1b999fc9.html</url>
    <content><![CDATA[<p>腾讯云服务器安装CentOS 7基本都是最小化安装，默认不带X WINDOWS，因此在安装桌面之前先安装一下X WINDOWS，这个控制功能。</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y upgrade</span><br><span class="line">yum -y groupinstall &quot;X Window System&quot;</span><br></pre></td></tr></table></figure>

<h1 id="GNOME桌面环境"><a href="#GNOME桌面环境" class="headerlink" title="GNOME桌面环境"></a>GNOME桌面环境</h1><p>安装GNOME桌面环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y groups install &quot;GNOME Desktop&quot;</span><br></pre></td></tr></table></figure>

<p>服务器在远程，需要用VNC，设置系统图形登录并重启系统</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl set-default graphical.target</span><br></pre></td></tr></table></figure>

<h1 id="安装vnc-server"><a href="#安装vnc-server" class="headerlink" title="安装vnc server"></a>安装vnc server</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install tigervnc-server -y</span><br></pre></td></tr></table></figure>

<h2 id="设置为服务"><a href="#设置为服务" class="headerlink" title="设置为服务"></a>设置为服务</h2><p>复制一个服务设置的模板</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;vncserver@.service &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;vncserver@:1.service</span><br></pre></td></tr></table></figure>

<p>修改此服务配置<br>以下为root用户的配置，每一个用户，都需要单独设置一个监听服务。且配置不同</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;vncserver@\:1.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;Remote desktop service (VNC)</span><br><span class="line">After&#x3D;syslog.target network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;forking</span><br><span class="line">User&#x3D;root</span><br><span class="line"></span><br><span class="line"># Clean any existing files in &#x2F;tmp&#x2F;.X11-unix environment</span><br><span class="line">ExecStartPre&#x3D;-&#x2F;usr&#x2F;bin&#x2F;vncserver -kill %i</span><br><span class="line">ExecStart&#x3D;&#x2F;sbin&#x2F;runuser -l root -c &quot;&#x2F;usr&#x2F;bin&#x2F;vncserver -geometry 1440x900 %i&quot;</span><br><span class="line">PIDFile&#x3D;&#x2F;root&#x2F;.vnc&#x2F;%H%i.pid</span><br><span class="line">ExecStop&#x3D;-&#x2F;usr&#x2F;bin&#x2F;vncserver -kill %i</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure>

<p>对于普通用户,如test</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下面举例进行设置： 首先一样复制一个server配置。改名为vncserver@:2.service</span><br><span class="line">cp &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;vncserver@.service &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;vncserver@:2.service</span><br><span class="line"># 然后进行修改</span><br><span class="line">vim &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;vncserver@\:2.service</span><br><span class="line">--------------------------------------------------------------------------------------------------------</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;Remote desktop service (VNC)</span><br><span class="line">After&#x3D;syslog.target network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;forking</span><br><span class="line">User&#x3D;root</span><br><span class="line"></span><br><span class="line"># Clean any existing files in &#x2F;tmp&#x2F;.X11-unix environment</span><br><span class="line">ExecStartPre&#x3D;-&#x2F;usr&#x2F;bin&#x2F;vncserver -kill %i</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;vncserver %i</span><br><span class="line">PIDFile&#x3D;&#x2F;home&#x2F;test&#x2F;.vnc&#x2F;%H%i.pid</span><br><span class="line">ExecStop&#x3D;-&#x2F;usr&#x2F;bin&#x2F;vncserver -kill %i</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure>

<h2 id="设置vnc密码"><a href="#设置vnc密码" class="headerlink" title="设置vnc密码"></a>设置vnc密码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# vncpasswd</span><br><span class="line">Password:</span><br><span class="line">Verify:</span><br><span class="line">Would you like to enter a view-only password (y&#x2F;n)? n</span><br><span class="line">A view-only password is not used</span><br><span class="line"># 这里不添加只读账号密码</span><br><span class="line"># 每个不用的系统用户，设置密码时，需要切换到该用户下，执行此命令</span><br><span class="line"># 如：su test  切换到test用户再执行上vncpasswd设置密码</span><br></pre></td></tr></table></figure>

<h2 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start vncserver@:1</span><br><span class="line">systemctl enable vncserver@:1</span><br><span class="line"></span><br><span class="line"># 查看服务监听的端口</span><br><span class="line">netstat -lnpt|grep Xvnc</span><br><span class="line"># 根据监听的端口，进行端口开放，每个用户会对应一个端口，第一个用户默认为5901端口</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>gnome</tag>
      </tags>
  </entry>
  <entry>
    <title>通过kubenetes Helm安装Ceph</title>
    <url>/posts/3b8e40b9.html</url>
    <content><![CDATA[<h1 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h1><ul>
<li><p>公共网络和集群网络必须相同</p>
</li>
<li><p>如果存储类用户标识不是admin，则必须在Ceph集群中手动创建用户并在Kubernetes中创建其密钥</p>
</li>
<li><p>ceph-mgr只能运行1个副本</p>
</li>
</ul>
<h1 id="启动本地Helm"><a href="#启动本地Helm" class="headerlink" title="启动本地Helm"></a>启动本地Helm</h1><p>ceph-helm项目默认使用本地Helm repo来存储images，要启动本地Helm repo服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ helm serve &amp;</span><br><span class="line">$ helm repo add local http:&#x2F;&#x2F;localhost:8879&#x2F;charts</span><br></pre></td></tr></table></figure>

<h1 id="增加本地helm仓库"><a href="#增加本地helm仓库" class="headerlink" title="增加本地helm仓库"></a>增加本地helm仓库</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;ceph&#x2F;ceph-helm</span><br><span class="line">$ cd ceph-helm&#x2F;ceph</span><br><span class="line">$ make</span><br></pre></td></tr></table></figure>

<h1 id="配置Ceph"><a href="#配置Ceph" class="headerlink" title="配置Ceph"></a>配置Ceph</h1><p>创建一个ceph-overrides.yaml Ceph配置来覆盖ceph-helm默认值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat ~&#x2F;ceph-overrides.yaml</span><br><span class="line">network:</span><br><span class="line">  public:   172.16.0.0&#x2F;20</span><br><span class="line">  cluster:   172.16.0.0&#x2F;20</span><br><span class="line"></span><br><span class="line">osd_devices:</span><br><span class="line">  - name: dev-sdb</span><br><span class="line">    device: &#x2F;dev&#x2F;sdb</span><br><span class="line">    zap: &quot;1&quot;</span><br><span class="line">  - name: dev-sdc</span><br><span class="line">    device: &#x2F;dev&#x2F;sdc</span><br><span class="line">    zap: &quot;1&quot;</span><br><span class="line"></span><br><span class="line">storageclass:</span><br><span class="line">  name: ceph-rbd</span><br><span class="line">  pool: rbd</span><br><span class="line">  user_id: k8s</span><br></pre></td></tr></table></figure>

<h1 id="创建kubernetes-命名空间"><a href="#创建kubernetes-命名空间" class="headerlink" title="创建kubernetes 命名空间"></a>创建kubernetes 命名空间</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl create namespace ceph</span><br></pre></td></tr></table></figure>

<h1 id="配置RBAC权限"><a href="#配置RBAC权限" class="headerlink" title="配置RBAC权限"></a>配置RBAC权限</h1><p>Kubernetes&gt; = v1.6使用RBAC为默认权限,rbac.yaml默认在git clone下来的文件夹里面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create -f ~&#x2F;ceph-helm&#x2F;ceph&#x2F;rbac.yaml</span><br></pre></td></tr></table></figure>

<h1 id="创建标签"><a href="#创建标签" class="headerlink" title="创建标签"></a>创建标签</h1><p>```</p>
<h1 id="Ceph-Monitor"><a href="#Ceph-Monitor" class="headerlink" title="Ceph Monitor"></a>Ceph Monitor</h1><p>$ kubectl label node <nodename> ceph-mon=enabled ceph-mgr=enabled</p>
<h1 id="OSD-node"><a href="#OSD-node" class="headerlink" title="OSD node"></a>OSD node</h1><p>$ kubectl label node <nodename> ceph-osd=enabled ceph-osd-device-dev-sdb=enabled ceph-osd-device-dev-sdc=enabled</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
        <tag>ceph</tag>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title>解决mysql配置文件my.cnf添加max_connections不生效</title>
    <url>/posts/7009fd55.html</url>
    <content><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>新增项目环境，通过mysql官方指定的yum源安装了mysql5.6.43，由于项目启动提示too many connections，于是修改mysql的配置文件my.cnf添加max_connections=1000后，重启mysql后发现不生效，但是如果通过交互端的命令行可以更改生效。</p>
<a id="more"></a>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>如果采用的是yum源安装，至少需要修改一个地方：</p>
<h2 id="添加LimitNOFILE-65535至mysql服务文件mysqld-service中的-Service-段下面（必选项）："><a href="#添加LimitNOFILE-65535至mysql服务文件mysqld-service中的-Service-段下面（必选项）：" class="headerlink" title="添加LimitNOFILE=65535至mysql服务文件mysqld.service中的[Service]段下面（必选项）："></a>添加LimitNOFILE=65535至mysql服务文件mysqld.service中的[Service]段下面（必选项）：</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~]# echo &quot;LimitNOFILE &#x3D; 65535&quot; &gt;&gt;  &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;mysqld.service</span><br><span class="line">~]# systemctl  daemon-reload</span><br><span class="line">~]# vim &#x2F;etc&#x2F;my.cnf　　　　&#x2F;&#x2F; 添加max_connections &#x3D; 期望值</span><br><span class="line">~]# systemctl  restart  mysqld</span><br></pre></td></tr></table></figure>

<h2 id="在系统级别修改mysql打开最大文件数（非必选项，最好修改）"><a href="#在系统级别修改mysql打开最大文件数（非必选项，最好修改）" class="headerlink" title="在系统级别修改mysql打开最大文件数（非必选项，最好修改）"></a>在系统级别修改mysql打开最大文件数（非必选项，最好修改）</h2><p>在/etc/security/limits.conf末尾添加以下两行（需要reboot）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql hard nofile 65535</span><br><span class="line">mysql soft nofile 65535</span><br></pre></td></tr></table></figure>
<p>然后就可以在my.cnf中修改较大值的max_connections了</p>
]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Tomcat8使用Redis作为session管理</title>
    <url>/posts/ecbcd30c.html</url>
    <content><![CDATA[<p>源代码网址:<a href="http://www.github.com/chexagon/redis-session-manager">http://www.github.com/chexagon/redis-session-manager</a><br>本次tomcat版本为8，非8.5，如8.5需要重新编译jar包</p>
<a id="more"></a>

<h2 id="在-tomcat-lib-中增加以下jar包"><a href="#在-tomcat-lib-中增加以下jar包" class="headerlink" title="在 tomcat/lib 中增加以下jar包"></a>在 tomcat/lib 中增加以下jar包</h2><p>可能版本号会不一致</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">commons-pool2-2.2.jar</span><br><span class="line">jedis-2.5.2.jar</span><br><span class="line">redis-session-manager-with-dependencies-2.2.2-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<p>依赖软件下载地址：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1r5qGw2v8wpUUJvaX8Bz9Iw</span><br></pre></td></tr></table></figure>

<h2 id="修改tomcat-conf-context-xml-增加以下内容"><a href="#修改tomcat-conf-context-xml-增加以下内容" class="headerlink" title="修改tomcat/conf/context.xml, 增加以下内容"></a>修改tomcat/conf/context.xml, 增加以下内容</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;Connector port&#x3D;&quot;18080&quot; protocol&#x3D;&quot;HTTP&#x2F;1.1&quot; </span><br><span class="line">               maxHttpHeaderSize&#x3D;&quot;8192&quot;</span><br><span class="line">               enableLookups&#x3D;&quot;false&quot; disableUploadTimeout&#x3D;&quot;true&quot;</span><br><span class="line">               maxThreads&#x3D;&quot;500&quot; </span><br><span class="line">               minSpareTherads&#x3D;&quot;100&quot;</span><br><span class="line">               maxSpareThreads&#x3D;&quot;200&quot;</span><br><span class="line">               acceptCount&#x3D;&quot;2000&quot;</span><br><span class="line">	       minProcessors&#x3D;&quot;200&quot;</span><br><span class="line">	       maxProcessors&#x3D;&quot;1000&quot;</span><br><span class="line">               compression&#x3D;&quot;on&quot; compressionMinSize&#x3D;&quot;2048&quot;</span><br><span class="line">               compressableMimeType&#x3D;&quot;text&#x2F;html,text&#x2F;xml,text&#x2F;javascript,text&#x2F;css,text&#x2F;plain&quot;</span><br><span class="line">               connectionTimeout&#x3D;&quot;900000&quot; server&#x3D;&quot;App Srv 1.0&quot; </span><br><span class="line">               redirectPort&#x3D;&quot;8443&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure>

<p>备注：具体使用方法可参考作者介绍，具体问题具体分析</p>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Xtrabackup进行MySQL备份</title>
    <url>/posts/3a2fcc8e.html</url>
    <content><![CDATA[<h2 id="Xtrabackup介绍"><a href="#Xtrabackup介绍" class="headerlink" title="Xtrabackup介绍"></a>Xtrabackup介绍</h2><p>Xtrabackup是由percona提供的mysql数据库备份工具，据官方介绍，这也是世界上惟一一款开源的能够对innodb和xtradb数据库进行热备的工具。特点：</p>
<p>备份过程快速、可靠；<br>备份过程不会打断正在执行的事务；<br>能够基于压缩等功能节约磁盘空间和流量；<br>自动实现备份检验；<br>还原速度快；</p>
<a id="more"></a>
<h2 id="下载安装xtrabackup"><a href="#下载安装xtrabackup" class="headerlink" title="下载安装xtrabackup"></a>下载安装xtrabackup</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.percona.com&#x2F;downloads&#x2F;XtraBackup&#x2F;LATEST&#x2F;</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# yum localinstall percona-xtrabackup-24-2.4.8-1.el6.x86_64.rpm</span><br><span class="line">[root@node1 ~]# yum install http:&#x2F;&#x2F;mirrors.sohu.com&#x2F;fedora-epel&#x2F;6&#x2F;x86_64&#x2F;epel-release-6-8.noarch.rpm</span><br></pre></td></tr></table></figure>

<h2 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h2><h3 id="完全备份"><a href="#完全备份" class="headerlink" title="完全备份"></a>完全备份</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# innobackupex --defaults-file&#x3D;&#x2F;usr&#x2F;local&#x2F;mysql-test3&#x2F;my.cnf --user&#x3D;root --password&#x3D;123456 --port&#x3D;3312 --host&#x3D;127.0.0.1 &#x2F;home&#x2F;backup&#x2F;</span><br></pre></td></tr></table></figure>

<p>使用innobakupex备份时，其会调用xtrabackup备份所有的InnoDB表，复制所有关于表结构定义的相关文件(.frm)、以及MyISAM、MERGE、CSV和ARCHIVE表的相关文件，同时还会备份触发器和数据库配置信息相关的文件。这些文件会被保存至一个以时间命令的目录中。</p>
<p>在备份的同时，innobackupex还会在备份目录中创建如下文件：<br>(1)xtrabackup_checkpoints – 备份类型（如完全或增量）、备份状态（如是否已经为prepared状态）和LSN(日志序列号)范围信息；每个InnoDB页(通常为16k大小)都会包含一个日志序列号，即LSN。LSN是整个数据库系统的系统版本号，每个页面相关的LSN能够表明此页面最近是如何发生改变的。</p>
<p>(2)xtrabackup_binlog_info – mysql服务器当前正在使用的二进制日志文件及至备份这一刻为止二进制日志事件的位置。</p>
<p>(3)xtrabackup_binlog_pos_innodb – 二进制日志文件及用于InnoDB或XtraDB表的二进制日志文件的当前position。</p>
<p>(4)xtrabackup_binary – 备份中用到的xtrabackup的可执行文件；</p>
<p>(5)backup-my.cnf – 备份命令用到的配置选项信息；</p>
<p>在使用innobackupex进行备份时，还可以使用–no-timestamp选项来阻止命令自动创建一个以时间命名的目录；如此一来，innobackupex命令将会创建一个BACKUP-DIR目录来存储备份数据。</p>
<h4 id="准备-prepare-一个完全备份"><a href="#准备-prepare-一个完全备份" class="headerlink" title="准备(prepare)一个完全备份"></a>准备(prepare)一个完全备份</h4><p>一般情况下，在备份完成后，数据尚且不能用于恢复操作，因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务。因此，此时数据文件仍处理不一致状态。“准备”的主要作用正是通过回滚未提交的事务及同步已经提交的事务至数据文件也使得数据文件处于一致性状态。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# innobackupex --defaults-file&#x3D;&#x2F;usr&#x2F;local&#x2F;mysql-test3&#x2F;my.cnf --user&#x3D;root --password&#x3D;123456 --port&#x3D;3312 --host&#x3D;127.0.0.1 --apply-log &#x2F;home&#x2F;backup&#x2F;2017-08-08_15-28-09&#x2F;</span><br></pre></td></tr></table></figure>

<p>在实现“准备”的过程中，innobackupex通常还可以使用–use-memory选项来指定其可以使用的内存的大小，默认通常为100M。如果有足够的内存可用，可以多划分一些内存给prepare的过程，以提高其完成速度。</p>
<h4 id="从一个完全备份中恢复数据"><a href="#从一个完全备份中恢复数据" class="headerlink" title="从一个完全备份中恢复数据"></a>从一个完全备份中恢复数据</h4><p>innobackupex命令的–copy-back选项用于执行恢复操作，其通过复制所有数据相关的文件至mysql服务器DATADIR目录中来执行恢复过程。innobackupex通过backup-my.cnf来获取DATADIR目录的相关信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# rm -rf data&#x2F;*</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# innobackupex --defaults-file&#x3D;&#x2F;usr&#x2F;local&#x2F;mysql-test3&#x2F;my.cnf  --copy-back &#x2F;home&#x2F;backup&#x2F;2017-08-08_15-28-09&#x2F;</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# chown -R mysql:mysql data</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# killall mysqld</span><br><span class="line"></span><br><span class="line">#####可以不用启动mysql恢复数据#####</span><br></pre></td></tr></table></figure>
<h3 id="增量备份"><a href="#增量备份" class="headerlink" title="增量备份"></a>增量备份</h3><p>每个InnoDB的页面都会包含一个LSN信息，每当相关的数据发生改变，相关的页面的LSN就会自动增长。这正是InnoDB表可以进行增量备份的基础，即innobackupex通过备份上次完全备份之后发生改变的页面来实现。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# innobackupex --defaults-file&#x3D;&#x2F;usr&#x2F;local&#x2F;mysql-test3&#x2F;my.cnf --user&#x3D;root --password&#x3D;123456 --port&#x3D;3312 --host&#x3D;127.0.0.1 --incremental &#x2F;home&#x2F;backup&#x2F; --incremental-basedir&#x3D;&#x2F;home&#x2F;backup&#x2F;2017-08-08_15-28-09&#x2F;</span><br></pre></td></tr></table></figure>

<p>此命令执行结束后，innobackupex命令会在/backup目录中创建一个新的以时间命名的目录以存放所有的增量备份数据。另外，在执行过增量备份之后再一次进行增量备份时，其–incremental-basedir应该指向上一次的增量备份所在的目录。</p>
<p>需要注意的是，增量备份仅能应用于InnoDB或XtraDB表，对于MyISAM表而言，执行增量备份时其实进行的是完全备份。</p>
<p>“准备”(prepare)增量备份与整理完全备份有着一些不同，尤其要注意的是：<br>(1)需要在每个备份(包括完全和各个增量备份)上，将已经提交的事务进行“重放”。“重放”之后，所有的备份数据将合并到完全备份上。<br>(2)基于所有的备份将未提交的事务进行“回滚”。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# innobackupex --defaults-file&#x3D;&#x2F;usr&#x2F;local&#x2F;mysql-test3&#x2F;my.cnf --user&#x3D;root --password&#x3D;123456 --port&#x3D;3312 --host&#x3D;127.0.0.1 --apply-log --redo-only &#x2F;home&#x2F;backup&#x2F;2017-08-08_15-28-09&#x2F;</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# innobackupex --defaults-file&#x3D;&#x2F;usr&#x2F;local&#x2F;mysql-test3&#x2F;my.cnf --user&#x3D;root --password&#x3D;123456 --port&#x3D;3312 --host&#x3D;127.0.0.1 --apply-log --redo-only &#x2F;home&#x2F;backup&#x2F;2017-08-08_15-28-09&#x2F; --incremental-dir&#x3D;&#x2F;home&#x2F;backup&#x2F;2017-08-08_16-10-12&#x2F;</span><br></pre></td></tr></table></figure>

<h3 id="Xtrabackup的“流”及“备份压缩”功能"><a href="#Xtrabackup的“流”及“备份压缩”功能" class="headerlink" title="Xtrabackup的“流”及“备份压缩”功能"></a>Xtrabackup的“流”及“备份压缩”功能</h3><p>Xtrabackup对备份的数据文件支持“流”功能，即可以将备份的数据通过STDOUT传输给tar程序进行归档，而不是默认的直接保存至某备份目录中。要使用此功能，仅需要使用–stream选项即可。如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex --stream&#x3D;tar  &#x2F;backup | gzip &gt; &#x2F;backup&#x2F;&#96;date +%F_%H-%M-%S&#96;.tar.gz</span><br></pre></td></tr></table></figure>

<p>甚至也可以使用类似如下命令将数据备份至其它服务器：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex --stream&#x3D;tar  &#x2F;backup | ssh   &quot;cat -  &gt; &#x2F;backups&#x2F;&#96;date +%F_%H-%M-%S&#96;.tar&quot;</span><br></pre></td></tr></table></figure>


<p>此外，在执行本地备份时，还可以使用–parallel选项对多个文件进行并行复制。此选项用于指定在复制时启动的线程数目。当然，在实际进行备份时要利用此功能的便利性，也需要启用innodb_file_per_table选项或共享的表空间通过innodb_data_file_path选项存储在多个ibdata文件中。对某一数据库的多个文件的复制无法利用到此功能。其简单使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex --parallel  &#x2F;path&#x2F;to&#x2F;backup</span><br></pre></td></tr></table></figure>


<p>同时，innobackupex备份的数据文件也可以存储至远程主机，这可以使用–remote-host选项来实现：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex --remote-host&#x3D;root@ww agedu.com &#x2F;path&#x2F;IN&#x2F;REMOTE&#x2F;HOST&#x2F;to&#x2F;backup</span><br></pre></td></tr></table></figure>

<h3 id="导入或导出单张表"><a href="#导入或导出单张表" class="headerlink" title="导入或导出单张表"></a>导入或导出单张表</h3><p>默认情况下，InnoDB表不能通过直接复制表文件的方式在mysql服务器之间进行移植，即便使用了innodb_file_per_table选项。而使用Xtrabackup工具可以实现此种功能，不过，此时需要“导出”表的mysql服务器启用了innodb_file_per_table选项（严格来说，是要“导出”的表在其创建之前，mysql服务器就启用了innodb_file_per_table选项），并且“导入”表的服务器同时启用了innodb_file_per_table和innodb_expand_import选项。</p>
<p>(1)“导出”表<br>导出表是在备份的prepare阶段进行的，因此，一旦完全备份完成，就可以在preparef过程中通过–export选项将某表导出了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex --apply-log --export &#x2F;path&#x2F;to&#x2F;backup</span><br></pre></td></tr></table></figure>

<p>此命令会为每个innodb表的表空间创建一个以.exp结尾的文件，这些以.exp结尾的文件则可以用于导入至其它服务器。</p>
<p>(2)“导入”表<br>要在mysql服务器上导入来自于其它服务器的某innodb表，需要先在当前服务器上创建一个跟原表表结构一致的表，而后才能实现将表导入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE mytable (...)  ENGINE&#x3D;InnoDB;</span><br></pre></td></tr></table></figure>

<p>然后将此表的表空间删除：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; ALTER TABLE mydatabase.mytable  DISCARD TABLESPACE;</span><br></pre></td></tr></table></figure>

<p>接下来，将来自于“导出”表的服务器的mytable表的mytable.ibd和mytable.exp文件复制到当前服务器的数据目录，然后使用如下命令将其“导入”：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; ALTER TABLE mydatabase.mytable  IMPORT TABLESPACE;</span><br></pre></td></tr></table></figure>




<h3 id="使用Xtrabackup对数据库进行部分备份"><a href="#使用Xtrabackup对数据库进行部分备份" class="headerlink" title="使用Xtrabackup对数据库进行部分备份"></a>使用Xtrabackup对数据库进行部分备份</h3><p>Xtrabackup也可以实现部分备份，即只备份某个或某些指定的数据库或某数据库中的某个或某些表。但要使用此功能，必须启用innodb_file_per_table选项，即每张表保存为一个独立的文件。同时，其也不支持–stream选项，即不支持将数据通过管道传输给其它程序进行处理。</p>
<p>此外，还原部分备份跟还原全部数据的备份也有所不同，即你不能通过简单地将prepared的部分备份使用–copy-back选项直接复制回数据目录，而是要通过导入表的方向来实现还原。当然，有些情况下，部分备份也可以直接通过–copy-back进行还原，但这种方式还原而来的数据多数会产生数据不一致的问题，因此，无论如何不推荐使用这种方式。</p>
<p>(1)创建部分备份</p>
<p>创建部分备份的方式有三种：正则表达式(–include), 枚举表文件(–tables-file)和列出要备份的数据库(–databases)。</p>
<p>(a)使用–include<br>使用–include时，要求为其指定要备份的表的完整名称，即形如databasename.tablename，如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex --include&#x3D;&#39;^mageedu[.]tb1&#39;  &#x2F;path&#x2F;to&#x2F;backup</span><br></pre></td></tr></table></figure>

<p>(b)使用–tables-file<br>此选项的参数需要是一个文件名，此文件中每行包含一个要备份的表的完整名称；如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo -e &#39;mageedu.tb1\nmageedu.tb2&#39; &gt; &#x2F;tmp&#x2F;tables.txt</span><br><span class="line">innobackupex --tables-file&#x3D;&#x2F;tmp&#x2F;tables.txt  &#x2F;path&#x2F;to&#x2F;backup</span><br></pre></td></tr></table></figure>

<p>(c)使用–databases<br>此选项接受的参数为数据名，如果要指定多个数据库，彼此间需要以空格隔开；同时，在指定某数据库时，也可以只指定其中的某张表。此外，此选项也可以接受一个文件为参数，文件中每一行为一个要备份的对象。如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex --databases&#x3D;&#39;mageedu.tb1 testdb&quot;  &#x2F;path&#x2F;to&#x2F;backup</span><br></pre></td></tr></table></figure>

<p>(2)整理(preparing)部分备份</p>
<p>prepare部分备份的过程类似于导出表的过程，要使用–export选项进行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innobackupex --apply-log --export  &#x2F;pat&#x2F;to&#x2F;partial&#x2F;backup</span><br></pre></td></tr></table></figure>

<p>此命令执行过程中，innobackupex会调用xtrabackup命令从数据字典中移除缺失的表，因此，会显示出许多关于“表不存在”类的警告信息。同时，也会显示出为备份文件中存在的表创建.exp文件的相关信息。</p>
<p>(3)还原部分备份</p>
<p>还原部分备份的过程跟导入表的过程相同。当然，也可以通过直接复制prepared状态的备份直接至数据目录中实现还原，不要此时要求数据目录处于一致状态。</p>
]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>xtrabackup</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes 中拉取私有仓库(腾讯云)</title>
    <url>/posts/7d0560c7.html</url>
    <content><![CDATA[<p>官方介绍：<a href="https://kubernetes.io/docs/concepts/containers/images/">https://kubernetes.io/docs/concepts/containers/images/</a></p>
<a id="more"></a>

<h2 id="创建保存仓库地址和密码的secret"><a href="#创建保存仓库地址和密码的secret" class="headerlink" title="创建保存仓库地址和密码的secret"></a>创建保存仓库地址和密码的secret</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create secret docker-registry registry-secret --docker-server&#x3D;registry.cn-shenzhen.aliyuncs.com --docker-username&#x3D;user-test --docker-password&#x3D;xxxxxx --docker-email&#x3D;xxx@xxx.com</span><br><span class="line">--docker-server: 仓库地址</span><br><span class="line">--docker-username: 仓库登陆账号</span><br><span class="line">--docker-password: 仓库登陆密码</span><br><span class="line">--docker-email: 邮件地址(选填)</span><br></pre></td></tr></table></figure>

<h2 id="deployment-yml加入拉取镜像的密钥"><a href="#deployment-yml加入拉取镜像的密钥" class="headerlink" title="deployment.yml加入拉取镜像的密钥"></a>deployment.yml加入拉取镜像的密钥</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: foo</span><br><span class="line">  namespace: awesomeapps</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: foo</span><br><span class="line">      image: janedoe&#x2F;awesomeapp:v1</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">    - name: myregistrykey</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>利用NFS client provisioner动态提供Kubernetes后端存储卷</title>
    <url>/posts/c823abdd.html</url>
    <content><![CDATA[<p>利用NFS Server给Kubernetes作为持久存储的后端，并且动态提供PV。前提条件是有已经安装好的NFS服务器，并且NFS服务器与Kubernetes的Slave节点都能网络连通。<br>官方文档：<a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client">https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client</a></p>
<a id="more"></a>
<h2 id="nfs-client-provisioner"><a href="#nfs-client-provisioner" class="headerlink" title="nfs-client-provisioner"></a>nfs-client-provisioner</h2><p>nfs-client-provisioner 是一个Kubernetes的简易NFS的外部provisioner，本身不提供NFS，需要现有的NFS服务器提供存储</p>
<ul>
<li><p>PV以 ${namespace}-${pvcName}-${pvName}的命名格式提供（在NFS服务器上）</p>
</li>
<li><p>PV回收的时候以 archieved-${namespace}-${pvcName}-${pvName} 的命名格式（在NFS服务器上）</p>
</li>
</ul>
<h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><ul>
<li><p>修改deployment文件并部署 deploy/deployment.yaml<br>需要修改的地方只有NFS服务器所在的IP地址（10.10.10.60），以及NFS服务器共享的路径（/ifs/kubernetes），两处都需要修改为你实际的NFS服务器和共享目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes-incubator&#x2F;external-storage&#x2F;master&#x2F;nfs-client&#x2F;deploy&#x2F;deployment.yaml</span><br><span class="line"></span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-client-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nfs-client-provisioner</span><br><span class="line">      containers:</span><br><span class="line">        - name: nfs-client-provisioner</span><br><span class="line">          image: quay.io&#x2F;external_storage&#x2F;nfs-client-provisioner:latest</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: nfs-client-root</span><br><span class="line">              mountPath: &#x2F;persistentvolumes</span><br><span class="line">          env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: fuseim.pri&#x2F;ifs</span><br><span class="line">            - name: NFS_SERVER</span><br><span class="line">              value: 10.10.10.60</span><br><span class="line">            - name: NFS_PATH</span><br><span class="line">              value: &#x2F;ifs&#x2F;kubernetes</span><br><span class="line">      volumes:</span><br><span class="line">        - name: nfs-client-root</span><br><span class="line">          nfs:</span><br><span class="line">            server: 10.10.10.60</span><br><span class="line">            path: &#x2F;ifs&#x2F;kubernetes</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改StorageClass文件并部署 deploy/class.yaml<br>此处可以不修改，或者修改provisioner的名字，需要与上面的deployment的PROVISIONER_NAME名字一致。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes-incubator&#x2F;external-storage&#x2F;master&#x2F;nfs-client&#x2F;deploy&#x2F;class.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: storage.k8s.io&#x2F;v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: managed-nfs-storage</span><br><span class="line">provisioner: fuseim.pri&#x2F;ifs</span><br></pre></td></tr></table></figure>

<h2 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h2><p>kubernetes 1.6以上默认开启了rbac认证。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">    resources: [&quot;storageclasses&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;events&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;services&quot;, &quot;endpoints&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;]</span><br><span class="line">  - apiGroups: [&quot;extensions&quot;]</span><br><span class="line">    resources: [&quot;podsecuritypolicies&quot;]</span><br><span class="line">    resourceNames: [&quot;nfs-client-provisioner&quot;]</span><br><span class="line">    verbs: [&quot;use&quot;]</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;endpoints&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">---</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>

<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试创建PVC"><a href="#测试创建PVC" class="headerlink" title="测试创建PVC"></a>测试创建PVC</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes-incubator&#x2F;external-storage&#x2F;master&#x2F;nfs-client&#x2F;deploy&#x2F;test-claim.yaml</span><br><span class="line"></span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: test-claim</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io&#x2F;storage-class: &quot;managed-nfs-storage&quot;</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Mi</span><br></pre></td></tr></table></figure>

<h3 id="测试创建POD"><a href="#测试创建POD" class="headerlink" title="测试创建POD"></a>测试创建POD</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes-incubator&#x2F;external-storage&#x2F;master&#x2F;nfs-client&#x2F;deploy&#x2F;test-pod.yaml</span><br><span class="line"></span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: test-pod</span><br><span class="line">    image: gcr.io&#x2F;google_containers&#x2F;busybox:1.24  #如访问缘故可以直接采用busybox镜像</span><br><span class="line">    command:</span><br><span class="line">      - &quot;&#x2F;bin&#x2F;sh&quot;</span><br><span class="line">    args:</span><br><span class="line">      - &quot;-c&quot;</span><br><span class="line">      - &quot;touch &#x2F;mnt&#x2F;SUCCESS &amp;&amp; exit 0 || exit 1&quot;</span><br><span class="line">    volumeMounts:</span><br><span class="line">      - name: nfs-pvc</span><br><span class="line">        mountPath: &quot;&#x2F;mnt&quot;</span><br><span class="line">  restartPolicy: &quot;Never&quot;</span><br><span class="line">  volumes:</span><br><span class="line">    - name: nfs-pvc</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: test-claim</span><br></pre></td></tr></table></figure>

<p>在NFS服务器上的共享目录下的卷子目录中检查创建的NFS PV卷下是否有”SUCCESS” 文件。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernetes</tag>
        <tag>NFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Vmware虚拟机系统重启后系统无法进入解决</title>
    <url>/posts/30d20df4.html</url>
    <content><![CDATA[<p>故障提示：fsck.ext4: No such file or directory while trying to open /dev/sdc1<br>故障原因：如挂着硬盘无法读取或者被删掉</p>
<a id="more"></a>
<img src="/images/20180727110011.png" width="100%" height="100%">

<p>解决方法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入root密码会得到一个 root shell , 重新加载根文件系统:</span><br><span class="line">mount -o remount,rw &#x2F;</span><br><span class="line">vi &#x2F;etc&#x2F;fstab</span><br><span class="line">将 &#x2F;dev&#x2F;sdc1那行注释掉，保存退出后</span><br><span class="line">reboot</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title>利用ps工具统计CPU/MEM消耗高或者负载高的进程</title>
    <url>/posts/2f1b30b7.html</url>
    <content><![CDATA[<p>CPU消耗高往往是系统异常的一种表现。因为平常检查系统运行有哪些进程，都是使用ps工具。因此，出现这种情况时，如果能够通过ps工具搜寻出CPU消耗高的线程，则对进一步搜寻线索或者排查应该有帮助。</p>
<p>一个进程可能包含多个线程。但是考察CPU消耗问题，只考虑进程而不考虑线程是不够的。就是说，我们总是得给ps命令加上“-T”选项。</p>
<p>为了便于按照CPU消耗的情况排序，我们也通过“-o”定制ps输出的字段，以确保CPU消耗占比字段是第一个字段，这样便于排序。</p>
<a id="more"></a>
<h1 id="列出消耗高的线程"><a href="#列出消耗高的线程" class="headerlink" title="列出消耗高的线程"></a>列出消耗高的线程</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LANG&#x3D;C ps -eT -o%cpu,pid,tid,ppid,comm | grep -v CPU | sort -n -r | head -20</span><br><span class="line"></span><br><span class="line">#效果如下</span><br><span class="line">[root@es ~]# LANG&#x3D;C ps -eT -o%cpu,pid,tid,ppid,comm | grep -v CPU | sort -n -r | head -20</span><br><span class="line">26.0  1760  1782     1 java</span><br><span class="line">17.6  1760 22003     1 java</span><br><span class="line">17.0  1760 22006     1 java</span><br><span class="line">16.2  1760 21986     1 java</span><br><span class="line">16.0  1760 22024     1 java</span><br><span class="line">15.0  1760 22025     1 java</span><br><span class="line">14.8 13825 21917     1 java</span><br><span class="line">14.7 13825 21992     1 java</span><br><span class="line">14.2  1760 21989     1 java</span><br><span class="line">14.0  1760 22042     1 java</span><br><span class="line">13.2 13825 21993     1 java</span><br><span class="line">12.6  1760 22008     1 java</span><br><span class="line">12.0  1760 22027     1 java</span><br><span class="line">12.0 13825 22043     1 java</span><br><span class="line">11.0 13825 22047     1 java</span><br><span class="line">10.5  1760  1783     1 java</span><br><span class="line"> 8.3 13825 22016     1 java</span><br><span class="line"> 8.0  1760  5966     1 java</span><br><span class="line"> 8.0  1760  5897     1 java</span><br><span class="line"> 8.0 13825 14598     1 java</span><br></pre></td></tr></table></figure>

<h1 id="统计线程消耗的总的CPU"><a href="#统计线程消耗的总的CPU" class="headerlink" title="统计线程消耗的总的CPU"></a>统计线程消耗的总的CPU</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123; LANG&#x3D;C ps -eT -o%cpu,pid,tid,ppid,comm | sed -e &#39;s&#x2F;^ *&#x2F;&#x2F;&#39; | tr -s &#39; &#39; | grep -v CPU | sort -n -r | cut -d &#39; &#39; -f 1 | xargs -I&#123;&#125; echo -n &quot;&#123;&#125; + &quot; &amp;&amp; echo &#39; 0&#39;; &#125; | bc -l</span><br><span class="line"></span><br><span class="line">#效果如下</span><br><span class="line">[root@es ~]# &#123; LANG&#x3D;C ps -eT -o%cpu,pid,tid,ppid,comm | sed -e &#39;s&#x2F;^ *&#x2F;&#x2F;&#39; | tr -s &#39; &#39; | grep -v CPU | sort -n -r | cut -d &#39; &#39; -f 1 | xargs -I&#123;&#125; echo -n &quot;&#123;&#125; + &quot; &amp;&amp; echo &#39; 0&#39;; &#125; | bc -l</span><br><span class="line"></span><br><span class="line">356.1</span><br><span class="line">[root@es ~]#</span><br></pre></td></tr></table></figure>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>设想我们需要追踪CPU占用最高的线程，则可以这样</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export LANG&#x3D;C;d&#x3D;$(mktemp -d) &amp;&amp; cd $d # 建并进入临时目录</span><br><span class="line"># 捕捉之</span><br><span class="line">strace -f -ff -s 256 -tt -T -o strace.log -p $(ps -eT -o%cpu,pid,tid,ppid,comm | grep -v CPU | sort -n -r | head -1 | sed -e &#39;s&#x2F;^ *&#x2F;&#x2F;&#39; | tr -s &#39; &#39;  | cut -d &#39; &#39; -f 3)</span><br></pre></td></tr></table></figure>

<h1 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h1><p>线程是共享内存空间的。所以，一般情况下，没有必要使用“-T”选项。</p>
<p>列出内存消耗高的进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LANG&#x3D;C ps -e -o%mem,pid,tid,ppid,comm | grep -v MEM | sort -n -r | head -20</span><br></pre></td></tr></table></figure>

<p>统计内存消耗情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123; LANG&#x3D;C ps -e -o%mem,pid,tid,ppid,comm | sed -e &#39;s&#x2F;^ *&#x2F;&#x2F;&#39; | tr -s &#39; &#39; | grep -v MEM | sort -n -r | cut -d &#39; &#39; -f 1 | xargs -I&#123;&#125; echo -n &quot;&#123;&#125; + &quot; &amp;&amp; echo &#39; 0&#39;; &#125; | bc -l</span><br></pre></td></tr></table></figure>

<h1 id="列出导致CPU负载高的线程"><a href="#列出导致CPU负载高的线程" class="headerlink" title="列出导致CPU负载高的线程"></a>列出导致CPU负载高的线程</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LANG&#x3D;C ps -eTo stat,pid,tid,ppid,comm,args | perl -ne &#39;chomp;if (m!^\s*(\S*[RD]+\S*.*)!) &#123;print qq[$1\n];&#125;&#39;</span><br></pre></td></tr></table></figure>

<h1 id="tomcat-cpu过高"><a href="#tomcat-cpu过高" class="headerlink" title="tomcat cpu过高"></a>tomcat cpu过高</h1><p>1，根据top命令，发现PID为2633的Java进程占用CPU高达300%，出现故障。</p>
<p>2，找到该进程后，如何定位具体线程或代码呢，首先显示线程列表,并按照CPU占用高的线程排序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -mp 2633 -o THREAD,tid,time | sort -rn</span><br><span class="line"></span><br><span class="line">显示结果如下：</span><br><span class="line">USER     %CPU PRI SCNT WCHAN  USER SYSTEM   TID     TIME</span><br><span class="line">root     10.5  19    - -         -      -  3626 00:12:48</span><br><span class="line">root     10.1  19    - -         -      -  3593 00:12:16</span><br></pre></td></tr></table></figure>
<p>找到了耗时最高的线程3626，占用CPU时间有12分钟了！</p>
<p>3.将需要的线程ID转换为16进制格式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">printf &quot;%x\n&quot; 3626</span><br><span class="line">e18</span><br></pre></td></tr></table></figure>

<p>4.最后打印线程的堆栈信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jstack 2633 |grep e18 -A 30</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>日常问题处理</category>
      </categories>
      <tags>
        <tag>cpu</tag>
        <tag>mem</tag>
        <tag>ps</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS7 安装 systemtap</title>
    <url>/posts/81284e8b.html</url>
    <content><![CDATA[<h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><p>一个文件正在被进程写入数据，想查找具体是哪个程序进行写入，使用lsof 文件 命令无法找出。可以使用systemtap工具进行查找，linux下每个文件都会在某个块设备上存放，当然也都有相应的inode, 那么透过vfs.write我们就可以知道谁在不停的写入特定的设备上的inode。</p>
<a id="more"></a>

<h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><ul>
<li><p>Linux发行版本：CentOS Linux release 7.4.1708 (Core)</p>
</li>
<li><p>内核版本：3.10.0-693.5.2.el7.x86_64</p>
</li>
<li><p>uname -a: Linux aliy-nt1 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</p>
</li>
</ul>
<h1 id="安装SystemTap"><a href="#安装SystemTap" class="headerlink" title="安装SystemTap"></a>安装SystemTap</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yum install systemtap systemtap-runtime</span><br></pre></td></tr></table></figure>

<p>在运行SystemTap之前，还需要装必要的内核信息包。可以运行如下stap-prep来安装这些包，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># stap-prep</span><br></pre></td></tr></table></figure>
<p>运行stap-prep的时候，它探测出还要安装kernel-devel-3.10.0-693.5.2.el7.x86_64包和kernel-debuginfo-3.10.0-693.5.2.el7.x86_64包 (实际上还有kernel-debuginfo-common包)。如果安装失败可以自行下载指定内核版本进行安装。<br>下载地址如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;debuginfo.centos.org&#x2F;7&#x2F;x86_64&#x2F;</span><br></pre></td></tr></table></figure>

<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>为了测试stap是否能正常运行，用如下简单命令打印：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># stap -e &#39;probe begin&#123;printf(&quot;Hello, World&quot;); exit();&#125;&#39;</span><br><span class="line">Hello, World</span><br></pre></td></tr></table></figure>

<h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliy-nt1 ~]#vim inodewatch.stp</span><br><span class="line"></span><br><span class="line">#! &#x2F;usr&#x2F;bin&#x2F;env stap</span><br><span class="line"></span><br><span class="line">probe vfs.write, vfs.read</span><br><span class="line">&#123;</span><br><span class="line">  # dev and ino are defined by vfs.write and vfs.read</span><br><span class="line">  if (dev &#x3D;&#x3D; MKDEV($1,$2) # major&#x2F;minor device</span><br><span class="line">      &amp;&amp; ino &#x3D;&#x3D; $3)</span><br><span class="line">    printf (&quot;%s(%d) %s 0x%x&#x2F;%u\n&quot;,</span><br><span class="line">      execname(), pid(), probefunc(), dev, ino)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@aliy-nt1 ~]# ls -al &#x2F;dev&#x2F;vda1 </span><br><span class="line">brw-rw---- 1 root disk 253, 1 10月 17 2018 &#x2F;dev&#x2F;vda1</span><br><span class="line"></span><br><span class="line">[root@aliy-nt1 ~]# stat -c &#39;%i&#39; &#x2F;var&#x2F;spool&#x2F;cron&#x2F;root </span><br><span class="line">530697</span><br><span class="line"></span><br><span class="line">[root@aliy-nt1 ~]# stap inodewatch.stp 253 1 530697</span><br><span class="line">crontab(5846) vfs_read 0xfd00001&#x2F;530697</span><br><span class="line">crontab(5846) vfs_read 0xfd00001&#x2F;530697</span><br><span class="line">crontab(5884) vfs_read 0xfd00001&#x2F;530697</span><br><span class="line">crontab(5884) vfs_read 0xfd00001&#x2F;530697</span><br></pre></td></tr></table></figure>

<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>文件锁住，禁止写入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#解锁</span><br><span class="line">chattr -i *</span><br><span class="line"></span><br><span class="line">#加锁</span><br><span class="line">chattr +i *</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>日常部署记录</category>
      </categories>
      <tags>
        <tag>systemtap</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos 7 安装vsftpd</title>
    <url>/posts/1c5e1cc3.html</url>
    <content><![CDATA[<h2 id="安装vsftpd"><a href="#安装vsftpd" class="headerlink" title="安装vsftpd"></a>安装vsftpd</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install vsftpd</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="配置信息-本地账户登录模式"><a href="#配置信息-本地账户登录模式" class="headerlink" title="配置信息(本地账户登录模式)"></a>配置信息(本地账户登录模式)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@web1 ~]# grep -v &quot;#&quot; &#x2F;etc&#x2F;vsftpd&#x2F;vsftpd.conf </span><br><span class="line">anonymous_enable&#x3D;NO</span><br><span class="line">local_enable&#x3D;YES</span><br><span class="line">write_enable&#x3D;YES</span><br><span class="line">local_umask&#x3D;022</span><br><span class="line">dirmessage_enable&#x3D;YES</span><br><span class="line">xferlog_enable&#x3D;YES</span><br><span class="line">connect_from_port_20&#x3D;YES</span><br><span class="line">xferlog_file&#x3D;&#x2F;var&#x2F;log&#x2F;xferlog</span><br><span class="line">xferlog_std_format&#x3D;YES</span><br><span class="line">ascii_upload_enable&#x3D;YES</span><br><span class="line">ascii_download_enable&#x3D;YES</span><br><span class="line">chroot_list_enable&#x3D;YES</span><br><span class="line">chroot_list_file&#x3D;&#x2F;etc&#x2F;vsftpd&#x2F;chroot_list</span><br><span class="line">listen&#x3D;NO</span><br><span class="line">listen_port&#x3D;2222</span><br><span class="line">listen_ipv6&#x3D;YES</span><br></pre></td></tr></table></figure>
<p>被动模式<br>pasv_enable=YES<br>pasv_min_port=30000<br>pasv_max_port=50000</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">pam_service_name&#x3D;vsftpd</span><br><span class="line">userlist_enable&#x3D;YES</span><br><span class="line">tcp_wrappers&#x3D;YES</span><br><span class="line">allow_writeable_chroot&#x3D;YES</span><br></pre></td></tr></table></figure>

<h2 id="新增用户"><a href="#新增用户" class="headerlink" title="新增用户"></a>新增用户</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@web1 vsftpd]# useradd -d &#x2F;data&#x2F;ftp -g ftp -s &#x2F;sbin&#x2F;nologin ftpuser</span><br><span class="line">[root@web1 data]# chown -R ftpuser:ftp &#x2F;data&#x2F;ftp</span><br><span class="line">[root@web1 data]# passwd ftpuser</span><br></pre></td></tr></table></figure>

<h2 id="新增文件权限"><a href="#新增文件权限" class="headerlink" title="新增文件权限"></a>新增文件权限</h2><p>如果希望某个用户可以访问根目录，把用户名加入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@web1 ~]# echo &quot;ftpuser&quot; &gt;&gt; &#x2F;etc&#x2F;vsftpd&#x2F;chroot_list</span><br></pre></td></tr></table></figure>

<h2 id="启动vsftp"><a href="#启动vsftp" class="headerlink" title="启动vsftp"></a>启动vsftp</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start  vsftpd</span><br><span class="line">systemctl enable vsftpd</span><br></pre></td></tr></table></figure>

<h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><p>windows 如IE访问ftp会提示200 Switching to ASCII mode，解决方案是打开ie浏览器-工具-internet选项-高级-不勾选使用被动FTP即可。</p>
<h2 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h2><p>问题：<br>常规部署完成后，帐号密码正确，一直提示无法登录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">331 Please specify the password.</span><br><span class="line">Password:</span><br><span class="line">530 Login incorrect.</span><br><span class="line">Login failed.</span><br></pre></td></tr></table></figure>

<p>解决方案：</p>
<p>查看 /etc/vsftpd/vsftpd.conf 查到 pam_service_name=vsftpd ，可知认证pam 文件位于 /etc/pam.d/vsftpd<br>查看 /etc/pam.d/vsftpd 看到一行 auth required pam_shells.so ,因为创建ftp账户时候，禁止了ssh登陆 所以此处应该改为 pam_nologin.so<br>重启 systemctl restart vsftpd.service  ，可以正常登陆</p>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>ftp</tag>
        <tag>vsftp</tag>
      </tags>
  </entry>
  <entry>
    <title>vxlan</title>
    <url>/posts/891a4efc.html</url>
    <content><![CDATA[<h1 id="VXLAN协议"><a href="#VXLAN协议" class="headerlink" title="VXLAN协议"></a>VXLAN协议</h1><p>VXLAN是Virtual eXtensible Local Area Network的缩写,虚拟可扩展的局域网，RFC 7348的标题“A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks”，说明了VXLAN是一个在传统Layer 3网络上架设出来的Layer 2 overlay网络。RFC Abstract如下：</p>
<a id="more"></a>
<p>This document describes Virtual eXtensible Local Area Network (VXLAN), which is used to address the need for overlay networks within virtualized data centers accommodating multiple tenants.  The scheme and the related protocols can be used in networks for cloud service providers and enterprise data centers.  This memo documents the deployed VXLAN protocol for the benefit of the Internet community.</p>
<h1 id="Vxlan报文格式"><a href="#Vxlan报文格式" class="headerlink" title="Vxlan报文格式"></a>Vxlan报文格式</h1><img src="/images/676015-20160315143504990-1334194838.png" width="100%" height="100%">

<p>Outer UDP端口使用4798，但可以修改</p>
<p>Outer IP头封装：源IP为发送报文的虚拟机所属的VTEP的IP地址，目的IP为目的虚拟机所属的VTEP IP地址。</p>
<p>Outer的目的IP地址可以是单播和组播地址，单播的情况下，目的IP为VTEP(Vxlan Tunnel End Point）的IP地址，在多播的情况下引入VXLAN的管理层，利用VNI和IP多播组的映射来确定VTEP。</p>
<p>当目的IP为接收端的VTEP的IP时，假如不知道这个IP地址，则需要执行ARP请求来获取，步骤如下：</p>
<ol>
<li>目标IP被替换成与源虚拟机具有相同VNI的多播组IP地址；</li>
<li>所有VTEP端都接收该多播报文，VTEP查找所在主机上的全部虚拟机来匹配源虚拟机的Inner目的MAC。</li>
<li>目标VTEP的虚拟机会回应该多播包，从而获得目标VTEP的IP地址。</li>
<li>发送端VTEP添加VNI-VTEP-虚拟机MAC的映射关系到自己的VXLAN表中，以避免再次组播学习。</li>
</ol>
<p>Outer 以太封装：SA为发送报文的虚拟机所属的VTEP MAC地址，DA为目的虚拟机所属的VTEP上路由表中下一跳MAC地址。</p>
<h1 id="VXLAN网络结构"><a href="#VXLAN网络结构" class="headerlink" title="VXLAN网络结构"></a>VXLAN网络结构</h1><img src="/images/676015-20160315153757084-1317081284.png" width="100%" height="100%">

<p>NVE(Network Virtrualization Edge网络虚拟边缘节点）是实现网络虚拟化的功能实体，VM里的报文经过NVE封装后，NVE之间就可以在基于L3的网络基础上建立起L2虚拟网络。网络设备实体以及服务器实体上的VSwitch都可以作为NVE。</p>
<p>VTEP为VXLAN隧道的端点，封装在NVE中，用于VXLAN报文的封装和解封装。VTEP与物理网络相连，分配的地址为物理网络IP地址。VXLAN报文中源IP地址为本节点的VTEP地址，VXLAN报文中目的IP地址为对端节点的VTEP地址，一对VTEP地址就对应着一个VXLAN隧道。</p>
<p>VNI（VXLAN Network Identifier）：VXLAN网络标识VNI类似VLAN ID，用于区分VXLAN段，不同VXLAN段的虚拟机不能直接二层相互通信。一个VNI表示一个租户，即使多个终端用户属于同一个VNI，也表示一个租户。VNI由24比特组成，支持多达16M（(2^24-1)/1024^2）的租户。</p>
<p>VAP（Virtual Access Point）：虚拟接入点VAP统一为二层子接口，用于接入数据报文。为二层子接口配置不同的流封装，可实现不同的数据报文接入不同的二层子接口。</p>
<h2 id="Vxlan网关"><a href="#Vxlan网关" class="headerlink" title="Vxlan网关"></a>Vxlan网关</h2><p>VM之间的通信模式主要有3种：同VNI下的不同VM（分布在同一实体和不同实体两种），不同VNI下的跨网访问，VXLAN和非VXLAN之间的访问。</p>
<p>VXLAN网关分为：</p>
<p>二层网关：位于同一网段的终端用户通信，L2网关收到用户报文后，根据报文中包含的目的MAC类型，报文转发流程分为：</p>
<ol>
<li><p>MAC地址为BUM（broadcast&amp;unknown-unicast&amp;multicast）地址，按照 BUM报文转发流程进行处理</p>
</li>
<li><p>MAC地址为已知单播地址，按照已知单播报文转发流程进行处理</p>
</li>
</ol>
<p>三层网关：用于非同一网段的终端用户通信或VXLAN和非VXLAN用户间的通信。<br><img src="/images/676015-20160316000214709-466929025.png" width="100%" height="100%"></p>
<h3 id="VXLAN二层网关"><a href="#VXLAN二层网关" class="headerlink" title="VXLAN二层网关"></a>VXLAN二层网关</h3><p>L2网关主要解决的就是同一VNI下的VM之间的互访。</p>
<h3 id="VXLAN-三层网关"><a href="#VXLAN-三层网关" class="headerlink" title="VXLAN 三层网关"></a>VXLAN 三层网关</h3><p>L3网关解决的就是不同VNI以及VXLAN和非VXLAN之间的互访。L3网关分为集中式网关和分布式网关，这2者的主要区别就在于L3网关是在leaf上还是在spine上。<br><img src="/images/676015-20160316001306506-1565112580.png" width="100%" height="100%"><br><img src="/images/676015-20160316001323740-1658139469.png" width="100%" height="100%"><br>如上图所示，集中式L3 GW在spine上，而分布式则是L2，L3 GW都在leaf上。当网络规模较大时，需采用分布式网关，因集中式spine的ARP表项瓶颈，并且子网流量转发绕远。分布式网关时，VTEP在leaf上。spine节点不感知VXLAN隧道。</p>
<p>需要注意的是当VM1和VM2在同一子网，但是挂在不同leaf上时，他们的三层网关需要配置相同的网关IP地址和MAC地址，当租户的VM位置移动时，不需要更改网关任何配置。</p>
<h3 id="BUM报文转发流程"><a href="#BUM报文转发流程" class="headerlink" title="BUM报文转发流程"></a>BUM报文转发流程</h3><p>当BUM报文进入VXLAN隧道时，接入端VTEP采用头端复制方式（接口收到BUM报文后本地VTEP通过控制平面获取同一VNI的VTEP列表，将收到的BUM报文根据VTEP列表进行复制并发送给属于同一VNI的所有VTEP））进行报文的VXLAN封装，BUM出VXLAN隧道时，出口端VTEP对报文解封装，</p>
<p>综述：VXLAN网络结构定义完了，这里用通俗易懂的话解释一遍：在一台实体服务器上可以虚拟出一个交换机来，这个交换机就是VSwitch，而这个VSwitch下挂的不再是实体服务器，而是一个个VM，一个VM其实就是一个租户租用的服务器，不同租户之间肯定是不能互访的，要不然租户数据的安全性如何保障，这个隔离就是靠的VNI这个ID，其实这个你可以向下VLAN是如何隔离的，目的就是为了隔离租户。我一个租户有2个VM的话，那么我这2个之间应该可以互访吧。所以说基于VNI定义的租户，而非基于VM。内部的结构说清楚了再来说上行如何访问，在一个L2交换机你要跨网访问必然要经过网关，这个网关的IP地址就是VTEP IP，在网络上有个概念叫arp-proxy，一般用途是为了保护内部私有网络，外界的所有应答都有网关来代替回答（可以理解为门卫）。在这里外界只需要你的VTEP IP即可，对端报文到达VTEP这个网关后自己在内部走L2进行转发。因此VXLAN报文中的目的IP就是对端的网关（VTEP IP），而源地址自然也是自己的网关（VTEP IP）。而对于不同leaf上的同一VNI的VM来说，他们的VTEP IP肯定要配置相同，想下同一vlan下的服务器的网关是如何配置的就明白了。</p>
]]></content>
  </entry>
  <entry>
    <title>CentOS7 yum 安装mysql 5.7</title>
    <url>/posts/d67fa6b1.html</url>
    <content><![CDATA[<h1 id="下载-MySQL-Yum-Repository"><a href="#下载-MySQL-Yum-Repository" class="headerlink" title="下载 MySQL Yum Repository"></a>下载 MySQL Yum Repository</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;dev.mysql.com&#x2F;get&#x2F;mysql80-community-release-el7-1.noarch.rpm</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="添加-MySQL-Yum-Repository"><a href="#添加-MySQL-Yum-Repository" class="headerlink" title="添加 MySQL Yum Repository"></a>添加 MySQL Yum Repository</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -ivh mysql80-community-release-el7-1.noarch.rpm</span><br><span class="line">或者</span><br><span class="line">yum localinstall mysql80-community-release-el7-1.noarch.rpm</span><br></pre></td></tr></table></figure>

<h1 id="验证下是否添加成功"><a href="#验证下是否添加成功" class="headerlink" title="验证下是否添加成功"></a>验证下是否添加成功</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum repolist enabled | grep &quot;mysql.*-community.*&quot;</span><br><span class="line">&#x2F;&#x2F; 查看 YUM 仓库关于 MySQL 的所有仓库列表</span><br><span class="line">yum repolist all | grep mysql</span><br><span class="line">&#x2F;&#x2F; 只查看启用的</span><br><span class="line">yum repolist enabled | grep mysql</span><br></pre></td></tr></table></figure>

<h1 id="选择要启用-MySQL-版本"><a href="#选择要启用-MySQL-版本" class="headerlink" title="选择要启用 MySQL 版本"></a>选择要启用 MySQL 版本</h1><p>查看 MySQL 版本，执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum repolist all | grep mysql</span><br></pre></td></tr></table></figure>

<p>可以通过类似下面的语句来启动某些版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 安装 YUM 管理工具包，此包提供了 yum-config-manager 命令工具</span><br><span class="line">yum install yum-utils</span><br><span class="line">禁用 8.0</span><br><span class="line">yum-config-manager --disable mysql80-community</span><br><span class="line">启用 5.7</span><br><span class="line">yum-config-manager --enable mysql57-community</span><br></pre></td></tr></table></figure>
<h1 id="通过-Yum-来安装-MySQL"><a href="#通过-Yum-来安装-MySQL" class="headerlink" title="通过 Yum 来安装 MySQL"></a>通过 Yum 来安装 MySQL</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install mysql-community-server</span><br></pre></td></tr></table></figure>

<h1 id="管理mysql"><a href="#管理mysql" class="headerlink" title="管理mysql"></a>管理mysql</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 启动</span><br><span class="line">systemctl start mysqld.service</span><br><span class="line">&#x2F;&#x2F; 查看状态</span><br><span class="line">systemctl status mysqld.service</span><br><span class="line">&#x2F;&#x2F; 开机自启动</span><br><span class="line">systemctl enable mysqld.server</span><br><span class="line">&#x2F;&#x2F; 查看监听端口，默认 3306</span><br><span class="line">ss -natl |grep 3306</span><br></pre></td></tr></table></figure>

<h1 id="MySQL-安全设置（设置密码）或者跳过"><a href="#MySQL-安全设置（设置密码）或者跳过" class="headerlink" title="MySQL 安全设置（设置密码）或者跳过"></a>MySQL 安全设置（设置密码）或者跳过</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql_secure_installation</span><br></pre></td></tr></table></figure>



<h1 id="初始化-Mysql"><a href="#初始化-Mysql" class="headerlink" title="初始化 Mysql"></a>初始化 Mysql</h1><p>在 MySQL 服务器初始启动时，如果服务器的数据目录为空，则会发生以下情况：</p>
<ul>
<li>MySQL 服务器已初始化。</li>
<li>在数据目录中生成SSL证书和密钥文件。</li>
<li>该<a href="https://dev.mysql.com/doc/refman/8.0/en/validate-password.html">validate_password插件</a>安装并启用。</li>
<li>将创建一个超级用户 帐户<code>&#39;root&#39;@&#39;localhost&#39;</code>。并会设置超级用户的密码，将其存储在错误日志文件中。要显示它，请使用以下命令：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep &#39;temporary password&#39; &#x2F;var&#x2F;log&#x2F;mysqld.log</span><br></pre></td></tr></table></figure></li>
<li>通过上面日志中的临时密码登录并为超级用户帐户设置自定义密码：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqladmin -p&#39;%#ootFm%a80&#39; password  &#39;%#ootFm%a801&#39;</span><br></pre></td></tr></table></figure>

<p><strong>取消密码复杂度</strong></p>
<ul>
<li>编辑 <code>my.cnf</code>配置文件, 在 <code>[mysqld]</code>配置块儿中添加如下内容<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">plugin-load&#x3D;validate_password.so </span><br><span class="line">validate-password&#x3D;OFF</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h1 id="远程访问设置"><a href="#远程访问设置" class="headerlink" title="远程访问设置"></a>远程访问设置</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt;use mysql;  </span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO root@&quot;%&quot; IDENTIFIED BY &quot;root&quot;;    &#x2F;&#x2F;为root添加远程连接的能力  </span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>

<h1 id="数据存储位置更改"><a href="#数据存储位置更改" class="headerlink" title="数据存储位置更改"></a>数据存储位置更改</h1><p>先关闭数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp -rp &#x2F;var&#x2F;lib&#x2F;mysql &#x2F;data&#x2F;</span><br><span class="line"></span><br><span class="line">-p：源目录或者文件的属性保留</span><br></pre></td></tr></table></figure>

<p>修改/etc/my.cnf文件，否则会提示ERROR 2002 (HY000): Can’t connect to local MySQL server through socket ‘/var/lib/mysql/mysql.sock’</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ln –s &#x2F;data&#x2F;mysql&#x2F;mysql.sock &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">[client]</span><br><span class="line">socket &#x3D; &#x2F;data&#x2F;mysql&#x2F;mysql.sock</span><br><span class="line">[mysqld]</span><br><span class="line">socket &#x3D; &#x2F;data&#x2F;mysql&#x2F;mysql.sock</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>zabbix延迟通知以及触发器表达式</title>
    <url>/posts/ba3333a3.html</url>
    <content><![CDATA[<p>接到一个需求，要求zabbix不需要立即告警，找了相关资料，以及探索，可使用一下几个方法解决。</p>
<a id="more"></a>
<h2 id="动作"><a href="#动作" class="headerlink" title="动作"></a>动作</h2><p>设置如图所示：<br><img src="/images/202005280854.png" width="100%" height="100%"></p>
<p>说明：<br>1、步骤持续时间：周期内发送通知<br>2、操作细节，步骤6代表第六个周期内才开始发送通知，0代表无穷大，意味着之后将会以周期发送预警通知。</p>
<h2 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h2><p>Zabbix触发器的语法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&lt;server&gt;:&lt;key&gt;.&lt;function&gt;(&lt;parameter&gt;)&#125;&lt;operator&gt;&lt;constant&gt;</span><br></pre></td></tr></table></figure>
<p>用Template App Zabbix Agent模板中的主机ping监控的触发器来进行说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;Template App Zabbix Agent:agent.ping.nodata(5m)&#125;&#x3D;1</span><br><span class="line"></span><br><span class="line">&lt;server&gt;    Template App Zabbix Agent 即监控模板</span><br><span class="line">&lt;key&gt;    agent.ping 即监控模板里的项目Items</span><br><span class="line">&lt;function&gt;     nodata() 及项目所使用的方法</span><br><span class="line">&lt;parameter&gt;    5m 及方法所使用的参数&lt;operator&gt;　　操作人，选填&lt;constant&gt;　　持续性，选填</span><br></pre></td></tr></table></figure>
<h3 id="Zabbix支持的function"><a href="#Zabbix支持的function" class="headerlink" title="Zabbix支持的function"></a>Zabbix支持的function</h3><p>参考官网：<a href="https://www.zabbix.com/documentation/2.2/manual/appendix/triggers/functions">https://www.zabbix.com/documentation/2.2/manual/appendix/triggers/functions</a></p>
<h3 id="官网实例的表达式举例"><a href="#官网实例的表达式举例" class="headerlink" title="官网实例的表达式举例"></a>官网实例的表达式举例</h3><h4 id="示例1"><a href="#示例1" class="headerlink" title="示例1"></a>示例1</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：Processor load is too high on www.zabbix.com</span><br><span class="line">&#123;www.zabbix.com:system.cpu.load[all,avg1].last(0)&#125;&gt;5</span><br><span class="line">触发器说明：</span><br><span class="line">www.zabbix.com：host名称</span><br><span class="line">system.cpu.load[all,avg1]：item值,一分内cpu平均负载值</span><br><span class="line">last(0)：最新值</span><br><span class="line">&gt;5：最新值大于5</span><br><span class="line">如上所示，www.zabbix.com这个主机的监控项，最新的CPU负载值如果大于5，那么表达式会返回true，这样一来触发器状态就改变为“problem”了。</span><br></pre></td></tr></table></figure>
<h4 id="示例2"><a href="#示例2" class="headerlink" title="示例2"></a>示例2</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：www.zabbix.com is overloaded</span><br><span class="line">&#123;www.zabbix.com:system.cpu.load[all,avg1].last(0)&#125;&gt;5|&#123;www.zabbix.com:system.cpu.load[all,avg1].min(10m)&#125;&gt;2 </span><br><span class="line">当前cpu负载大于5或者最近10分内的cpu负载大于2，那么表达式将会返回true.</span><br></pre></td></tr></table></figure>
<h4 id="示例3"><a href="#示例3" class="headerlink" title="示例3"></a>示例3</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：&#x2F;etc&#x2F;passwd has been changed 使用函数 diff():</span><br><span class="line">&#123;www.zabbix.com:vfs.file.cksum[&#x2F;etc&#x2F;passwd].diff(0)&#125;&gt;0 </span><br><span class="line">&#x2F;etc&#x2F;passwd最新的checksum与上一次获取到的checksum不同，表达式将会返回true. 我们可以使用同样的方法监控系统重要的配置文件,例如&#x2F;etc&#x2F;passwd,&#x2F;etc&#x2F;inetd.conf等等。这些zabbix一般都会自带，没带的你自己加上吧。</span><br></pre></td></tr></table></figure>
<h4 id="示例4"><a href="#示例4" class="headerlink" title="示例4"></a>示例4</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：Someone is downloading a large file from the Internet 使用函数 min:</span><br><span class="line">&#123;www.zabbix.com:net.if.in[eth0,bytes].min(5m)&#125;&gt;100K </span><br><span class="line">当前主机网卡eth0最后5分钟内接收到的流量超过100KB那么触发器表达式将会返回true</span><br></pre></td></tr></table></figure>
<h4 id="示例5"><a href="#示例5" class="headerlink" title="示例5"></a>示例5</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：Both nodes of clustered SMTP server are down</span><br><span class="line">&#123;smtp1.zabbix.com:net.tcp.service[smtp].last(0)&#125;&#x3D;0 &amp; &#123;smtp2.zabbix.com:net.tcp.service[smtp].last(0)&#125;&#x3D;0 </span><br><span class="line">当smtp1.zabbix.com和smtp2.zabbix.com两台主机上的SMTP服务器都离线，表达式将会返回true.</span><br></pre></td></tr></table></figure>
<h4 id="示例6"><a href="#示例6" class="headerlink" title="示例6"></a>示例6</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：Zabbix agent needs to be upgraded 使用函数str():</span><br><span class="line">&#123;zabbix.zabbix.com:agent.version.str(&quot;beta8&quot;)&#125;&#x3D;1 </span><br><span class="line">如果当前zabbix agent版本包含beta8（假设当前版本为1.0beta8），这个表达式会返回true.</span><br></pre></td></tr></table></figure>
<h4 id="示例7"><a href="#示例7" class="headerlink" title="示例7"></a>示例7</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：Server is unreachable</span><br><span class="line">&#123;zabbix.zabbix.com:icmpping.count(30m,0)&#125;&gt;5 </span><br><span class="line">如上表达式表示最近30分钟zabbix.zabbix.com这个主机超过5次不可到达。</span><br></pre></td></tr></table></figure>
<h4 id="示例8"><a href="#示例8" class="headerlink" title="示例8"></a>示例8</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：No heartbeats within last 3 minutes 使用函数 nodata():</span><br><span class="line">&#123;zabbix.zabbix.com:tick.nodata(3m)&#125;&#x3D;1 </span><br><span class="line">tick为Zabbix trapper类型，首先我们要定义一个类型为Zabbix trapper，key为tick的item。我们使用zabbix_sender定期发送数据给tick，如果在3分钟内还未收到zabbix_sender发送来的数据，那么表达式返回一个true，与此同时触发器的值变为“PROBLEM”。</span><br></pre></td></tr></table></figure>
<h4 id="示例9"><a href="#示例9" class="headerlink" title="示例9"></a>示例9</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：CPU activity at night time 使用函数 time():</span><br><span class="line">&#123;zabbix:system.cpu.load[all,avg1].min(5m)&#125;&gt;2 &amp; &#123;zabbix:system.cpu.load[all,avg1].time(0)&#125;&gt;000000 &amp; &#123;zabbix:system.cpu.load[all,avg1].time(0)&#125;&lt;060000 </span><br><span class="line">只有在凌晨0点到6点整，最近5分钟内cpu负载大于2，表达式返回true，触发器的状态变更为“problem”</span><br></pre></td></tr></table></figure>
<h4 id="示例10"><a href="#示例10" class="headerlink" title="示例10"></a>示例10</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：Check if client local time is in sync with Zabbix server time 使用函数 fuzzytime():</span><br><span class="line">&#123;MySQL_DB:system.localtime.fuzzytime(10)&#125;&#x3D;0 </span><br><span class="line">主机MySQL_DB当前服务器时间如果与zabbix server之间的时间相差10秒以上，表达式返回true，触发器状态改变为“problem”</span><br></pre></td></tr></table></figure>
<h4 id="示例11"><a href="#示例11" class="headerlink" title="示例11"></a>示例11</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：Comparing average load today with average load of the same time yesterday (使用 time_shift 时间偏移量参数).</span><br><span class="line">&#123;server:system.cpu.load.avg(1h)&#125; &#x2F; &#123;server:system.cpu.load.avg(1h,1d)&#125;&gt;2 </span><br><span class="line">This expression will fire if the average load of the last hour tops the average load of the same hour yesterday more than two times.</span><br><span class="line">最新一小时的平均负载峰值超过昨天同时段指标两次进行报警</span><br></pre></td></tr></table></figure>

<h3 id="特性之Hysteresis（迟滞-滞后）"><a href="#特性之Hysteresis（迟滞-滞后）" class="headerlink" title="特性之Hysteresis（迟滞,滞后）"></a>特性之Hysteresis（迟滞,滞后）</h3><p>简单的说触发器状态转变为problem需要一个条件，从problem转变回来还需要一个条件才行。一般触发器只需要不满足触发器为problem条件即可恢复。明白了么？不明白就看例子吧。 有时候触发器需要使用不同的条件来表示不同的状态，举个官网很有趣的例子：机房温度正常稳定为15-20°，当温度超过20°，触发器值为problem，当前情况下，只有温度处在这个温度之间触发器值才会为FALSE。（慢慢理解，这个表达式有点绕） 为了达到这个效果，我们需要使用如下触发器表达式:</p>
<h4 id="示例1-1"><a href="#示例1-1" class="headerlink" title="示例1"></a>示例1</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：Temperature in server room is too high</span><br><span class="line">(&#123;TRIGGER.VALUE&#125;&#x3D;0&amp;&#123;server:temp.last(0)&#125;&gt;20) | (&#123;TRIGGER.VALUE&#125;&#x3D;1&amp;&#123;server:temp.last(0)&#125;&lt;15) </span><br><span class="line">注意：宏变量 &#123;TRIGGER.VALUE&#125;将会返回当前触发器的值</span><br></pre></td></tr></table></figure>
<h4 id="实例2"><a href="#实例2" class="headerlink" title="实例2"></a>实例2</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">触发器名称：Free disk space is too low </span><br><span class="line">Problem: 最近5分钟内剩余磁盘空间小于10GB。 </span><br><span class="line">Recovery: 最近10分钟内磁盘空间大于40GB</span><br><span class="line">简单说便是一旦剩余空间小于10G就触发异常，然后接下来剩余空间必须大于40G才能解除这个异常，就算你剩余空间达到了39G（不在报警条件里）。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(&#123;TRIGGER.VALUE&#125;&#x3D;0&amp;&#123;server:vfs.fs.size[&#x2F;,free].max(5m)&#125;&lt;10G) | (&#123;TRIGGER.VALUE&#125;&#x3D;1&amp;&#123;server:vfs.fs.size[&#x2F;,free].min(10m)&#125;&lt;40G)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title>winscp 普通用户登录sftp后切换到root权限</title>
    <url>/posts/9970a01b.html</url>
    <content><![CDATA[<p>工具： Xshell、winscp<br>服务器环境： linux RedHat 6.4<br>遇到的问题：普通用户使用winscp账户登录服务器，没有操作权限！</p>
<a id="more"></a>

<h2 id="一、普通用户，通过Xshell登录服务器。"><a href="#一、普通用户，通过Xshell登录服务器。" class="headerlink" title="一、普通用户，通过Xshell登录服务器。"></a>一、普通用户，通过Xshell登录服务器。</h2><p>输入以下命令，再输入密码。切换为root。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su</span><br></pre></td></tr></table></figure>

<h2 id="二、先查找sftp-server-文件夹所在的系统路径，得到sftp-server文件路径后。"><a href="#二、先查找sftp-server-文件夹所在的系统路径，得到sftp-server文件路径后。" class="headerlink" title="二、先查找sftp-server 文件夹所在的系统路径，得到sftp-server文件路径后。"></a>二、先查找sftp-server 文件夹所在的系统路径，得到sftp-server文件路径后。</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@LJS-CSS-APP3 ~]# cat &#x2F;etc&#x2F;ssh&#x2F;sshd_config|grep sftp</span><br><span class="line">[root@LJS-CSS-APP3 ~]# &#x2F;usr&#x2F;libexec&#x2F;openssh&#x2F;sftp-server</span><br></pre></td></tr></table></figure>
<img src="/images/stfp.png" width="100%" height="100%">

<p>再输入vim命令来编辑修改 /etc/sudoers配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@LJS-CSS-APP3 ~]# vim &#x2F;etc&#x2F;sudoers 或者visudo</span><br></pre></td></tr></table></figure>

<p>修改内容具体如下：<br>** 1、在文本中找到下语句 **</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root ALL&#x3D;(ALL) ALL</span><br></pre></td></tr></table></figure>

<p>** 2、在其下面增加以下语句（ljs是普通用户名,改成你自己的!）**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ljs ALL&#x3D;NOPASSWD:&#x2F;usr&#x2F;libexec&#x2F;openssh&#x2F;sftp-server</span><br></pre></td></tr></table></figure>

<p>** 3、然后向上找到下面语句，将其注释掉 **</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Defaults requiretty</span><br><span class="line">修改为</span><br><span class="line">#Defaults requiretty</span><br></pre></td></tr></table></figure>

<p>** 4、保存并退出 **</p>
<h2 id="三、打开winscp-设置-sftp和shell"><a href="#三、打开winscp-设置-sftp和shell" class="headerlink" title="三、打开winscp, 设置 sftp和shell"></a>三、打开winscp, 设置 sftp和shell</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo &#x2F;usr&#x2F;libexec&#x2F;openssh&#x2F;sftp-server</span><br><span class="line">sudu -i</span><br></pre></td></tr></table></figure>

<p><img src="/images/stfp1.png" alt=""><br><img src="/images/stfp2.png" alt=""></p>
]]></content>
  </entry>
  <entry>
    <title>yum安装redis、mongodb、memcached、rabbitmq</title>
    <url>/posts/d60357db.html</url>
    <content><![CDATA[<p>  因测试需要，简便安装测试环境所需软件，环境CentOS 7</p>
<a id="more"></a>
<h1 id="update"><a href="#update" class="headerlink" title="update"></a>update</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y update</span><br></pre></td></tr></table></figure>

<h1 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y redis</span><br><span class="line"></span><br><span class="line">systemctl enable redis</span><br><span class="line">systemctl start redis</span><br></pre></td></tr></table></figure>

<h1 id="memcached"><a href="#memcached" class="headerlink" title="memcached"></a>memcached</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install memcached</span><br><span class="line"></span><br><span class="line">#配置文件</span><br><span class="line">vi &#x2F;etc&#x2F;sysconfig&#x2F;memcached</span><br><span class="line"></span><br><span class="line">可添加OPTIONS&#x3D;&quot;-l 127.0.0.1 -U 0&quot; 只允许本机连接</span><br><span class="line"></span><br><span class="line">PORT：Memcached用来运行的端口。</span><br><span class="line">USER：Memcached服务的启动守护程序。</span><br><span class="line">MAXCONN：用于将最大同时连接数设置为1024的值。对于繁忙的Web服务器，您可以根据需要增加任何数量。</span><br><span class="line">CACHESIZE：将高速缓存大小内存设置为2048。对于繁忙的服务器，您最多可以增加4GB。</span><br><span class="line">OPTIONS：设置服务器的IP地址，以便Apache或Nginx Web服务器可以连接到它。</span><br><span class="line"></span><br><span class="line">#启动</span><br><span class="line">systemctl enable memcached</span><br><span class="line">systemctl restart memcached</span><br><span class="line"></span><br><span class="line">#检查服务器的统计信息</span><br><span class="line">memcached-tool 127.0.0.1 stats</span><br><span class="line"></span><br><span class="line">#客户端工具libmemcached</span><br><span class="line">Libmemcached是一个开源的Memcached客户端库，其内部实现了分布式管理、内存池等功能</span><br><span class="line"></span><br><span class="line">Libmemcached特性：</span><br><span class="line"></span><br><span class="line">异步和同步传输支持。</span><br><span class="line">支持一致性hash分布式算法。</span><br><span class="line">可调哈希算法来匹配密钥。</span><br><span class="line">访问大对象支持。</span><br><span class="line">本地复制。</span><br><span class="line">提供了一些管理memcached服务器的工具命令</span><br><span class="line"></span><br><span class="line">yum install libmemcached</span><br></pre></td></tr></table></figure>


<h1 id="mongodb"><a href="#mongodb" class="headerlink" title="mongodb"></a>mongodb</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#添加yum源</span><br><span class="line">vim &#x2F;etc&#x2F;yum.repos.d&#x2F;mongodb-org.repo</span><br><span class="line"></span><br><span class="line">[mongodb-org]</span><br><span class="line">name&#x3D;MongoDB Repository</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;mongodb&#x2F;yum&#x2F;redhat&#x2F;7Server&#x2F;mongodb-org&#x2F;3.2&#x2F;x86_64&#x2F;</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line"></span><br><span class="line">#清空缓存和更新yum源</span><br><span class="line">yum clean all</span><br><span class="line">yum make cache</span><br><span class="line"></span><br><span class="line">#安装</span><br><span class="line">yum install -y mongodb-org</span><br><span class="line">service mongod start</span><br><span class="line">chkconfig mongod on</span><br><span class="line"></span><br><span class="line">#使用认证</span><br><span class="line">vim &#x2F;etc&#x2F;mongod.conf</span><br><span class="line"></span><br><span class="line">security:</span><br><span class="line">  authorization: enabled</span><br><span class="line"></span><br><span class="line">#导出数据</span><br><span class="line">mongodump -h 192.168.100.65:27017 -d test -o &#x2F;home&#x2F; -u test -p 123456 --authenticationDatabase test</span><br><span class="line"></span><br><span class="line">#导入数据(有创建用户)</span><br><span class="line">mongorestore -h 192.168.1.79:27017 -d test --dir &#x2F;home&#x2F;test&#x2F; -u lolaage -p 123456 --authenticationDatabase admin</span><br><span class="line"></span><br><span class="line">#导入数据(无用户)</span><br><span class="line">mongorestore -h 192.168.1.79:27017 -d test --dir &#x2F;home&#x2F;test&#x2F;</span><br><span class="line"></span><br><span class="line">#导入数据（压缩）</span><br><span class="line">mongorestore -h 10.66.225.207:27017 -d test -u test -p 123456 --gzip --authenticationDatabase admin &#x2F;home&#x2F;test</span><br><span class="line"></span><br><span class="line">#创建用户</span><br><span class="line">db.createUser( </span><br><span class="line">  &#123; </span><br><span class="line">    user: &quot;test&quot;,</span><br><span class="line">    pwd: &quot;123456&quot;, </span><br><span class="line">    roles: [ &#123; role: &quot;root&quot;, db: &quot;admin&quot; &#125; ] </span><br><span class="line">  &#125; )</span><br><span class="line">db.system.users.update(&#123;&quot;db&quot;:&quot;admin&quot;&#125;,&#123;$set:&#123;&quot;db&quot;:&quot;test&quot;&#125;&#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># mongodb 4.0版本</span><br><span class="line">vi &#x2F;etc&#x2F;yum.repos.d&#x2F;mongodb-org-4.0.repo</span><br><span class="line"></span><br><span class="line">[mongodb-org-4.0]</span><br><span class="line">name&#x3D;MongoDB Repository</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;repo.mongodb.org&#x2F;yum&#x2F;redhat&#x2F;$releasever&#x2F;mongodb-org&#x2F;4.0&#x2F;x86_64&#x2F;</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;www.mongodb.org&#x2F;static&#x2F;pgp&#x2F;server-4.0.asc</span><br></pre></td></tr></table></figure>

<h1 id="rabbitmq"><a href="#rabbitmq" class="headerlink" title="rabbitmq"></a>rabbitmq</h1><p>官网：<a href="https://www.rabbitmq.com/install-rpm.html">https://www.rabbitmq.com/install-rpm.html</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># erlang</span><br><span class="line">curl -s https:&#x2F;&#x2F;packagecloud.io&#x2F;install&#x2F;repositories&#x2F;rabbitmq&#x2F;erlang&#x2F;script.rpm.sh | sudo bash</span><br><span class="line"></span><br><span class="line"># 导入2018年12月1日（GMT）开始使用的新PackageCloud密钥</span><br><span class="line">rpm --import https:&#x2F;&#x2F;packagecloud.io&#x2F;rabbitmq&#x2F;rabbitmq-server&#x2F;gpgkey</span><br><span class="line"></span><br><span class="line"># 导入2018年12月1日停止使用的旧PackageCloud密钥（GMT）</span><br><span class="line">rpm --import https:&#x2F;&#x2F;packagecloud.io&#x2F;gpg.key</span><br><span class="line"></span><br><span class="line"># 导入RabbitMQ签名密钥</span><br><span class="line">rpm --import https:&#x2F;&#x2F;github.com&#x2F;rabbitmq&#x2F;signing-keys&#x2F;releases&#x2F;download&#x2F;2.0&#x2F;rabbitmq-release-signing-key.asc</span><br><span class="line"></span><br><span class="line"># 添加Yum存储库</span><br><span class="line">vim &#x2F;etc&#x2F;yum.repos.d&#x2F;rabbitmq.repo</span><br><span class="line">[bintray-rabbitmq-server]</span><br><span class="line">name&#x3D;bintray-rabbitmq-rpm</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;dl.bintray.com&#x2F;rabbitmq&#x2F;rpm&#x2F;rabbitmq-server&#x2F;v3.7.x&#x2F;el&#x2F;7&#x2F;</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">repo_gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line"></span><br><span class="line">yum install rabbitmq-server</span><br><span class="line"></span><br><span class="line">chkconfig rabbitmq-server on</span><br><span class="line">service rabbitmq-server start</span><br><span class="line"></span><br><span class="line"># 查看RabbitMQ中用户命令</span><br><span class="line">rabbitmqctl list_users</span><br><span class="line"></span><br><span class="line"># 创建用户命令</span><br><span class="line">rabbitmqctl add_user test 123456</span><br><span class="line"></span><br><span class="line"># 赋予用户权限命令</span><br><span class="line">rabbitmqctl  set_permissions -p &quot;&#x2F;&quot; test &#39;.*&#39; &#39;.*&#39; &#39;.*&#39;</span><br><span class="line"></span><br><span class="line"># 赋予用户角色命令</span><br><span class="line">rabbitmqctl set_user_tags test administrator</span><br><span class="line"></span><br><span class="line"># 开启rabbitmq管理控制台命令</span><br><span class="line">rabbitmq-plugins enable rabbitmq_management</span><br><span class="line"></span><br><span class="line"># 错误</span><br><span class="line">偶遇一次rabbitmq启动失败，查看错误日志如下：</span><br><span class="line">* connected to epmd (port 4369) on k8s-node2</span><br><span class="line">* epmd reports: node &#39;rabbit&#39; not running at all</span><br><span class="line">no other nodes on k8s-node2</span><br><span class="line"></span><br><span class="line">由于hostname 对应的ip解析错误</span><br><span class="line">需要添加&#x2F;etc&#x2F;hosts即可</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>rabbitmq</tag>
        <tag>mongodb</tag>
        <tag>memcached</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>据说是小米招聘运维工程师的题目</title>
    <url>/posts/f1ba066e.html</url>
    <content><![CDATA[<h1 id="第一部分：Linux基础"><a href="#第一部分：Linux基础" class="headerlink" title="第一部分：Linux基础"></a>第一部分：Linux基础</h1><h2 id="题目1："><a href="#题目1：" class="headerlink" title="题目1："></a>题目1：</h2><p>有一百个图片文件，它们的地址都是<br><a href="http://down.xiaomi.com/img/1.png">http://down.xiaomi.com/img/1.png</a><br><a href="http://down.xiaomi.com/img/2.png">http://down.xiaomi.com/img/2.png</a><br>…<br>一直到<a href="http://down.xiaomi.com/img/100.png">http://down.xiaomi.com/img/100.png</a><br>批量下载这100个图片文件，并找出其中大于500KB的文件。</p>
<a id="more"></a>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">同相册的图片地址会有一定的规律，可以用：</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# echo http:&#x2F;&#x2F;down.xiaomi.com&#x2F;img&#x2F;&#123;001..100&#125;.png &gt; url.txt</span><br><span class="line"></span><br><span class="line">得到图片的地址是用空格分开的,再用vim编辑url文件，把空格替换成回车（\r）</span><br><span class="line">：s&#x2F; &#x2F;\r&#x2F;g</span><br><span class="line"></span><br><span class="line">在用wget命令批量下载：</span><br><span class="line">[root@localhost ~]# wget -i url.txt -P .&#x2F;photo</span><br><span class="line">wget命令的-i参数是从指定的文件读取地址，-P参数是把下载的文件放到指定的路径下。</span><br><span class="line"></span><br><span class="line">找出其中大于500KB的文件</span><br><span class="line">[root@localhost ~]# find .&#x2F;photo -type f -size +500k</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">echo &quot;downloading the picture......&quot;</span><br><span class="line">for i in &#123;1..100&#125;</span><br><span class="line">do</span><br><span class="line">    wget &quot;http:&#x2F;&#x2F;down.xiaomi.com&#x2F;img&#x2F;$i.png&quot;</span><br><span class="line">done</span><br><span class="line"> </span><br><span class="line">echo &quot;download done!&quot;</span><br><span class="line">echo &quot;find the file which is big than 500k&quot;</span><br><span class="line">find . -type f -size +500c -print</span><br></pre></td></tr></table></figure>

<h2 id="题目2："><a href="#题目2：" class="headerlink" title="题目2："></a>题目2：</h2><p>一个文本文件info.txt的内容如下：</p>
<p>aa,201<br>zz,502<br>bb,1<br>ee,42</p>
<p>每行都是按照逗号分隔，其中第二列都是数字，请对该文件按照第二列数字从大到小排列。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# sort  -n -r -k 2 -t&#39;,&#39; info.txt </span><br><span class="line">zz,502</span><br><span class="line">aa,201</span><br><span class="line">ee,42</span><br><span class="line">bb,1</span><br></pre></td></tr></table></figure>

<h2 id="题目3："><a href="#题目3：" class="headerlink" title="题目3："></a>题目3：</h2><p>查看当前Linux服务器是否监听80端口，如果在监听，请找出其进程ID，并结束该进程。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# netstat -an | grep -i listen | grep 80</span><br><span class="line">[root@localhost ~]# lsof -i:80</span><br><span class="line">[root@localhost ~]# kill -9 PID</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">方法一：使用for </span><br><span class="line">for i in &#96;netstat -tanp |grep 3306 |awk &#39;&#123;print $NF&#125;&#39; |cut -d &quot;&#x2F;&quot; -f 1&#96;;do kill -9 $i;done</span><br><span class="line"></span><br><span class="line">方法二：使用xargs</span><br><span class="line">netstat -tanp |grep 3306 |awk &#39;&#123;print $NF&#125;&#39; |cut -d &quot;&#x2F;&quot; -f 1 |xargs -I &#39;&#123;&#125;&#39; kill -9 &#123;&#125;</span><br><span class="line"></span><br><span class="line">方法三：使用awk</span><br><span class="line">netstat -tanp |grep 3306 |awk &#39;&#123;print $NF&#125;&#39; |cut -d &quot;&#x2F;&quot; -f 1 |awk &#39;&#123;print &quot;kill -9 &quot;$0&#125;&#39; |bash</span><br></pre></td></tr></table></figure>

<h2 id="题目4："><a href="#题目4：" class="headerlink" title="题目4："></a>题目4：</h2><p>使用curl或wget命令获取http服务的header信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# curl -I http:&#x2F;&#x2F;www.xiaomi.com</span><br><span class="line">HTTP&#x2F;1.1 301 Moved Permanently</span><br><span class="line">Server: Tengine</span><br><span class="line">Date: Thu, 16 Nov 2017 06:44:46 GMT</span><br><span class="line">Content-Type: text&#x2F;html</span><br><span class="line">Content-Length: 275</span><br><span class="line">Connection: close</span><br><span class="line">Location: https:&#x2F;&#x2F;www.mi.com</span><br></pre></td></tr></table></figure>

<h2 id="题目5："><a href="#题目5：" class="headerlink" title="题目5："></a>题目5：</h2><p>关于Linux的用户账号，下面说法正确的有：</p>
<p>A.用户的密码，是以明文形式存储在 /etc/passwd 文件中的</p>
<p>B.用户的密码，是以密文形式存储在 /etc/passwd 文件中的</p>
<p><strong>C.用户的密码，是以密文形式存储在 /etc/shadow 文件中的</strong></p>
<p>D.用户登录的时候，会把用户的密码明文与保存的密码做对比</p>
<h2 id="题目6："><a href="#题目6：" class="headerlink" title="题目6："></a>题目6：</h2><p>对于N块硬盘组成的硬盘阵列，下面的说法哪个是错误的：</p>
<p><strong>A.raid1 与 raid5 相比，读取数据的速度 raid5 更快</strong></p>
<p>B.raid1 与 raid5 相比，raid5 的磁盘空间利用率更高</p>
<p>C.raid1 在 （N-1）块磁盘损坏的情况下，不影响数据的完整性</p>
<p>D.raid0 相比于raid1、raid5，读写速度最快</p>
<h2 id="题目7："><a href="#题目7：" class="headerlink" title="题目7："></a>题目7：</h2><p>负载均衡，你了解的常用软件有哪些？请写出至少三种以上，并评价各自的缺点。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Nginx的缺点是：</span><br><span class="line">1. Nginx仅能支持http、https和Email协议，这样就在适用范围上面小些，这个是它的缺点。 </span><br><span class="line">2. 对后端服务器的健康检查，只支持通过端口来检测，不支持通过url来检测。不支持Session的直接保持，但能通过ip_hash来解决。</span><br><span class="line"></span><br><span class="line">LVS的缺点是：   </span><br><span class="line">1. 软件本身不支持正则表达式处理（仅仅支持4层负载均衡），不能做动静分离；而现在许多网站在这方面都有较强的需求，这个是Nginx&#x2F;HAProxy+Keepalived的优势所在。 </span><br><span class="line">2. 如果是网站应用比较庞大的话，LVS&#x2F;DR+Keepalived实施起来就比较复杂了，特别后面有 Windows Server的机器的话，如果实施及配置还有维护过程就比较复杂了，相对而言Nginx&#x2F;HAProxy+Keepalived就简单多了。</span><br></pre></td></tr></table></figure>

<h2 id="题目8："><a href="#题目8：" class="headerlink" title="题目8："></a>题目8：</h2><p>执行 $ time sleep 2，输出如下：</p>
<p>real    0m2.003s<br>user   0m0.004s<br>sys    0m0.000s</p>
<p>请说明 real、user、sys三者具体代表的意思和区别。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当测试一个程序或比较不同算法时，执行时间是非常重要的，一个好的算法应该是用时最短的。所有类UNIX系统都包含time命令，使用这个命令可以统计时间消耗。</span><br><span class="line"></span><br><span class="line">real时间是指挂钟时间，也就是命令开始执行到结束的时间。这个短时间包括其他进程所占用的时间片，和进程被阻塞时所花费的时间。</span><br><span class="line">user时间是指进程花费在用户模式中的CPU时间，这是唯一真正用于执行进程所花费的时间，其他进程和花费阻塞状态中的时间没有计算在内。 </span><br><span class="line">sys时间是指花费在内核模式中的CPU时间，代表在内核中执系统调用所花费的时间，这也是真正由进程使用的CPU时间。</span><br></pre></td></tr></table></figure>

<h2 id="题目9："><a href="#题目9：" class="headerlink" title="题目9："></a>题目9：</h2><p>nginx rewrite 规则中 last、break、redirect、permanent 的含义。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rewite</span><br><span class="line"></span><br><span class="line">在server块下，会优先执行rewrite部分，然后才会去匹配location块 </span><br><span class="line">server中的rewrite break和last没什么区别，都会去匹配location，所以没必要用last再发起新的请求，可以留空</span><br><span class="line"></span><br><span class="line">location中的rewirte：</span><br><span class="line"></span><br><span class="line">不写last和break - 那么流程就是依次执行这些rewrite </span><br><span class="line">1. rewrite break - url重写后，直接使用当前资源，不再执行location里余下的语句，完成本次请求，地址栏url不变 </span><br><span class="line">2. rewrite last - url重写后，马上发起一个新的请求，再次进入server块，重试location匹配，超过10次匹配不到报500错误，地址栏url不变 </span><br><span class="line">3. rewrite redirect – 返回302临时重定向，地址栏显示重定向后的url，爬虫不会更新url（因为是临时） </span><br><span class="line">4. rewrite permanent – 返回301永久重定向, 地址栏显示重定向后的url，爬虫更新url</span><br><span class="line"></span><br><span class="line">使用last会对server标签重新发起请求</span><br><span class="line"></span><br><span class="line">如果location中rewrite后是对静态资源的请求，不需要再进行其他匹配，一般要使用break或不写，直接使用当前location中的数据源，完成本次请求 </span><br><span class="line">如果location中rewrite后，还需要进行其他处理，如动态fastcgi请求(.php,.jsp)等，要用last继续发起新的请求 </span><br><span class="line">(根的location使用last比较好, 因为如果有.php等fastcgi请求还要继续处理)</span><br><span class="line"></span><br><span class="line">使用alias指定源：必须使用last</span><br><span class="line"></span><br><span class="line">if语句主要用来判断一些在rewrite语句中无法直接匹配的条件,比如检测文件存在与否,http header,cookie等</span><br><span class="line"></span><br><span class="line">location匹配规则及优先级</span><br><span class="line"></span><br><span class="line">&#x3D; 严格匹配这个查询。如果找到，停止搜索。</span><br><span class="line">^~ 匹配路径的前缀，如果找到，停止搜索。</span><br><span class="line">~ 为区分大小写的正则匹配</span><br><span class="line">~* 为不区分大小写匹配 </span><br><span class="line">优先级： &#x3D;, ^~, ~&#x2F;~*, 无</span><br><span class="line">break语句</span><br><span class="line"></span><br><span class="line">放在server块rewrite语句前面 </span><br><span class="line">如果是直接请求某个真实存在的文件,则用break语句停止rewrite检查 </span><br><span class="line">if (-f $request_filename) &#123; </span><br><span class="line">break; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="题目10："><a href="#题目10：" class="headerlink" title="题目10："></a>题目10：</h2><p>WEB 服务 cookies 和 session 的区别。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、cookie数据存放在客户的浏览器上，session数据放在服务器上 </span><br><span class="line">2、cookie不是很安全，别人可以分析存放在本地的COOKIE并进行 </span><br><span class="line">3、session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能 </span><br><span class="line">4、单个cookie在客户端的限制是3K，就是说一个站点在客户端存放的COOKIE不能大于3K。</span><br><span class="line"></span><br><span class="line">出处：http:&#x2F;&#x2F;blog.csdn.net&#x2F;u010168160&#x2F;article&#x2F;details&#x2F;47128443</span><br><span class="line"></span><br><span class="line">更多解释：</span><br><span class="line">https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;19786827</span><br></pre></td></tr></table></figure>


<h2 id="题目11："><a href="#题目11：" class="headerlink" title="题目11："></a>题目11：</h2><p>http 1.0 和 http 1.1下有何区别？http 2.0的主要变化或优势有哪些？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HTTP 协议老的标准是HTTP&#x2F;1.0，目前最通用的标准是HTTP&#x2F;1.1。　　</span><br><span class="line"></span><br><span class="line">在同一个tcp的连接中可以传送多个HTTP请求和响应,多个请求和响应可以重叠，多个请求和响应可以同时进行,更加多的请求头和响应头(比如HTTP1.0没有host的字段)。</span><br><span class="line"></span><br><span class="line">它们最大的区别：</span><br><span class="line">1、在 HTTP&#x2F;1.0 中,大多实现为每个请求&#x2F;响应交换使用新的连接。HTTP 1.0规定浏览器与服务器只保持短暂的连接，浏览器的每次请求都需要与服务器建立一个TCP连接，服务器完成请求处理后立即断开TCP连接，服务器不跟踪每个客户也不记录过去的请求。</span><br><span class="line"></span><br><span class="line">2、在 HTTP&#x2F;1.1 中,一个连接可用于一次或多次请求&#x2F;响应交换,尽管连接可能由于各种原因被关闭。HTTP 1.1支持持久连接，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟。一个包含有许多图像的网页文件的多个请求和应答可以在一个连接中传输，但每个单独的网页文件的请求和应答仍然需要使用各自的连接。HTTP 1.1还允许客户端不用等待上一次请求结果返回，就可以发出下一次请求，但服务器端必须按照接收到客户端请求的先后顺序依次回送响应结果，以保证客户端能够区分出每次请求的响应内容，这样也显著地减少了整个下载过程所需要的时间。</span><br><span class="line"></span><br><span class="line">相比 HTTP&#x2F;1.x，HTTP&#x2F;2 在底层传输做了很大的改动和优化：</span><br><span class="line"></span><br><span class="line">１.HTTP&#x2F;2 采用二进制格式传输数据，而非 HTTP&#x2F;1.x 的文本格式。二进制格式在协议的解析和优化扩展上带来更多的优势和可能。 </span><br><span class="line">2.HTTP&#x2F;2 对消息头采用 HPACK 进行压缩传输，能够节省消息头占用的网络的流量。而 HTTP&#x2F;1.x 每次请求，都会携带大量冗余头信息，浪费了很多带宽资源。头压缩能够很好的解决该问题。 </span><br><span class="line">3.多路复用，直白的说就是所有的请求都是通过一个 TCP 连接并发完成。HTTP&#x2F;1.x 虽然通过 pipeline 也能并发请求，但是多个请求之间的响应会被阻塞的，所以 pipeline 至今也没有被普及应用，而 HTTP&#x2F;2 做到了真正的并发请求。同时，流还支持优先级和流量控制。 </span><br><span class="line">4.Server Push：服务端能够更快的把资源推送给客户端。例如服务端可以主动把 JS 和 CSS 文件推送给客户端，而不需要客户端解析 HTML 再发送这些请求。当客户端需要的时候，它已经在客户端了。</span><br></pre></td></tr></table></figure>

<h1 id="第二部分：安全部分"><a href="#第二部分：安全部分" class="headerlink" title="第二部分：安全部分"></a>第二部分：安全部分</h1><h2 id="题目12："><a href="#题目12：" class="headerlink" title="题目12："></a>题目12：</h2><p>请列举WEB常见安全问题（不少于三项），阐述其原理、危害。</p>
<h2 id="题目13："><a href="#题目13：" class="headerlink" title="题目13："></a>题目13：</h2><p>请列举常见DOS攻击类型，并分别介绍其原理和防御方法。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;blog.csdn.net&#x2F;libin_1&#x2F;article&#x2F;details&#x2F;8116091</span><br></pre></td></tr></table></figure>

<h2 id="题目14："><a href="#题目14：" class="headerlink" title="题目14："></a>题目14：</h2><p>服务器遭到入侵，作为安全管理人员，你应做如何处理？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.cnblogs.com&#x2F;lidong94&#x2F;p&#x2F;7161793.html</span><br></pre></td></tr></table></figure>


<h1 id="第三部分：网络部分"><a href="#第三部分：网络部分" class="headerlink" title="第三部分：网络部分"></a>第三部分：网络部分</h1><h2 id="题目15："><a href="#题目15：" class="headerlink" title="题目15："></a>题目15：</h2><p>请简述TCP的三次握手过程。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TCP(Transmission Control Protocol)　传输控制协议</span><br><span class="line"></span><br><span class="line">TCP是主机对主机层的传输控制协议，提供可靠的连接服务，采用三次握手确认建立一个连接:</span><br><span class="line"></span><br><span class="line">位码即tcp标志位,有6种标示:SYN(synchronous建立联机) ACK(acknowledgement 确认) PSH(push传送) FIN(finish结束) RST(reset重置) URG(urgent紧急)</span><br><span class="line"></span><br><span class="line">Sequence number(顺序号码) Acknowledge number(确认号码)</span><br><span class="line"></span><br><span class="line">第一次握手：主机A发送位码为syn＝1,随机产生seq number&#x3D;1234567的数据包到服务器，主机B由SYN&#x3D;1知道，A要求建立联机；</span><br><span class="line"></span><br><span class="line">第二次握手：主机B收到请求后要确认联机信息，向A发送ack number&#x3D;(主机A的seq+1),syn&#x3D;1,ack&#x3D;1,随机产生seq&#x3D;7654321的包</span><br><span class="line"></span><br><span class="line">第三次握手：主机A收到后检查ack number是否正确，即第一次发送的seq number+1,以及位码ack是否为1，若正确，主机A会再发送ack number&#x3D;(主机B的seq+1),ack&#x3D;1，主机B收到后确认seq值与ack&#x3D;1则连接建立成功。</span><br><span class="line"></span><br><span class="line">完成三次握手，主机A与主机B开始传送数据。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在TCP&#x2F;IP协议中，TCP协议提供可靠的连接服务，采用三次握手建立一个连接。 </span><br><span class="line">第一次握手：建立连接时，客户端发送syn包(syn&#x3D;j)到服务器，并进入SYN_SEND状态，等待服务器确认； </span><br><span class="line">第二次握手：服务器收到syn包，必须确认客户的SYN（ack&#x3D;j+1），同时自己也发送一个SYN包（syn&#x3D;k），即SYN+ACK包，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack&#x3D;k+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。 完成三次握手，客户端与服务器开始传送数据.</span><br></pre></td></tr></table></figure>

<h2 id="题目16："><a href="#题目16：" class="headerlink" title="题目16："></a>题目16：</h2><p>发现系统中存在大量TIME_WAIT，分析原因并提出三条以上优化建议。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;blog.sina.com.cn&#x2F;s&#x2F;blog_5d74fde50102x3di.html</span><br><span class="line">http:&#x2F;&#x2F;kerry.blog.51cto.com&#x2F;172631&#x2F;105233&#x2F;</span><br></pre></td></tr></table></figure>


<h1 id="第四部分：Python部分"><a href="#第四部分：Python部分" class="headerlink" title="第四部分：Python部分"></a>第四部分：Python部分</h1><h2 id="题目17："><a href="#题目17：" class="headerlink" title="题目17："></a>题目17：</h2><p>xrange和range的异同。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">range range([start,] stop[, step])，根据start与stop指定的范围以及step设定的步长，生成一个序列。</span><br><span class="line"></span><br><span class="line">xrange 用法与 range 完全相同，所不同的是生成的不是一个list对象，而是一个生成器。要生成很大的数字序列的时候，用xrange会比range性能优很多，因为不需要一上来就开辟一块很大的内存空间。</span><br></pre></td></tr></table></figure>

<h2 id="题目18："><a href="#题目18：" class="headerlink" title="题目18："></a>题目18：</h2><p>列表 A 和 B，使用Python快速获取 A 和 B 中的共有元素。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(set(A) &amp; set(B))</span><br><span class="line"></span><br><span class="line">for i in A:</span><br><span class="line">    if i in B:</span><br><span class="line">        print i</span><br></pre></td></tr></table></figure>

<p>题目19：</p>
<p>有20台服务器，需要在所有机器上 echo “123”，要求同时并行操作5台服务器，请使用Python或shell写出相关代码。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># encoding:utf-8</span><br><span class="line"> </span><br><span class="line">import time</span><br><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import threadpool</span><br><span class="line"> </span><br><span class="line">def cmd(ip):</span><br><span class="line">    #需先设置远程无密码执行命令</span><br><span class="line">    os.system(‘ssh ip echo &quot;123&quot; 2&gt;&amp;1’)</span><br><span class="line">    time.sleep(2)</span><br><span class="line"> </span><br><span class="line">pool &#x3D; threadpool.ThreadPool(5)                     #创建一个线程池，包括线程数10个</span><br><span class="line">requests &#x3D; threadpool.makeRequests(cmd, ip_list)    #传递函数和参数</span><br><span class="line">[pool.putRequest(req) for req in requests]          #将请求放入线程池进行处理</span><br><span class="line">pool.wait()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>linux招聘题目</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s实现优雅关闭pod，实现真正的零宕机迁移</title>
    <url>/posts/b9c57dba.html</url>
    <content><![CDATA[<p>首先，要实现这个目标的先决条件是我们的容器要正确处理终止信号，在 SIGTERM 信号上实现优雅关闭。下一步需要添加 readiness 可读探针，来检查我们的应用程序是否已经准备好来处理流量了。</p>
<p>可读探针只是我们平滑滚动更新的起点，为了解决 Pod 停止的时候不会阻塞并等到负载均衡器重新配置的问题，我们需要使用 preStop 这个生命周期的钩子，在容器终止之前调用该钩子。</p>
<a id="more"></a>
<p>生命周期钩子函数是同步的，所以必须在将最终终止信号发送到容器之前完成，在我们的示例中，我们使用该钩子简单的等待，然后 SIGTERM 信号将停止应用程序进程。同时，Kubernetes 将从 Endpoints 对象中删除该 Pod，所以该 Pod 将会从我们的负载均衡器中排除，基本上来说我们的生命周期钩子函数等待的时间可以确保在应用程序停止之前重新配置负载均衡器。</p>
<p>这里我们在 zero-downtime 这个 Deployment 中添加一个 preStop 钩子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: zero-downtime</span><br><span class="line">  labels:</span><br><span class="line">    app: zero-downtime</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zero-downtime</span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: zero-downtime</span><br><span class="line">        image: nginx</span><br><span class="line">        livenessProbe:</span><br><span class="line">          # ...</span><br><span class="line">        readinessProbe:</span><br><span class="line">          # ...</span><br><span class="line">        lifecycle:</span><br><span class="line">          preStop:</span><br><span class="line">            exec:</span><br><span class="line">              command: [&quot;&#x2F;bin&#x2F;bash&quot;, &quot;-c&quot;, &quot;sleep 20&quot;]</span><br><span class="line">  strategy:</span><br><span class="line">    # ...</span><br></pre></td></tr></table></figure>

<p>我们这里使用 preStop 设置了一个 20s 的宽限期，Pod 在真正销毁前会先 sleep 等待 20s，这就相当于留了时间给 Endpoints 控制器和 kube-proxy 更新去 Endpoints 对象和转发规则，这段时间 Pod 虽然处于 Terminating 状态，即便在转发规则更新完全之前有请求被转发到这个 Terminating 的 Pod，依然可以被正常处理，因为它还在 sleep，没有被真正销毁。</p>
<p>现在，当我们去查看滚动更新期间的 Pod 行为时，我们将看到正在终止的 Pod 处于 Terminating 状态，但是在等待时间结束之前不会关闭的。</p>
<h1 id="何为Pod容器钩子"><a href="#何为Pod容器钩子" class="headerlink" title="何为Pod容器钩子"></a>何为Pod容器钩子</h1><p>Kubernetes最小调度单位为Pod,它为Pod中的容器提供了生命周期钩子，钩子能够使得容器感知其生命周期内的所有事件，并且当相应的生命周期的钩子被调用时运行执行的代码，而Pod 钩子是由Kubelet发起的。</p>
<p>容器钩子两类触发点：</p>
<ul>
<li>PostStart：容器创建后</li>
</ul>
<p>这个钩子在容器创建后立即执行。<br>但是，并不能保证钩子将在容器ENTRYPOINT之前运行。<br>没有参数传递给处理程序。</p>
<p>容器ENTRYPOINT和钩子执行是异步操作。<br>如果钩子花费太长时间以至于容器不能运行或者挂起， 容器将不能达到running状态</p>
<ul>
<li>PreStop：容器终止前</li>
</ul>
<p>这个钩子在容器终止之前立即被调用。<br>它是阻塞的，意味着它是同步的， 所以它必须在删除容器的调用发出之前完成</p>
<p>如果钩子在执行期间挂起， Pod阶段将停留在running状态并且永不会达到failed状态。</p>
<p>如果PostStart或者PreStop钩子失败， 容器将会被kill。<br>用户应该使他们的钩子处理程序尽可能的轻量。</p>
<h1 id="Pod容器钩子有何作用"><a href="#Pod容器钩子有何作用" class="headerlink" title="Pod容器钩子有何作用"></a>Pod容器钩子有何作用</h1><p>微服务中，网关会把流量分配给每个Pod节点，如：我们线上更新Pod的时候</p>
<ol>
<li><p>如果我们直接把Pod给杀死，那这部分流量就无法得到正确的处理，会影响到部分用户访问，一般来说网关或者注册中心会将我们的服务保持一个心跳，过了心跳超时后就会自动摘除我们的服务，但是有一个问题就是超时时间可能是10s、30s、甚至是60s，虽然不会大规模的影响我们业务系统，但是一定会对用户产生轻微的抖动。</p>
</li>
<li><p>如果我们在停止服务前执行一条命令，通知网关或注册中心这台Pod，即服务进行下线，那么注册中心就会标记这个Pod/服务已经下线，不进行流量转发，用户也就不会有任何的影响，这就是优雅停止，将滚动更新的影响最小化。</p>
</li>
</ol>
<h1 id="基于PostStart演示"><a href="#基于PostStart演示" class="headerlink" title="基于PostStart演示"></a>基于PostStart演示</h1><p>如果PostStart或者PreStop钩子失败，它会杀死容器。所以我们应该让钩子函数尽可能的轻量。当然有些情况下，长时间运行命令是合理的，比如在停止容器之前预先保留状态。</p>
<h2 id="我们echo一段话追加到-tmp-message，在Pod启动前操作"><a href="#我们echo一段话追加到-tmp-message，在Pod启动前操作" class="headerlink" title="我们echo一段话追加到/tmp/message，在Pod启动前操作"></a>我们echo一段话追加到/tmp/message，在Pod启动前操作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;&gt;hook_test.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: hook-demo1</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: hook-demo1</span><br><span class="line">    image: nginx</span><br><span class="line">    lifecycle:</span><br><span class="line">      postStart:</span><br><span class="line">        exec:</span><br><span class="line">          command: [&quot;&#x2F;bin&#x2F;sh&quot;, &quot;-c&quot;, &quot;echo 1 &gt; &#x2F;tmp&#x2F;message&quot;]</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h2 id="应用hook-test-yaml"><a href="#应用hook-test-yaml" class="headerlink" title="应用hook_test.yaml"></a>应用hook_test.yaml</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f  hook_test.yaml</span><br></pre></td></tr></table></figure>

<h2 id="查看结果"><a href="#查看结果" class="headerlink" title="查看结果"></a>查看结果</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get pods | grep hook-demo1</span><br><span class="line">hook-demo1                 1&#x2F;1     Running   0          49s</span><br><span class="line"></span><br><span class="line">$ kubectl exec -it hook-demo1 &#x2F;bin&#x2F;bash</span><br><span class="line">root@hook-demo1:&#x2F;# cat &#x2F;tmp&#x2F;message</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<h1 id="基于PreStop演示"><a href="#基于PreStop演示" class="headerlink" title="基于PreStop演示"></a>基于PreStop演示</h1><p>下面示例中，定义一个Nginx Pod，设置了PreStop钩子函数，即在容器退出之前，优雅的关闭Nginx。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &gt;&gt;hook_test.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: hook-demo2</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: hook-demo2</span><br><span class="line">    image: nginx</span><br><span class="line">    lifecycle:</span><br><span class="line">      preStop:</span><br><span class="line">        exec:</span><br><span class="line">          command: [&quot;&#x2F;usr&#x2F;sbin&#x2F;nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h1 id="优雅停止Java应用"><a href="#优雅停止Java应用" class="headerlink" title="优雅停止Java应用"></a>优雅停止Java应用</h1><p>我们都知道java应用的启动和停止都需要时间，为了更加优雅的停止，可以通过pidof获取到java进程ID，循环通过kill命令往PID发送SIGTERM信号。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lifecycle:</span><br><span class="line">  preStop:</span><br><span class="line">    exec:</span><br><span class="line">      command: [&quot;&#x2F;bin&#x2F;bash&quot;,&quot;-c&quot;,&quot;PID&#x3D;&#96;pidof java&#96; &amp;&amp; kill -SIGTERM $PID &amp;&amp; while ps -p $PID &gt; &#x2F;dev&#x2F;null;do sleep 1; done;&quot;]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubernets</tag>
      </tags>
  </entry>
  <entry>
    <title>备份数据库脚本并将备份数据库同步到又拍云存储空间</title>
    <url>/posts/6869d857.html</url>
    <content><![CDATA[<p>经历过一次意外，可能某拥有服务器权限的同事将测试器将全部数据给删除了，导致服务器数据库全部丢失，服务器崩溃，只能重装。特弄了一个简单的数据库脚本备份并将数据库同步到又拍云存储空间。</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">##又拍云UPX工具</span><br><span class="line"></span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;polym&#x2F;upx</span><br></pre></td></tr></table></figure>

<p>同步脚本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost backup]# cat backup.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">##用于本地目录中的所有文件同步上传到又拍云存储，用时监控目录变化</span><br><span class="line"></span><br><span class="line">service_name&#x3D;&quot;testbackup&quot;</span><br><span class="line">username&#x3D;&quot;******&quot;</span><br><span class="line">password&#x3D;&quot;******&quot;</span><br><span class="line"></span><br><span class="line">&#x2F;home&#x2F;backup&#x2F;upx login testbackup ****** ******</span><br><span class="line"></span><br><span class="line">&#x2F;home&#x2F;backup&#x2F;upx sync &#x2F;home&#x2F;backup</span><br></pre></td></tr></table></figure>

<p>mysql备份脚本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost backup]# cat xtrabackup-full.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash  </span><br><span class="line"> </span><br><span class="line">user&#x3D;&#39;root&#39; </span><br><span class="line">passwd&#x3D;&#39;123456&#39;</span><br><span class="line">host&#x3D;&#39;127.0.0.1&#39;</span><br><span class="line">port&#x3D;&#39;3307&#39;</span><br><span class="line">database&#x3D;&#39;cloud_yssj&#39; </span><br><span class="line">my_config&#x3D;&#39;&#x2F;usr&#x2F;local&#x2F;mysql-test1&#x2F;my.cnf&#39; </span><br><span class="line">log&#x3D;$database-$(date +%Y%m%d%H%M).log  </span><br><span class="line">str&#x3D;$database-$(date +%Y%m%d%H%M).tar.gz  </span><br><span class="line">backup_dir&#x3D;&#39;&#x2F;home&#x2F;backup&#x2F;mysql&#39;</span><br><span class="line"></span><br><span class="line">echo &quot;Start to backup at $(date +%Y%m%d%H%M)&quot;  </span><br><span class="line">if [ ! -d &quot;$backup_dir&quot; ];then  </span><br><span class="line">    mkdir -p $backup_dir  </span><br><span class="line">fi  </span><br><span class="line"></span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;innobackupex --defaults-file&#x3D;$my_config --user&#x3D;$user --password&#x3D;$passwd --port&#x3D;$port --host&#x3D;$host --database&#x3D;$database --stream&#x3D;tar $backup_dir 2&gt;$backup_dir&#x2F;$log | gzip 1&gt;$backup_dir&#x2F;$str  </span><br><span class="line"> </span><br><span class="line">if [ $? -eq 0 ];then  </span><br><span class="line">    echo &quot;Backup is finish! at $(date +%Y%m%d%H%M)&quot;  </span><br><span class="line">    exit 0  </span><br><span class="line">else  </span><br><span class="line">    echo &quot;Backup is Fail! at $(date +%Y%m%d%H%M)&quot;  </span><br><span class="line">    exit 1  </span><br><span class="line">fi  </span><br><span class="line">echo &quot;Backup Process Done&quot;</span><br></pre></td></tr></table></figure>

<p>mongodb、redis备份脚本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost backup]# cat mongodb.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash  </span><br><span class="line"></span><br><span class="line">host&#x3D;&#39;127.0.0.1:37018&#39;</span><br><span class="line">database&#x3D;&#39;cloud_yssj&#39; </span><br><span class="line"> </span><br><span class="line">backup_dir1&#x3D;&#39;&#x2F;home&#x2F;backup&#x2F;mongodb&#39;</span><br><span class="line">backup_dir2&#x3D;&#39;&#x2F;home&#x2F;backup&#x2F;redis&#39;</span><br><span class="line"> </span><br><span class="line">if [ ! -d &quot;$backup_dir1&quot; ];then  </span><br><span class="line">    mkdir -p $backup_dir1  </span><br><span class="line">fi  </span><br><span class="line"></span><br><span class="line">if [ ! -d &quot;$backup_dir2&quot; ];then  </span><br><span class="line">    mkdir -p $backup_dir2  </span><br><span class="line">fi  </span><br><span class="line"></span><br><span class="line">&#x2F;home&#x2F;mongodb&#x2F;bin&#x2F;mongodump -h $host -d $database -o $backup_dir1  </span><br><span class="line"> </span><br><span class="line">tar zcvf $backup_dir1&#x2F;mongodb-$(date +%Y%m%d%H%M).tar.gz $backup_dir1&#x2F;$database</span><br><span class="line"></span><br><span class="line">rm -rf &#x2F;home&#x2F;backup&#x2F;mongodb&#x2F;cloud_yssj</span><br><span class="line"></span><br><span class="line">tar zcvf $backup_dir2&#x2F;redis-$(date +%Y%m%d%H%M).tar.gz &#x2F;home&#x2F;redis&#x2F;data&#x2F;dump8000.rdb</span><br></pre></td></tr></table></figure>

<p>增加定时任务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost backup]# crontab -l</span><br><span class="line">30 1 * * * &#x2F;home&#x2F;backup&#x2F;mongodb.sh</span><br><span class="line">20 2 * * * &#x2F;home&#x2F;backup&#x2F;xtrabackup-full.sh</span><br><span class="line">40 3 * * * &#x2F;home&#x2F;backup&#x2F;backup.sh</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>备份</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mongodb</tag>
        <tag>redis</tag>
        <tag>xtrabackup</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Supervisor 管理服务器后台进程</title>
    <url>/posts/cd8b16a0.html</url>
    <content><![CDATA[<p>Supervisor (<a href="http://supervisord.org">http://supervisord.org</a>) 是一个用 Python 写的进程管理工具。提供web页面管理，能对进程进行自动重启等操作。</p>
<a id="more"></a>
<p>优点：</p>
<ul>
<li>可以将非后台运行程序后台运行</li>
<li>自动监控，重启进程</li>
</ul>
<p>缺点：</p>
<ul>
<li>不能管理后台运行程序</li>
<li>对多进程服务，不能使用kill关闭</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>直接使用 pip 进行安装：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo pip install supervisor</span><br></pre></td></tr></table></figure>
<p>如没有pip命令，需进行安装pip</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ curl https:&#x2F;&#x2F;bootstrap.pypa.io&#x2F;get-pip.py -o get-pip.py   # 下载安装脚本</span><br><span class="line">$ sudo python get-pip.py    # 运行安装脚本</span><br></pre></td></tr></table></figure>
<p><strong>注意：用哪个版本的 Python 运行安装脚本，pip 就被关联到哪个版本，如果是 Python3 则执行以下命令：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo python3 get-pip.py    # 运行安装脚本。</span><br></pre></td></tr></table></figure>

<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>生产默认配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ echo_supervisord_conf &gt; &#x2F;etc&#x2F;supervisord.conf</span><br></pre></td></tr></table></figure>
<p>配置文件相关说明:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ vim &#x2F;etc&#x2F;supervisord.conf</span><br><span class="line"></span><br><span class="line">内容：</span><br><span class="line"></span><br><span class="line"># 指定了socket file的位置</span><br><span class="line">[unix_http_server]</span><br><span class="line">file&#x3D;&#x2F;tmp&#x2F;supervisor.sock   ;UNIX socket 文件，supervisorctl 会使用</span><br><span class="line">;chmod&#x3D;0700                 ;socket文件的mode，默认是0700</span><br><span class="line">;chown&#x3D;nobody:nogroup       ;socket文件的owner，格式：uid:gid</span><br><span class="line"> </span><br><span class="line"> #用于启动一个含有前端的服务，可以从Web页面中管理服务。其中，port用于设置访问地址，username和password用于设置授权认证。</span><br><span class="line">;[inet_http_server]         ;HTTP服务器，提供web管理界面</span><br><span class="line">;port&#x3D;127.0.0.1:9001        ;Web管理后台运行的IP和端口，如果开放到公网，需要注意安全性</span><br><span class="line">;username&#x3D;user              ;登录管理后台的用户名</span><br><span class="line">;password&#x3D;123               ;登录管理后台的密码</span><br><span class="line"> </span><br><span class="line"> # 管理服务本身的配置</span><br><span class="line">[supervisord]</span><br><span class="line">logfile&#x3D;&#x2F;tmp&#x2F;supervisord.log ;日志文件，默认是 $CWD&#x2F;supervisord.log</span><br><span class="line">logfile_maxbytes&#x3D;50MB        ;日志文件大小，超出会rotate，默认 50MB，如果设成0，表示不限制大小</span><br><span class="line">logfile_backups&#x3D;10           ;日志文件保留备份数量默认10，设为0表示不备份</span><br><span class="line">loglevel&#x3D;info                ;日志级别，默认info，其它: debug,warn,trace</span><br><span class="line">pidfile&#x3D;&#x2F;tmp&#x2F;supervisord.pid ;pid 文件</span><br><span class="line">nodaemon&#x3D;false               ;是否在前台启动，默认是false，即以 daemon 的方式启动</span><br><span class="line">minfds&#x3D;1024                  ;可以打开的文件描述符的最小值，默认 1024</span><br><span class="line">minprocs&#x3D;200                 ;可以打开的进程数的最小值，默认 200</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">[supervisorctl]</span><br><span class="line">serverurl&#x3D;unix:&#x2F;&#x2F;&#x2F;tmp&#x2F;supervisor.sock ;通过UNIX socket连接supervisord，路径与unix_http_server部分的file一致</span><br><span class="line">;serverurl&#x3D;http:&#x2F;&#x2F;127.0.0.1:9001 ; 通过HTTP的方式连接supervisord</span><br><span class="line"> </span><br><span class="line">; [program:xx]是被管理的进程配置参数，xx是进程的名称</span><br><span class="line">[program:xx]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;apache-tomcat-8.0.35&#x2F;bin&#x2F;catalina.sh run  ; 程序启动命令</span><br><span class="line">autostart&#x3D;true       ; 在supervisord启动的时候也自动启动</span><br><span class="line">startsecs&#x3D;10         ; 启动10秒后没有异常退出，就表示进程正常启动了，默认为1秒</span><br><span class="line">autorestart&#x3D;true     ; 程序退出后自动重启,可选值：[unexpected,true,false]，默认为unexpected，表示进程意外杀死后才重启</span><br><span class="line">startretries&#x3D;3       ; 启动失败自动重试次数，默认是3</span><br><span class="line">user&#x3D;tomcat          ; 用哪个用户启动进程，默认是root</span><br><span class="line">priority&#x3D;999         ; 进程启动优先级，默认999，值小的优先启动</span><br><span class="line">redirect_stderr&#x3D;true ; 把stderr重定向到stdout，默认false</span><br><span class="line">stdout_logfile_maxbytes&#x3D;20MB  ; stdout 日志文件大小，默认50MB</span><br><span class="line">stdout_logfile_backups &#x3D; 20   ; stdout 日志文件备份数，默认是10</span><br><span class="line">; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件）</span><br><span class="line">stdout_logfile&#x3D;&#x2F;opt&#x2F;apache-tomcat-8.0.35&#x2F;logs&#x2F;catalina.out</span><br><span class="line">stopasgroup&#x3D;false     ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup&#x3D;false     ;默认为false，向进程组发送kill信号，包括子进程</span><br><span class="line"> # 对事件进行的管理</span><br><span class="line">;[eventlistener:theeventlistenername]</span><br><span class="line"></span><br><span class="line"># 对任务组的管理 ,包含其它配置文件</span><br><span class="line">;[group:thegroupname]</span><br><span class="line">;programs&#x3D;progname1,progname2  ; each refers to &#39;x&#39; in [program:x] definitions</span><br><span class="line">;priority&#x3D;999                  ; the relative start priority (default 999)</span><br><span class="line"></span><br><span class="line">[include]</span><br><span class="line">files &#x3D; supervisord.d&#x2F;*.ini    ;可以指定一个或多个以.ini结束的配置文件可以是 *.conf</span><br></pre></td></tr></table></figure>

<h2 id="配置需要管理的进程"><a href="#配置需要管理的进程" class="headerlink" title="配置需要管理的进程"></a>配置需要管理的进程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">注：supervisord.d目录用来存放用户自定义的进程配置</span><br><span class="line"></span><br><span class="line">例子：文件 qqc.ini</span><br><span class="line"># 程序名称，在 supervisorctl 中通过这个值来对程序进行一系列的操作</span><br><span class="line">[program:qqc_one]</span><br><span class="line"># 启动命令，与手动在命令行启动的命令是一样的</span><br><span class="line">command&#x3D;gunicorn wsgi:application -b 0.0.0.0:8000 -k gevent -w 2 -t 10 -n yun-gunicorn -m 0022 --log-level&#x3D;info --access-logfile&#x3D;- --error-logfile&#x3D;- --max-requests&#x3D;5000</span><br><span class="line"># 程序的启动目录</span><br><span class="line">directory&#x3D;&#x2F;home&#x2F;yun&#x2F;projects&#x2F;yun</span><br><span class="line"># 可以通过 environment 来添加需要的环境变量，一种常见的用法是使用指定的 virtualenv 环境</span><br><span class="line">environment&#x3D;DJANGO_SETTINGS_MODULE&#x3D;settings.test</span><br><span class="line"># 指定用户名</span><br><span class="line">user&#x3D;opt</span><br><span class="line"># 日志目录 需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件）</span><br><span class="line">stdout_logfile&#x3D;&#x2F;var&#x2F;log&#x2F;supervisor&#x2F;yun-gunicorn.stdout.log</span><br><span class="line">stderr_logfile&#x3D;&#x2F;var&#x2F;log&#x2F;supervisor&#x2F;yun-gunicorn.stderr.log</span><br><span class="line"># 日志备份数量</span><br><span class="line">stdout_logfile_backups &#x3D; 20</span><br><span class="line"># 指定日志文件大小</span><br><span class="line">stdout_logfile_maxbytes &#x3D; 20MB </span><br><span class="line"># 把 stderr 重定向到 stdout，默认 false,错误日志也会写进stdout_logfile中</span><br><span class="line">redirect_stderr&#x3D;True</span><br><span class="line"># 在 supervisord 启动的时候也自动启动</span><br><span class="line">autostart&#x3D;True    </span><br><span class="line"># 程序异常退出后自动重启</span><br><span class="line">autorestart&#x3D;True</span><br><span class="line"></span><br><span class="line">[program:qqc_two]</span><br><span class="line">...</span><br><span class="line">[program:qqc_free]</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># 集中管理多个进程</span><br><span class="line">[group:qqc]</span><br><span class="line">programs&#x3D;qqc_one,qqc_two,qqc_free</span><br></pre></td></tr></table></figure>
<h2 id="启动-Supervisor"><a href="#启动-Supervisor" class="headerlink" title="启动 Supervisor"></a>启动 Supervisor</h2><p>Supervisor 有两个主要的组成部分：</p>
<ul>
<li>supervisord，运行 Supervisor 时会启动一个进程 supervisord，它负责启动所管理的进程，并将所管理的进程作为自己的子进程来启动，而且可以在所管理的进程出现崩溃时自动重启。</li>
<li>supervisorctl，是命令行管理工具，可以用来执行 stop、start、restart 等命令，来对这些子进程进行管理。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ supervisord -c &#x2F;etc&#x2F;supervisord.conf</span><br><span class="line">$ supervisorctl -c &#x2F;etc&#x2F;supervisord.conf status</span><br></pre></td></tr></table></figure>

<h2 id="可视化管理进程"><a href="#可视化管理进程" class="headerlink" title="可视化管理进程"></a>可视化管理进程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ vim &#x2F;etc&#x2F;supervisord.conf</span><br><span class="line"></span><br><span class="line"># 取消注释和更改设置</span><br><span class="line">[inet_http_server]         ; HTTP 服务器，提供 web 管理界面</span><br><span class="line">port&#x3D;0.0.0.0:8080          ; Web 管理后台运行的 IP 和端口，如果开放到公网，需要注意安全性</span><br><span class="line">username&#x3D;user              ; 登录管理后台的用户名</span><br><span class="line">password&#x3D;123               ; 登录管理后台的密码</span><br><span class="line"></span><br><span class="line">[rpcinterface:supervisor]</span><br><span class="line">supervisor.rpcinterface_factory &#x3D; supervisor.rpcinterface:make_main_rpcinterface</span><br><span class="line"></span><br><span class="line">[supervisorctl]</span><br><span class="line">serverurl&#x3D;http:&#x2F;&#x2F;0.0.0.0:8080    ; 通过 HTTP 的方式连接 supervisord</span><br></pre></td></tr></table></figure>

<h2 id="开机自启动-Supervisor"><a href="#开机自启动-Supervisor" class="headerlink" title="开机自启动 Supervisor"></a>开机自启动 Supervisor</h2><p>Linux 在启动的时候会执行 /etc/rc.local 里面的脚本，所以只要在这里添加执行命令就可以：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如果是 Centos 添加以下内容</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;supervisord -c &#x2F;etc&#x2F;supervisord.conf</span><br><span class="line"></span><br><span class="line"># 以上内容需要添加在 exit 命令前，而且由于在执行 rc.local 脚本时，PATH 环境变量</span><br><span class="line"># 未全部初始化，因此命令需要使用绝对路径。可以使用一下命令查看绝对路径：</span><br><span class="line"></span><br><span class="line">$ sudo find &#x2F; -name supervisord</span><br><span class="line">&gt; &#x2F;usr&#x2F;local&#x2F;python&#x2F;bin&#x2F;supervisord</span><br><span class="line"></span><br><span class="line">所以要改下路径：</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;python&#x2F;bin&#x2F;supervisord -c &#x2F;etc&#x2F;supervisord.conf</span><br></pre></td></tr></table></figure>
<h2 id="管理命令"><a href="#管理命令" class="headerlink" title="管理命令"></a>管理命令</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">supervisorctl stop program_name       # 停止某一个进程，program_name 为 [program:x] 里的 x</span><br><span class="line">supervisorctl start program_name      # 启动某个进程</span><br><span class="line">supervisorctl restart program_name    # 重启某个进程</span><br><span class="line">supervisorctl stop groupworker:       # 结束所有属于名为 groupworker 这个分组的进程 (start，restart 同理)</span><br><span class="line">supervisorctl stop groupworker:name1  # 结束 groupworker:name1 这个进程 (start，restart 同理)</span><br><span class="line">supervisorctl stop all                # 停止全部进程，注：start、restart、stop 都不会载入最新的配置文件</span><br><span class="line">supervisorctl reload                  # 载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程</span><br><span class="line">supervisorctl update                  # 根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启</span><br><span class="line">supervisorctl shutdown                # 关闭</span><br></pre></td></tr></table></figure>

<p><strong>注意: supervisor不能监控后台进程，command 不能为后台运行命令</strong></p>
]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
      <tags>
        <tag>Supervisor</tag>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title>linux查看哪个进程占用磁盘IO</title>
    <url>/posts/4c372563.html</url>
    <content><![CDATA[<h2 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ iotop -oP</span><br><span class="line"></span><br><span class="line">命令的含义：只显示有I&#x2F;O行为的进程</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<ul>
<li>CentOS/Fedora/RHEL版本的linux中则使用下面的命令：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install iotop -y</span><br></pre></td></tr></table></figure>

<h2 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pidstat -d 1</span><br><span class="line"></span><br><span class="line">命令的含义：展示I&#x2F;O统计，每秒更新一次</span><br></pre></td></tr></table></figure>

<ul>
<li>CentOS/Fedora/RHEL版本的linux中则使用下面的命令</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install sysstat -y</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维部署记录</category>
      </categories>
  </entry>
</search>
